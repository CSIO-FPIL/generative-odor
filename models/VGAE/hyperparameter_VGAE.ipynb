{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "\n",
    "class DeepChemToPyGDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        \"\"\"\n",
    "        Converts a list of PyG `Data` objects into an InMemoryDataset.\n",
    "        \n",
    "        Args:\n",
    "            data_list (list of Data): List of PyG Data objects.\n",
    "            transform (callable, optional): Optional transform applied to each data object.\n",
    "        \"\"\"\n",
    "        super(DeepChemToPyGDataset, self).__init__('.', transform, None, None)\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the data object at the specified index.\n",
    "        \"\"\"\n",
    "        return super().get(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\AppData\\Local\\Temp\\ipykernel_14700\\3101895145.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pyg_dataset = torch.load('../../data/curated dataset/frag_pyg_dataset.pth')\n"
     ]
    }
   ],
   "source": [
    "pyg_dataset = torch.load('../../data/curated dataset/frag_pyg_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = pyg_dataset[0].x.shape[1]  # Should be 134 based on your description\n",
    "edge_features = pyg_dataset[0].edge_attr.shape[1]  # Should be 6 based on your description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGAE model\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim, latent_dim):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.encoder = VGEncoder(node_features, edge_features, hidden_dim, latent_dim)\n",
    "        self.decoder = VGDecoder(latent_dim, hidden_dim, node_features, edge_features)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        mu, logvar = self.encode(x, edge_index, edge_attr)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_x, reconstructed_edge_attr = self.decode(z, edge_index)\n",
    "        return reconstructed_x, reconstructed_edge_attr, mu, logvar\n",
    "\n",
    "    def generate_multiple_outputs(self, x, edge_index, edge_attr, num_samples=5):\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            z, _ = self.encode(x, edge_index, edge_attr)\n",
    "            for _ in range(num_samples):\n",
    "                # Add noise to z (sampling from normal distribution)\n",
    "                z_noisy = z + torch.randn_like(z) * 0.5  # Adjust std deviation for noise level\n",
    "                reconstructed_x, reconstructed_edge_attr = self.decode(z_noisy, edge_index)\n",
    "                outputs.append((reconstructed_x, reconstructed_edge_attr, edge_index))\n",
    "        return outputs\n",
    "\n",
    "class VGEncoder(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim, latent_dim):\n",
    "        super(VGEncoder, self).__init__()\n",
    "        self.conv1 = SAGEConv(node_features + edge_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)  # Additional layer\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_attr_expanded = torch.zeros((x.size(0), edge_attr.size(1))).to(x.device)\n",
    "        edge_attr_expanded[edge_index[0]] = edge_attr\n",
    "        x = torch.cat([x, edge_attr_expanded], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))  # Added layer\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "class VGDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, node_features, edge_features):\n",
    "        super(VGDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.node_decoder = nn.Linear(hidden_dim, node_features)\n",
    "        self.edge_decoder = nn.Linear(hidden_dim * 2, edge_features)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        h = F.relu(self.fc(z))\n",
    "        reconstructed_x = self.node_decoder(h)\n",
    "\n",
    "        # Edge reconstruction\n",
    "        row, col = edge_index\n",
    "        edge_h = torch.cat([h[row], h[col]], dim=1)\n",
    "        reconstructed_edge_attr = self.edge_decoder(edge_h)\n",
    "\n",
    "        return reconstructed_x, reconstructed_edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, recon_edge_attr, edge_attr, mu, logvar):\n",
    "    # Reconstruction losses\n",
    "    node_recon_loss = F.mse_loss(recon_x, x)\n",
    "    edge_recon_loss = F.mse_loss(recon_edge_attr, edge_attr)\n",
    "\n",
    "    # KL divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return node_recon_loss + edge_recon_loss + kl_divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-12-06 10:42:51,983] A new study created in memory with name: no-name-c2bb1d01-ee04-44ce-b420-3b0d6b528140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] Epoch 1/60, Training Loss: 3.6297, Validation Loss: 0.2238\n",
      "[Trial 6] Epoch 1/60, Training Loss: 3895.1328, Validation Loss: 0.2734\n",
      "[Trial 3] Epoch 1/60, Training Loss: 9.3771, Validation Loss: 0.9101\n",
      "[Trial 4] Epoch 1/60, Training Loss: 1.2932, Validation Loss: 0.2659\n",
      "[Trial 0] Epoch 2/60, Training Loss: 0.1337, Validation Loss: 0.1926\n",
      "[Trial 6] Epoch 2/60, Training Loss: 0.1717, Validation Loss: 0.1866\n",
      "[Trial 1] Epoch 1/60, Training Loss: 0.6068, Validation Loss: 0.2283\n",
      "[Trial 3] Epoch 2/60, Training Loss: 0.6096, Validation Loss: 0.5453\n",
      "[Trial 4] Epoch 2/60, Training Loss: 0.1722, Validation Loss: 0.2328\n",
      "[Trial 0] Epoch 3/60, Training Loss: 0.1296, Validation Loss: 0.1722\n",
      "[Trial 5] Epoch 1/60, Training Loss: 0.8370, Validation Loss: 0.2761\n",
      "[Trial 6] Epoch 3/60, Training Loss: 0.1443, Validation Loss: 0.1810\n",
      "[Trial 3] Epoch 3/60, Training Loss: 0.3873, Validation Loss: 0.4226\n",
      "[Trial 4] Epoch 3/60, Training Loss: 0.1531, Validation Loss: 0.2209\n",
      "[Trial 0] Epoch 4/60, Training Loss: 0.1281, Validation Loss: 0.1601\n",
      "[Trial 6] Epoch 4/60, Training Loss: 0.1418, Validation Loss: 0.1720\n",
      "[Trial 2] Epoch 1/60, Training Loss: 2.9881, Validation Loss: 0.3914\n",
      "[Trial 1] Epoch 2/60, Training Loss: 0.1472, Validation Loss: 0.2095\n",
      "[Trial 0] Epoch 5/60, Training Loss: 0.1272, Validation Loss: 0.1508\n",
      "[Trial 3] Epoch 4/60, Training Loss: 0.2960, Validation Loss: 0.3636\n",
      "[Trial 4] Epoch 4/60, Training Loss: 0.1451, Validation Loss: 0.2129\n",
      "[Trial 6] Epoch 5/60, Training Loss: 0.1396, Validation Loss: 0.1654\n",
      "[Trial 0] Epoch 6/60, Training Loss: 0.1271, Validation Loss: 0.1415\n",
      "[Trial 5] Epoch 2/60, Training Loss: 0.1592, Validation Loss: 0.2577\n",
      "[Trial 6] Epoch 6/60, Training Loss: 0.1374, Validation Loss: 0.1604\n",
      "[Trial 3] Epoch 5/60, Training Loss: 0.2488, Validation Loss: 0.3305\n",
      "[Trial 4] Epoch 5/60, Training Loss: 0.1401, Validation Loss: 0.2043\n",
      "[Trial 0] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1388\n",
      "[Trial 1] Epoch 3/60, Training Loss: 0.1383, Validation Loss: 0.2000\n",
      "[Trial 6] Epoch 7/60, Training Loss: 0.1370, Validation Loss: 0.1559\n",
      "[Trial 3] Epoch 6/60, Training Loss: 0.2210, Validation Loss: 0.3090\n",
      "[Trial 4] Epoch 6/60, Training Loss: 0.1371, Validation Loss: 0.1992\n",
      "[Trial 0] Epoch 8/60, Training Loss: 0.1264, Validation Loss: 0.1337\n",
      "[Trial 6] Epoch 8/60, Training Loss: 0.1347, Validation Loss: 0.1519\n",
      "[Trial 2] Epoch 2/60, Training Loss: 0.2708, Validation Loss: 0.2968\n",
      "[Trial 3] Epoch 7/60, Training Loss: 0.2032, Validation Loss: 0.2960\n",
      "[Trial 4] Epoch 7/60, Training Loss: 0.1350, Validation Loss: 0.1943\n",
      "[Trial 0] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1324\n",
      "[Trial 5] Epoch 3/60, Training Loss: 0.1455, Validation Loss: 0.2506\n",
      "[Trial 6] Epoch 9/60, Training Loss: 0.1342, Validation Loss: 0.1485\n",
      "[Trial 1] Epoch 4/60, Training Loss: 0.1346, Validation Loss: 0.1909\n",
      "[Trial 3] Epoch 8/60, Training Loss: 0.1910, Validation Loss: 0.2858\n",
      "[Trial 0] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1306\n",
      "[Trial 4] Epoch 8/60, Training Loss: 0.1339, Validation Loss: 0.1892\n",
      "[Trial 6] Epoch 10/60, Training Loss: 0.1338, Validation Loss: 0.1465\n",
      "[Trial 0] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1299\n",
      "[Trial 3] Epoch 9/60, Training Loss: 0.1819, Validation Loss: 0.2789\n",
      "[Trial 6] Epoch 11/60, Training Loss: 0.1336, Validation Loss: 0.1456\n",
      "[Trial 4] Epoch 9/60, Training Loss: 0.1327, Validation Loss: 0.1835\n",
      "[Trial 1] Epoch 5/60, Training Loss: 0.1328, Validation Loss: 0.1855\n",
      "[Trial 0] Epoch 12/60, Training Loss: 0.1257, Validation Loss: 0.1298\n",
      "[Trial 6] Epoch 12/60, Training Loss: 0.1347, Validation Loss: 0.1433\n",
      "[Trial 5] Epoch 4/60, Training Loss: 0.1393, Validation Loss: 0.2464\n",
      "[Trial 3] Epoch 10/60, Training Loss: 0.1751, Validation Loss: 0.2730\n",
      "[Trial 4] Epoch 10/60, Training Loss: 0.1321, Validation Loss: 0.1809\n",
      "[Trial 2] Epoch 3/60, Training Loss: 0.2069, Validation Loss: 0.2689\n",
      "[Trial 0] Epoch 13/60, Training Loss: 0.1256, Validation Loss: 0.1291\n",
      "[Trial 6] Epoch 13/60, Training Loss: 0.1331, Validation Loss: 0.1413\n",
      "[Trial 3] Epoch 11/60, Training Loss: 0.1698, Validation Loss: 0.2684\n",
      "[Trial 4] Epoch 11/60, Training Loss: 0.1310, Validation Loss: 0.1784\n",
      "[Trial 0] Epoch 14/60, Training Loss: 0.1256, Validation Loss: 0.1290\n",
      "[Trial 1] Epoch 6/60, Training Loss: 0.1310, Validation Loss: 0.1785\n",
      "[Trial 6] Epoch 14/60, Training Loss: 0.1343, Validation Loss: 0.1388\n",
      "[Trial 3] Epoch 12/60, Training Loss: 0.1655, Validation Loss: 0.2637\n",
      "[Trial 4] Epoch 12/60, Training Loss: 0.1307, Validation Loss: 0.1729\n",
      "[Trial 0] Epoch 15/60, Training Loss: 0.1256, Validation Loss: 0.1287\n",
      "[Trial 6] Epoch 15/60, Training Loss: 0.1319, Validation Loss: 0.1378\n",
      "[Trial 5] Epoch 5/60, Training Loss: 0.1360, Validation Loss: 0.2433\n",
      "[Trial 3] Epoch 13/60, Training Loss: 0.1620, Validation Loss: 0.2607\n",
      "[Trial 0] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1286\n",
      "[Trial 4] Epoch 13/60, Training Loss: 0.1302, Validation Loss: 0.1683\n",
      "[Trial 6] Epoch 16/60, Training Loss: 0.1328, Validation Loss: 0.1360\n",
      "[Trial 1] Epoch 7/60, Training Loss: 0.1298, Validation Loss: 0.1731\n",
      "[Trial 0] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1284\n",
      "[Trial 2] Epoch 4/60, Training Loss: 0.1799, Validation Loss: 0.2545\n",
      "[Trial 3] Epoch 14/60, Training Loss: 0.1589, Validation Loss: 0.2577\n",
      "[Trial 4] Epoch 14/60, Training Loss: 0.1297, Validation Loss: 0.1655\n",
      "[Trial 6] Epoch 17/60, Training Loss: 0.1574, Validation Loss: 0.1382\n",
      "[Trial 0] Epoch 18/60, Training Loss: 0.1256, Validation Loss: 0.1284\n",
      "[Trial 3] Epoch 15/60, Training Loss: 0.1559, Validation Loss: 0.2552\n",
      "[Trial 6] Epoch 18/60, Training Loss: 0.1307, Validation Loss: 0.1328\n",
      "[Trial 4] Epoch 15/60, Training Loss: 0.1297, Validation Loss: 0.1620\n",
      "[Trial 5] Epoch 6/60, Training Loss: 0.1337, Validation Loss: 0.2400\n",
      "[Trial 1] Epoch 8/60, Training Loss: 0.1297, Validation Loss: 0.1673\n",
      "[Trial 0] Epoch 19/60, Training Loss: 0.1256, Validation Loss: 0.1281\n",
      "[Trial 6] Epoch 19/60, Training Loss: 0.1323, Validation Loss: 0.1327\n",
      "[Trial 3] Epoch 16/60, Training Loss: 0.1539, Validation Loss: 0.2525\n",
      "[Trial 4] Epoch 16/60, Training Loss: 0.1295, Validation Loss: 0.1604\n",
      "[Trial 0] Epoch 20/60, Training Loss: 0.1259, Validation Loss: 0.1286\n",
      "[Trial 6] Epoch 20/60, Training Loss: 0.9556, Validation Loss: 0.1941\n",
      "[Trial 3] Epoch 17/60, Training Loss: 0.1520, Validation Loss: 0.2508\n",
      "[Trial 4] Epoch 17/60, Training Loss: 0.1288, Validation Loss: 0.1590\n",
      "[Trial 0] Epoch 21/60, Training Loss: 0.1254, Validation Loss: 0.1284\n",
      "[Trial 1] Epoch 9/60, Training Loss: 0.1287, Validation Loss: 0.1611\n",
      "[Trial 2] Epoch 5/60, Training Loss: 0.1652, Validation Loss: 0.2467\n",
      "[Trial 6] Epoch 21/60, Training Loss: 0.1477, Validation Loss: 0.1302\n",
      "[Trial 5] Epoch 7/60, Training Loss: 0.1325, Validation Loss: 0.2388\n",
      "[Trial 0] Epoch 22/60, Training Loss: 0.1255, Validation Loss: 0.1281\n",
      "[Trial 3] Epoch 18/60, Training Loss: 0.1505, Validation Loss: 0.2493\n",
      "[Trial 4] Epoch 18/60, Training Loss: 0.1288, Validation Loss: 0.1565\n",
      "[Trial 6] Epoch 22/60, Training Loss: 0.1315, Validation Loss: 0.1294\n",
      "[Trial 0] Epoch 23/60, Training Loss: 0.1253, Validation Loss: 0.1280\n",
      "[Trial 3] Epoch 19/60, Training Loss: 0.1487, Validation Loss: 0.2472\n",
      "[Trial 4] Epoch 19/60, Training Loss: 0.1285, Validation Loss: 0.1530\n",
      "[Trial 1] Epoch 10/60, Training Loss: 0.1284, Validation Loss: 0.1581\n",
      "[Trial 6] Epoch 23/60, Training Loss: 0.1320, Validation Loss: 0.1287\n",
      "[Trial 0] Epoch 24/60, Training Loss: 0.1254, Validation Loss: 0.1284\n",
      "[Trial 3] Epoch 20/60, Training Loss: 0.1474, Validation Loss: 0.2461\n",
      "[Trial 6] Epoch 24/60, Training Loss: 0.1289, Validation Loss: 0.1275\n",
      "[Trial 4] Epoch 20/60, Training Loss: 0.1284, Validation Loss: 0.1507\n",
      "[Trial 5] Epoch 8/60, Training Loss: 0.1314, Validation Loss: 0.2365\n",
      "[Trial 0] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1278\n",
      "[Trial 6] Epoch 25/60, Training Loss: 0.1283, Validation Loss: 0.1268\n",
      "[Trial 3] Epoch 21/60, Training Loss: 0.1460, Validation Loss: 0.2441\n",
      "[Trial 2] Epoch 6/60, Training Loss: 0.1559, Validation Loss: 0.2415\n",
      "[Trial 4] Epoch 21/60, Training Loss: 0.1283, Validation Loss: 0.1493\n",
      "[Trial 1] Epoch 11/60, Training Loss: 0.1282, Validation Loss: 0.1547\n",
      "[Trial 0] Epoch 26/60, Training Loss: 0.1254, Validation Loss: 0.1279\n",
      "[Trial 6] Epoch 26/60, Training Loss: 0.1290, Validation Loss: 0.1264\n",
      "[Trial 3] Epoch 22/60, Training Loss: 0.1448, Validation Loss: 0.2431\n",
      "[Trial 0] Epoch 27/60, Training Loss: 0.1253, Validation Loss: 0.1286\n",
      "[Trial 4] Epoch 22/60, Training Loss: 0.1278, Validation Loss: 0.1470\n",
      "[Trial 6] Epoch 27/60, Training Loss: 0.1289, Validation Loss: 0.1257\n",
      "[Trial 0] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1283\n",
      "[Trial 5] Epoch 9/60, Training Loss: 0.1305, Validation Loss: 0.2340\n",
      "[Trial 3] Epoch 23/60, Training Loss: 0.1441, Validation Loss: 0.2409\n",
      "[Trial 1] Epoch 12/60, Training Loss: 0.1277, Validation Loss: 0.1504\n",
      "[Trial 4] Epoch 23/60, Training Loss: 0.1282, Validation Loss: 0.1456\n",
      "[Trial 6] Epoch 28/60, Training Loss: 0.1297, Validation Loss: 0.1254\n",
      "[Trial 0] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1282\n",
      "[Trial 3] Epoch 24/60, Training Loss: 0.1430, Validation Loss: 0.2401\n",
      "[Trial 4] Epoch 24/60, Training Loss: 0.1279, Validation Loss: 0.1437\n",
      "[Trial 6] Epoch 29/60, Training Loss: 0.1285, Validation Loss: 0.1251\n",
      "[Trial 0] Epoch 30/60, Training Loss: 0.1253, Validation Loss: 0.1283\n",
      "[Trial 2] Epoch 7/60, Training Loss: 0.1505, Validation Loss: 0.2369\n",
      "[Trial 3] Epoch 25/60, Training Loss: 0.1419, Validation Loss: 0.2381\n",
      "[Trial 1] Epoch 13/60, Training Loss: 0.1274, Validation Loss: 0.1471\n",
      "[Trial 4] Epoch 25/60, Training Loss: 0.1277, Validation Loss: 0.1423\n",
      "[Trial 6] Epoch 30/60, Training Loss: 0.1276, Validation Loss: 0.1247\n",
      "[Trial 0] Epoch 31/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 5] Epoch 10/60, Training Loss: 0.1304, Validation Loss: 0.2321\n",
      "[Trial 3] Epoch 26/60, Training Loss: 0.1412, Validation Loss: 0.2376\n",
      "[Trial 6] Epoch 31/60, Training Loss: 0.1279, Validation Loss: 0.1244\n",
      "[Trial 4] Epoch 26/60, Training Loss: 0.1275, Validation Loss: 0.1409\n",
      "[Trial 0] Epoch 32/60, Training Loss: 0.1250, Validation Loss: 0.1278\n",
      "[Trial 6] Epoch 32/60, Training Loss: 0.1278, Validation Loss: 0.1241\n",
      "[Trial 3] Epoch 27/60, Training Loss: 0.1407, Validation Loss: 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 10:50:07,825] Trial 0 finished with value: 0.12777193387349448 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.0032363994918188973, 'batch_size': 64, 'patience': 8}. Best is trial 0 with value: 0.12777193387349448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 4] Epoch 27/60, Training Loss: 0.1277, Validation Loss: 0.1396\n",
      "[Trial 0] Epoch 33/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 0] Early stopping after 33 epochs.\n",
      "[Trial 1] Epoch 14/60, Training Loss: 0.1273, Validation Loss: 0.1428\n",
      "[Trial 7] Epoch 1/60, Training Loss: 13.0969, Validation Loss: 0.4191\n",
      "[Trial 6] Epoch 33/60, Training Loss: 0.1284, Validation Loss: 0.1240\n",
      "[Trial 3] Epoch 28/60, Training Loss: 0.1399, Validation Loss: 0.2356\n",
      "[Trial 4] Epoch 28/60, Training Loss: 0.1273, Validation Loss: 0.1385\n",
      "[Trial 5] Epoch 11/60, Training Loss: 0.1295, Validation Loss: 0.2290\n",
      "[Trial 7] Epoch 2/60, Training Loss: 0.2460, Validation Loss: 0.2541\n",
      "[Trial 2] Epoch 8/60, Training Loss: 0.1455, Validation Loss: 0.2340\n",
      "[Trial 6] Epoch 34/60, Training Loss: 0.1289, Validation Loss: 0.1239\n",
      "[Trial 7] Epoch 3/60, Training Loss: 0.1837, Validation Loss: 0.2172\n",
      "[Trial 3] Epoch 29/60, Training Loss: 0.1390, Validation Loss: 0.2348\n",
      "[Trial 1] Epoch 15/60, Training Loss: 0.1271, Validation Loss: 0.1415\n",
      "[Trial 4] Epoch 29/60, Training Loss: 0.1274, Validation Loss: 0.1371\n",
      "[Trial 6] Epoch 35/60, Training Loss: 0.1279, Validation Loss: 0.1239\n",
      "[Trial 7] Epoch 4/60, Training Loss: 0.1632, Validation Loss: 0.1964\n",
      "[Trial 3] Epoch 30/60, Training Loss: 0.1386, Validation Loss: 0.2336\n",
      "[Trial 4] Epoch 30/60, Training Loss: 0.1276, Validation Loss: 0.1364\n",
      "[Trial 7] Epoch 5/60, Training Loss: 0.1530, Validation Loss: 0.1822\n",
      "[Trial 6] Epoch 36/60, Training Loss: 0.1275, Validation Loss: 0.1237\n",
      "[Trial 5] Epoch 12/60, Training Loss: 0.1295, Validation Loss: 0.2278\n",
      "[Trial 7] Epoch 6/60, Training Loss: 0.1470, Validation Loss: 0.1715\n",
      "[Trial 4] Epoch 31/60, Training Loss: 0.1272, Validation Loss: 0.1366\n",
      "[Trial 1] Epoch 16/60, Training Loss: 0.1271, Validation Loss: 0.1385\n",
      "[Trial 3] Epoch 31/60, Training Loss: 0.1381, Validation Loss: 0.2322\n",
      "[Trial 6] Epoch 37/60, Training Loss: 0.1276, Validation Loss: 0.1237\n",
      "[Trial 7] Epoch 7/60, Training Loss: 0.1427, Validation Loss: 0.1634\n",
      "[Trial 2] Epoch 9/60, Training Loss: 0.1423, Validation Loss: 0.2315\n",
      "[Trial 4] Epoch 32/60, Training Loss: 0.1277, Validation Loss: 0.1348\n",
      "[Trial 3] Epoch 32/60, Training Loss: 0.1377, Validation Loss: 0.2319\n",
      "[Trial 6] Epoch 38/60, Training Loss: 0.1277, Validation Loss: 0.1238\n",
      "[Trial 7] Epoch 8/60, Training Loss: 0.1397, Validation Loss: 0.1565\n",
      "[Trial 7] Epoch 9/60, Training Loss: 0.1377, Validation Loss: 0.1520\n",
      "[Trial 6] Epoch 39/60, Training Loss: 0.1275, Validation Loss: 0.1237\n",
      "[Trial 1] Epoch 17/60, Training Loss: 0.1270, Validation Loss: 0.1364\n",
      "[Trial 4] Epoch 33/60, Training Loss: 0.1271, Validation Loss: 0.1341\n",
      "[Trial 3] Epoch 33/60, Training Loss: 0.1370, Validation Loss: 0.2310\n",
      "[Trial 5] Epoch 13/60, Training Loss: 0.1290, Validation Loss: 0.2273\n",
      "[Trial 7] Epoch 10/60, Training Loss: 0.1359, Validation Loss: 0.1463\n",
      "[Trial 6] Epoch 40/60, Training Loss: 0.1286, Validation Loss: 0.1237\n",
      "[Trial 4] Epoch 34/60, Training Loss: 0.1268, Validation Loss: 0.1333\n",
      "[Trial 3] Epoch 34/60, Training Loss: 0.1367, Validation Loss: 0.2299\n",
      "[Trial 7] Epoch 11/60, Training Loss: 0.1346, Validation Loss: 0.1442\n",
      "[Trial 6] Epoch 41/60, Training Loss: 0.1279, Validation Loss: 0.1237\n",
      "[Trial 7] Epoch 12/60, Training Loss: 0.1334, Validation Loss: 0.1407\n",
      "[Trial 1] Epoch 18/60, Training Loss: 0.1270, Validation Loss: 0.1345\n",
      "[Trial 4] Epoch 35/60, Training Loss: 0.1272, Validation Loss: 0.1325\n",
      "[Trial 3] Epoch 35/60, Training Loss: 0.1365, Validation Loss: 0.2298\n",
      "[Trial 7] Epoch 13/60, Training Loss: 0.1327, Validation Loss: 0.1392\n",
      "[Trial 6] Epoch 42/60, Training Loss: 0.1267, Validation Loss: 0.1238\n",
      "[Trial 2] Epoch 10/60, Training Loss: 0.1396, Validation Loss: 0.2292\n",
      "[Trial 5] Epoch 14/60, Training Loss: 0.1289, Validation Loss: 0.2307\n",
      "[Trial 4] Epoch 36/60, Training Loss: 0.1278, Validation Loss: 0.1318\n",
      "[Trial 7] Epoch 14/60, Training Loss: 0.1318, Validation Loss: 0.1376\n",
      "[Trial 3] Epoch 36/60, Training Loss: 0.1361, Validation Loss: 0.2285\n",
      "[Trial 6] Epoch 43/60, Training Loss: 0.1269, Validation Loss: 0.1236\n",
      "[Trial 7] Epoch 15/60, Training Loss: 0.1311, Validation Loss: 0.1360\n",
      "[Trial 1] Epoch 19/60, Training Loss: 0.1269, Validation Loss: 0.1335\n",
      "[Trial 4] Epoch 37/60, Training Loss: 0.1268, Validation Loss: 0.1317\n",
      "[Trial 6] Epoch 44/60, Training Loss: 0.1271, Validation Loss: 0.1236\n",
      "[Trial 3] Epoch 37/60, Training Loss: 0.1356, Validation Loss: 0.2278\n",
      "[Trial 7] Epoch 16/60, Training Loss: 0.1307, Validation Loss: 0.1348\n",
      "[Trial 6] Epoch 45/60, Training Loss: 0.1268, Validation Loss: 0.1238\n",
      "[Trial 7] Epoch 17/60, Training Loss: 0.1301, Validation Loss: 0.1339\n",
      "[Trial 4] Epoch 38/60, Training Loss: 0.1266, Validation Loss: 0.1314\n",
      "[Trial 3] Epoch 38/60, Training Loss: 0.1352, Validation Loss: 0.2275\n",
      "[Trial 5] Epoch 15/60, Training Loss: 0.1291, Validation Loss: 0.2239\n",
      "[Trial 7] Epoch 18/60, Training Loss: 0.1296, Validation Loss: 0.1333\n",
      "[Trial 1] Epoch 20/60, Training Loss: 0.1268, Validation Loss: 0.1319\n",
      "[Trial 6] Epoch 46/60, Training Loss: 0.1307, Validation Loss: 0.1252\n",
      "[Trial 2] Epoch 11/60, Training Loss: 0.1375, Validation Loss: 0.2275\n",
      "[Trial 4] Epoch 39/60, Training Loss: 0.1274, Validation Loss: 0.1307\n",
      "[Trial 3] Epoch 39/60, Training Loss: 0.1350, Validation Loss: 0.2260\n",
      "[Trial 7] Epoch 19/60, Training Loss: 0.1293, Validation Loss: 0.1323\n",
      "[Trial 6] Epoch 47/60, Training Loss: 0.1268, Validation Loss: 0.1237\n",
      "[Trial 7] Epoch 20/60, Training Loss: 0.1289, Validation Loss: 0.1321\n",
      "[Trial 4] Epoch 40/60, Training Loss: 0.1270, Validation Loss: 0.1306\n",
      "[Trial 3] Epoch 40/60, Training Loss: 0.1346, Validation Loss: 0.2251\n",
      "[Trial 6] Epoch 48/60, Training Loss: 0.1263, Validation Loss: 0.1236\n",
      "[Trial 7] Epoch 21/60, Training Loss: 0.1288, Validation Loss: 0.1312\n",
      "[Trial 1] Epoch 21/60, Training Loss: 0.1267, Validation Loss: 0.1307\n",
      "[Trial 5] Epoch 16/60, Training Loss: 0.1282, Validation Loss: 0.2246\n",
      "[Trial 4] Epoch 41/60, Training Loss: 0.1270, Validation Loss: 0.1303\n",
      "[Trial 3] Epoch 41/60, Training Loss: 0.1343, Validation Loss: 0.2244\n",
      "[Trial 7] Epoch 22/60, Training Loss: 0.1283, Validation Loss: 0.1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 10:53:16,580] Trial 6 finished with value: 0.12361051738262177 and parameters: {'hidden_dim': 320, 'latent_dim': 128, 'learning_rate': 0.013478292543993551, 'batch_size': 64, 'patience': 5}. Best is trial 6 with value: 0.12361051738262177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 6] Epoch 49/60, Training Loss: 0.1262, Validation Loss: 0.1236\n",
      "[Trial 6] Early stopping after 49 epochs.\n",
      "[Trial 7] Epoch 23/60, Training Loss: 0.1280, Validation Loss: 0.1305\n",
      "[Trial 4] Epoch 42/60, Training Loss: 0.1265, Validation Loss: 0.1299\n",
      "[Trial 3] Epoch 42/60, Training Loss: 0.1342, Validation Loss: 0.2241\n",
      "[Trial 7] Epoch 24/60, Training Loss: 0.1279, Validation Loss: 0.1308\n",
      "[Trial 2] Epoch 12/60, Training Loss: 0.1361, Validation Loss: 0.2256\n",
      "[Trial 1] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1301\n",
      "[Trial 8] Epoch 1/60, Training Loss: 0.8732, Validation Loss: 0.1475\n",
      "[Trial 4] Epoch 43/60, Training Loss: 0.1268, Validation Loss: 0.1299\n",
      "[Trial 7] Epoch 25/60, Training Loss: 0.1276, Validation Loss: 0.1300\n",
      "[Trial 3] Epoch 43/60, Training Loss: 0.1338, Validation Loss: 0.2241\n",
      "[Trial 5] Epoch 17/60, Training Loss: 0.1289, Validation Loss: 0.2230\n",
      "[Trial 7] Epoch 26/60, Training Loss: 0.1276, Validation Loss: 0.1298\n",
      "[Trial 4] Epoch 44/60, Training Loss: 0.1264, Validation Loss: 0.1297\n",
      "[Trial 3] Epoch 44/60, Training Loss: 0.1336, Validation Loss: 0.2227\n",
      "[Trial 7] Epoch 27/60, Training Loss: 0.1273, Validation Loss: 0.1296\n",
      "[Trial 8] Epoch 2/60, Training Loss: 0.1285, Validation Loss: 0.1239\n",
      "[Trial 1] Epoch 23/60, Training Loss: 0.1264, Validation Loss: 0.1293\n",
      "[Trial 4] Epoch 45/60, Training Loss: 0.1315, Validation Loss: 0.1304\n",
      "[Trial 7] Epoch 28/60, Training Loss: 0.1273, Validation Loss: 0.1295\n",
      "[Trial 3] Epoch 45/60, Training Loss: 0.1334, Validation Loss: 0.2222\n",
      "[Trial 7] Epoch 29/60, Training Loss: 0.1271, Validation Loss: 0.1295\n",
      "[Trial 8] Epoch 3/60, Training Loss: 0.1272, Validation Loss: 0.1235\n",
      "[Trial 4] Epoch 46/60, Training Loss: 0.1265, Validation Loss: 0.1295\n",
      "[Trial 5] Epoch 18/60, Training Loss: 0.1288, Validation Loss: 0.2236\n",
      "[Trial 3] Epoch 46/60, Training Loss: 0.1335, Validation Loss: 0.2213\n",
      "[Trial 2] Epoch 13/60, Training Loss: 0.1348, Validation Loss: 0.2240\n",
      "[Trial 7] Epoch 30/60, Training Loss: 0.1271, Validation Loss: 0.1294\n",
      "[Trial 1] Epoch 24/60, Training Loss: 0.1266, Validation Loss: 0.1291\n",
      "[Trial 7] Epoch 31/60, Training Loss: 0.1268, Validation Loss: 0.1290\n",
      "[Trial 4] Epoch 47/60, Training Loss: 0.1264, Validation Loss: 0.1294\n",
      "[Trial 3] Epoch 47/60, Training Loss: 0.1330, Validation Loss: 0.2206\n",
      "[Trial 8] Epoch 4/60, Training Loss: 0.1270, Validation Loss: 0.1236\n",
      "[Trial 7] Epoch 32/60, Training Loss: 0.1268, Validation Loss: 0.1290\n",
      "[Trial 4] Epoch 48/60, Training Loss: 0.1265, Validation Loss: 0.1289\n",
      "[Trial 3] Epoch 48/60, Training Loss: 0.1327, Validation Loss: 0.2207\n",
      "[Trial 7] Epoch 33/60, Training Loss: 0.1267, Validation Loss: 0.1294\n",
      "[Trial 1] Epoch 25/60, Training Loss: 0.1266, Validation Loss: 0.1286\n",
      "[Trial 5] Epoch 19/60, Training Loss: 0.1277, Validation Loss: 0.2187\n",
      "[Trial 7] Epoch 34/60, Training Loss: 0.1267, Validation Loss: 0.1289\n",
      "[Trial 4] Epoch 49/60, Training Loss: 0.1267, Validation Loss: 0.1287\n",
      "[Trial 8] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1235\n",
      "[Trial 3] Epoch 49/60, Training Loss: 0.1327, Validation Loss: 0.2202\n",
      "[Trial 7] Epoch 35/60, Training Loss: 0.1267, Validation Loss: 0.1285\n",
      "[Trial 2] Epoch 14/60, Training Loss: 0.1336, Validation Loss: 0.2230\n",
      "[Trial 4] Epoch 50/60, Training Loss: 0.1270, Validation Loss: 0.1287\n",
      "[Trial 7] Epoch 36/60, Training Loss: 0.1264, Validation Loss: 0.1285\n",
      "[Trial 3] Epoch 50/60, Training Loss: 0.1323, Validation Loss: 0.2192\n",
      "[Trial 1] Epoch 26/60, Training Loss: 0.1267, Validation Loss: 0.1287\n",
      "[Trial 8] Epoch 6/60, Training Loss: 0.1268, Validation Loss: 0.1237\n",
      "[Trial 7] Epoch 37/60, Training Loss: 0.1264, Validation Loss: 0.1287\n",
      "[Trial 5] Epoch 20/60, Training Loss: 0.1276, Validation Loss: 0.2178\n",
      "[Trial 4] Epoch 51/60, Training Loss: 0.1261, Validation Loss: 0.1286\n",
      "[Trial 3] Epoch 51/60, Training Loss: 0.1323, Validation Loss: 0.2184\n",
      "[Trial 7] Epoch 38/60, Training Loss: 0.1264, Validation Loss: 0.1286\n",
      "[Trial 7] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1286\n",
      "[Trial 4] Epoch 52/60, Training Loss: 0.1269, Validation Loss: 0.1287\n",
      "[Trial 8] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1238\n",
      "[Trial 3] Epoch 52/60, Training Loss: 0.1328, Validation Loss: 0.2176\n",
      "[Trial 1] Epoch 27/60, Training Loss: 0.1265, Validation Loss: 0.1274\n",
      "[Trial 7] Epoch 40/60, Training Loss: 0.1261, Validation Loss: 0.1285\n",
      "[Trial 4] Epoch 53/60, Training Loss: 0.1263, Validation Loss: 0.1287\n",
      "[Trial 3] Epoch 53/60, Training Loss: 0.1319, Validation Loss: 0.2171\n",
      "[Trial 7] Epoch 41/60, Training Loss: 0.1262, Validation Loss: 0.1284\n",
      "[Trial 2] Epoch 15/60, Training Loss: 0.1326, Validation Loss: 0.2215\n",
      "[Trial 5] Epoch 21/60, Training Loss: 0.1283, Validation Loss: 0.2166\n",
      "[Trial 8] Epoch 8/60, Training Loss: 0.1259, Validation Loss: 0.1234\n",
      "[Trial 7] Epoch 42/60, Training Loss: 0.1261, Validation Loss: 0.1283\n",
      "[Trial 4] Epoch 54/60, Training Loss: 0.1266, Validation Loss: 0.1285\n",
      "[Trial 3] Epoch 54/60, Training Loss: 0.1315, Validation Loss: 0.2167\n",
      "[Trial 1] Epoch 28/60, Training Loss: 0.1267, Validation Loss: 0.1270\n",
      "[Trial 7] Epoch 43/60, Training Loss: 0.1260, Validation Loss: 0.1283\n",
      "[Trial 4] Epoch 55/60, Training Loss: 0.1264, Validation Loss: 0.1286\n",
      "[Trial 3] Epoch 55/60, Training Loss: 0.1317, Validation Loss: 0.2176\n",
      "[Trial 7] Epoch 44/60, Training Loss: 0.1260, Validation Loss: 0.1282\n",
      "[Trial 8] Epoch 9/60, Training Loss: 0.1257, Validation Loss: 0.1234\n",
      "[Trial 7] Epoch 45/60, Training Loss: 0.1260, Validation Loss: 0.1283\n",
      "[Trial 4] Epoch 56/60, Training Loss: 0.1285, Validation Loss: 0.1286\n",
      "[Trial 5] Epoch 22/60, Training Loss: 0.1276, Validation Loss: 0.2144\n",
      "[Trial 1] Epoch 29/60, Training Loss: 0.1266, Validation Loss: 0.1278\n",
      "[Trial 3] Epoch 56/60, Training Loss: 0.1318, Validation Loss: 0.2160\n",
      "[Trial 7] Epoch 46/60, Training Loss: 0.1260, Validation Loss: 0.1280\n",
      "[Trial 8] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1243\n",
      "[Trial 4] Epoch 57/60, Training Loss: 0.1260, Validation Loss: 0.1283\n",
      "[Trial 2] Epoch 16/60, Training Loss: 0.1318, Validation Loss: 0.2205\n",
      "[Trial 7] Epoch 47/60, Training Loss: 0.1260, Validation Loss: 0.1283\n",
      "[Trial 3] Epoch 57/60, Training Loss: 0.1314, Validation Loss: 0.2151\n",
      "[Trial 1] Epoch 30/60, Training Loss: 0.1263, Validation Loss: 0.1271\n",
      "[Trial 7] Epoch 48/60, Training Loss: 0.1258, Validation Loss: 0.1282\n",
      "[Trial 4] Epoch 58/60, Training Loss: 0.1260, Validation Loss: 0.1282\n",
      "[Trial 3] Epoch 58/60, Training Loss: 0.1312, Validation Loss: 0.2149\n",
      "[Trial 7] Epoch 49/60, Training Loss: 0.1259, Validation Loss: 0.1281\n",
      "[Trial 5] Epoch 23/60, Training Loss: 0.1277, Validation Loss: 0.2206\n",
      "[Trial 8] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1236\n",
      "[Trial 4] Epoch 59/60, Training Loss: 0.1261, Validation Loss: 0.1287\n",
      "[Trial 7] Epoch 50/60, Training Loss: 0.1258, Validation Loss: 0.1282\n",
      "[Trial 3] Epoch 59/60, Training Loss: 0.1317, Validation Loss: 0.2140\n",
      "[Trial 1] Epoch 31/60, Training Loss: 0.1263, Validation Loss: 0.1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 10:57:04,032] Trial 7 finished with value: 0.12804880291223525 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0008329554974374463, 'batch_size': 64, 'patience': 5}. Best is trial 6 with value: 0.12361051738262177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 7] Epoch 51/60, Training Loss: 0.1258, Validation Loss: 0.1281\n",
      "[Trial 7] Early stopping after 51 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 10:57:06,741] Trial 4 finished with value: 0.12819329549868902 and parameters: {'hidden_dim': 512, 'latent_dim': 32, 'learning_rate': 0.0003332476839319116, 'batch_size': 64, 'patience': 9}. Best is trial 6 with value: 0.12361051738262177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 4] Epoch 60/60, Training Loss: 0.1263, Validation Loss: 0.1283\n",
      "[Trial 8] Epoch 12/60, Training Loss: 0.1263, Validation Loss: 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 10:57:09,504] Trial 3 finished with value: 0.2137864351272583 and parameters: {'hidden_dim': 512, 'latent_dim': 128, 'learning_rate': 0.00010774620386143657, 'batch_size': 64, 'patience': 3}. Best is trial 6 with value: 0.12361051738262177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 3] Epoch 60/60, Training Loss: 0.1316, Validation Loss: 0.2138\n",
      "[Trial 2] Epoch 17/60, Training Loss: 0.1314, Validation Loss: 0.2193\n",
      "[Trial 9] Epoch 1/60, Training Loss: 22.7033, Validation Loss: 4.5695\n",
      "[Trial 5] Epoch 24/60, Training Loss: 0.1280, Validation Loss: 0.2121\n",
      "[Trial 1] Epoch 32/60, Training Loss: 0.1262, Validation Loss: 0.1269\n",
      "[Trial 8] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1235\n",
      "[Trial 9] Epoch 2/60, Training Loss: 1.8238, Validation Loss: 0.8291\n",
      "[Trial 1] Epoch 33/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 9] Epoch 3/60, Training Loss: 0.6335, Validation Loss: 0.5814\n",
      "[Trial 5] Epoch 25/60, Training Loss: 0.1270, Validation Loss: 0.2131\n",
      "[Trial 8] Epoch 14/60, Training Loss: 0.1258, Validation Loss: 0.1234\n",
      "[Trial 11] Epoch 1/60, Training Loss: 0.3744, Validation Loss: 0.1415\n",
      "[Trial 10] Epoch 1/60, Training Loss: 0.2472, Validation Loss: 0.2265\n",
      "[Trial 2] Epoch 18/60, Training Loss: 0.1310, Validation Loss: 0.2181\n",
      "[Trial 9] Epoch 4/60, Training Loss: 0.4720, Validation Loss: 0.4847\n",
      "[Trial 8] Epoch 15/60, Training Loss: 0.1262, Validation Loss: 0.1234\n",
      "[Trial 1] Epoch 34/60, Training Loss: 0.1261, Validation Loss: 0.1263\n",
      "[Trial 9] Epoch 5/60, Training Loss: 0.3903, Validation Loss: 0.4283\n",
      "[Trial 5] Epoch 26/60, Training Loss: 0.1276, Validation Loss: 0.2111\n",
      "[Trial 9] Epoch 6/60, Training Loss: 0.3381, Validation Loss: 0.3898\n",
      "[Trial 8] Epoch 16/60, Training Loss: 0.1259, Validation Loss: 0.1235\n",
      "[Trial 11] Epoch 2/60, Training Loss: 0.1283, Validation Loss: 0.1252\n",
      "[Trial 1] Epoch 35/60, Training Loss: 0.1264, Validation Loss: 0.1265\n",
      "[Trial 10] Epoch 2/60, Training Loss: 0.1297, Validation Loss: 0.2235\n",
      "[Trial 9] Epoch 7/60, Training Loss: 0.3005, Validation Loss: 0.3621\n",
      "[Trial 2] Epoch 19/60, Training Loss: 0.1306, Validation Loss: 0.2172\n",
      "[Trial 5] Epoch 27/60, Training Loss: 0.1273, Validation Loss: 0.2101\n",
      "[Trial 8] Epoch 17/60, Training Loss: 0.1258, Validation Loss: 0.1234\n",
      "[Trial 9] Epoch 8/60, Training Loss: 0.2732, Validation Loss: 0.3411\n",
      "[Trial 1] Epoch 36/60, Training Loss: 0.1261, Validation Loss: 0.1261\n",
      "[Trial 8] Epoch 18/60, Training Loss: 0.1262, Validation Loss: 0.1238\n",
      "[Trial 11] Epoch 3/60, Training Loss: 0.1273, Validation Loss: 0.1251\n",
      "[Trial 9] Epoch 9/60, Training Loss: 0.2523, Validation Loss: 0.3250\n",
      "[Trial 5] Epoch 28/60, Training Loss: 0.1270, Validation Loss: 0.2094\n",
      "[Trial 1] Epoch 37/60, Training Loss: 0.1265, Validation Loss: 0.1265\n",
      "[Trial 10] Epoch 3/60, Training Loss: 0.1283, Validation Loss: 0.2074\n",
      "[Trial 9] Epoch 10/60, Training Loss: 0.2355, Validation Loss: 0.3119\n",
      "[Trial 2] Epoch 20/60, Training Loss: 0.1300, Validation Loss: 0.2166\n",
      "[Trial 8] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1234\n",
      "[Trial 9] Epoch 11/60, Training Loss: 0.2229, Validation Loss: 0.3016\n",
      "[Trial 5] Epoch 29/60, Training Loss: 0.1271, Validation Loss: 0.2073\n",
      "[Trial 1] Epoch 38/60, Training Loss: 0.1266, Validation Loss: 0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 10:59:21,184] Trial 8 finished with value: 0.12338427988191446 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.012185893836109393, 'batch_size': 16, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 8] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1235\n",
      "[Trial 8] Early stopping after 20 epochs.\n",
      "[Trial 11] Epoch 4/60, Training Loss: 0.1273, Validation Loss: 0.1255\n",
      "[Trial 9] Epoch 12/60, Training Loss: 0.2125, Validation Loss: 0.2932\n",
      "[Trial 10] Epoch 4/60, Training Loss: 0.1279, Validation Loss: 0.1978\n",
      "[Trial 1] Epoch 39/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 9] Epoch 13/60, Training Loss: 0.2040, Validation Loss: 0.2861\n",
      "[Trial 2] Epoch 21/60, Training Loss: 0.1297, Validation Loss: 0.2152\n",
      "[Trial 5] Epoch 30/60, Training Loss: 0.1268, Validation Loss: 0.2066\n",
      "[Trial 9] Epoch 14/60, Training Loss: 0.1969, Validation Loss: 0.2800\n",
      "[Trial 11] Epoch 5/60, Training Loss: 0.1269, Validation Loss: 0.1251\n",
      "[Trial 1] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 9] Epoch 15/60, Training Loss: 0.1910, Validation Loss: 0.2744\n",
      "[Trial 12] Epoch 1/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 5] Epoch 31/60, Training Loss: 0.1269, Validation Loss: 0.2052\n",
      "[Trial 9] Epoch 16/60, Training Loss: 0.1858, Validation Loss: 0.2699\n",
      "[Trial 10] Epoch 5/60, Training Loss: 0.1281, Validation Loss: 0.1878\n",
      "[Trial 2] Epoch 22/60, Training Loss: 0.1295, Validation Loss: 0.2140\n",
      "[Trial 1] Epoch 41/60, Training Loss: 0.1260, Validation Loss: 0.1256\n",
      "[Trial 9] Epoch 17/60, Training Loss: 0.1811, Validation Loss: 0.2652\n",
      "[Trial 5] Epoch 32/60, Training Loss: 0.1271, Validation Loss: 0.2039\n",
      "[Trial 11] Epoch 6/60, Training Loss: 0.1271, Validation Loss: 0.1253\n",
      "[Trial 9] Epoch 18/60, Training Loss: 0.1768, Validation Loss: 0.2614\n",
      "[Trial 1] Epoch 42/60, Training Loss: 0.1260, Validation Loss: 0.1254\n",
      "[Trial 12] Epoch 2/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 9] Epoch 19/60, Training Loss: 0.1730, Validation Loss: 0.2579\n",
      "[Trial 2] Epoch 23/60, Training Loss: 0.1295, Validation Loss: 0.2137\n",
      "[Trial 5] Epoch 33/60, Training Loss: 0.1266, Validation Loss: 0.2048\n",
      "[Trial 10] Epoch 6/60, Training Loss: 0.1273, Validation Loss: 0.1786\n",
      "[Trial 1] Epoch 43/60, Training Loss: 0.1262, Validation Loss: 0.1264\n",
      "[Trial 9] Epoch 20/60, Training Loss: 0.1696, Validation Loss: 0.2545\n",
      "[Trial 11] Epoch 7/60, Training Loss: 0.1268, Validation Loss: 0.1254\n",
      "[Trial 9] Epoch 21/60, Training Loss: 0.1667, Validation Loss: 0.2519\n",
      "[Trial 1] Epoch 44/60, Training Loss: 0.1263, Validation Loss: 0.1253\n",
      "[Trial 5] Epoch 34/60, Training Loss: 0.1268, Validation Loss: 0.2014\n",
      "[Trial 12] Epoch 3/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 9] Epoch 22/60, Training Loss: 0.1642, Validation Loss: 0.2491\n",
      "[Trial 2] Epoch 24/60, Training Loss: 0.1291, Validation Loss: 0.2126\n",
      "[Trial 10] Epoch 7/60, Training Loss: 0.1274, Validation Loss: 0.1705\n",
      "[Trial 9] Epoch 23/60, Training Loss: 0.1617, Validation Loss: 0.2462\n",
      "[Trial 1] Epoch 45/60, Training Loss: 0.1264, Validation Loss: 0.1257\n",
      "[Trial 11] Epoch 8/60, Training Loss: 0.1270, Validation Loss: 0.1255\n",
      "[Trial 5] Epoch 35/60, Training Loss: 0.1265, Validation Loss: 0.2004\n",
      "[Trial 9] Epoch 24/60, Training Loss: 0.1594, Validation Loss: 0.2442\n",
      "[Trial 1] Epoch 46/60, Training Loss: 0.1263, Validation Loss: 0.1258\n",
      "[Trial 9] Epoch 25/60, Training Loss: 0.1572, Validation Loss: 0.2417\n",
      "[Trial 12] Epoch 4/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 5] Epoch 36/60, Training Loss: 0.1270, Validation Loss: 0.1987\n",
      "[Trial 2] Epoch 25/60, Training Loss: 0.1287, Validation Loss: 0.2117\n",
      "[Trial 10] Epoch 8/60, Training Loss: 0.1270, Validation Loss: 0.1639\n",
      "[Trial 9] Epoch 26/60, Training Loss: 0.1555, Validation Loss: 0.2398\n",
      "[Trial 11] Epoch 9/60, Training Loss: 0.1284, Validation Loss: 0.1252\n",
      "[Trial 1] Epoch 47/60, Training Loss: 0.1264, Validation Loss: 0.1258\n",
      "[Trial 9] Epoch 27/60, Training Loss: 0.1536, Validation Loss: 0.2376\n",
      "[Trial 5] Epoch 37/60, Training Loss: 0.1267, Validation Loss: 0.1989\n",
      "[Trial 9] Epoch 28/60, Training Loss: 0.1519, Validation Loss: 0.2359\n",
      "[Trial 1] Epoch 48/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 12] Epoch 5/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 2] Epoch 26/60, Training Loss: 0.1288, Validation Loss: 0.2115\n",
      "[Trial 11] Epoch 10/60, Training Loss: 0.1271, Validation Loss: 0.1252\n",
      "[Trial 9] Epoch 29/60, Training Loss: 0.1506, Validation Loss: 0.2343\n",
      "[Trial 10] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1573\n",
      "[Trial 5] Epoch 38/60, Training Loss: 0.1268, Validation Loss: 0.1970\n",
      "[Trial 1] Epoch 49/60, Training Loss: 0.1260, Validation Loss: 0.1254\n",
      "[Trial 9] Epoch 30/60, Training Loss: 0.1494, Validation Loss: 0.2325\n",
      "[Trial 9] Epoch 31/60, Training Loss: 0.1482, Validation Loss: 0.2314\n",
      "[Trial 1] Epoch 50/60, Training Loss: 0.1263, Validation Loss: 0.1254\n",
      "[Trial 11] Epoch 11/60, Training Loss: 0.1263, Validation Loss: 0.1252\n",
      "[Trial 5] Epoch 39/60, Training Loss: 0.1267, Validation Loss: 0.1965\n",
      "[Trial 2] Epoch 27/60, Training Loss: 0.1285, Validation Loss: 0.2105\n",
      "[Trial 9] Epoch 32/60, Training Loss: 0.1469, Validation Loss: 0.2298\n",
      "[Trial 12] Epoch 6/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 10] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1522\n",
      "[Trial 9] Epoch 33/60, Training Loss: 0.1458, Validation Loss: 0.2283\n",
      "[Trial 1] Epoch 51/60, Training Loss: 0.1261, Validation Loss: 0.1259\n",
      "[Trial 5] Epoch 40/60, Training Loss: 0.1269, Validation Loss: 0.1941\n",
      "[Trial 9] Epoch 34/60, Training Loss: 0.1447, Validation Loss: 0.2270\n",
      "[Trial 11] Epoch 12/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 9] Epoch 35/60, Training Loss: 0.1438, Validation Loss: 0.2261\n",
      "[Trial 1] Epoch 52/60, Training Loss: 0.1261, Validation Loss: 0.1254\n",
      "[Trial 2] Epoch 28/60, Training Loss: 0.1284, Validation Loss: 0.2097\n",
      "[Trial 12] Epoch 7/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 10] Epoch 11/60, Training Loss: 0.1267, Validation Loss: 0.1486\n",
      "[Trial 9] Epoch 36/60, Training Loss: 0.1428, Validation Loss: 0.2240\n",
      "[Trial 5] Epoch 41/60, Training Loss: 0.1265, Validation Loss: 0.1940\n",
      "[Trial 1] Epoch 53/60, Training Loss: 0.1259, Validation Loss: 0.1254\n",
      "[Trial 9] Epoch 37/60, Training Loss: 0.1421, Validation Loss: 0.2233\n",
      "[Trial 11] Epoch 13/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 9] Epoch 38/60, Training Loss: 0.1413, Validation Loss: 0.2224\n",
      "[Trial 5] Epoch 42/60, Training Loss: 0.1265, Validation Loss: 0.1934\n",
      "[Trial 2] Epoch 29/60, Training Loss: 0.1282, Validation Loss: 0.2089\n",
      "[Trial 1] Epoch 54/60, Training Loss: 0.1259, Validation Loss: 0.1254\n",
      "[Trial 10] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1431\n",
      "[Trial 12] Epoch 8/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 9] Epoch 39/60, Training Loss: 0.1408, Validation Loss: 0.2214\n",
      "[Trial 9] Epoch 40/60, Training Loss: 0.1399, Validation Loss: 0.2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:04:58,611] Trial 1 finished with value: 0.12518160069982212 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.00022629003655799652, 'batch_size': 16, 'patience': 7}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1] Epoch 55/60, Training Loss: 0.1261, Validation Loss: 0.1253\n",
      "[Trial 1] Early stopping after 55 epochs.\n",
      "[Trial 5] Epoch 43/60, Training Loss: 0.1265, Validation Loss: 0.1924\n",
      "[Trial 11] Epoch 14/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 13] Epoch 1/60, Training Loss: 5727422.5610, Validation Loss: 70.9895\n",
      "[Trial 9] Epoch 41/60, Training Loss: 0.1393, Validation Loss: 0.2192\n",
      "[Trial 2] Epoch 30/60, Training Loss: 0.1281, Validation Loss: 0.2084\n",
      "[Trial 13] Epoch 2/60, Training Loss: 108.6956, Validation Loss: 5.8101\n",
      "[Trial 9] Epoch 42/60, Training Loss: 0.1385, Validation Loss: 0.2182\n",
      "[Trial 10] Epoch 13/60, Training Loss: 0.1264, Validation Loss: 0.1396\n",
      "[Trial 5] Epoch 44/60, Training Loss: 0.1268, Validation Loss: 0.1903\n",
      "[Trial 13] Epoch 3/60, Training Loss: 22.3172, Validation Loss: 2.0154\n",
      "[Trial 12] Epoch 9/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 9] Epoch 43/60, Training Loss: 0.1382, Validation Loss: 0.2173\n",
      "[Trial 11] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 13] Epoch 4/60, Training Loss: 8.5413, Validation Loss: 0.5358\n",
      "[Trial 13] Epoch 5/60, Training Loss: 4.6558, Validation Loss: 0.4788\n",
      "[Trial 9] Epoch 44/60, Training Loss: 0.1378, Validation Loss: 0.2168\n",
      "[Trial 2] Epoch 31/60, Training Loss: 0.1280, Validation Loss: 0.2076\n",
      "[Trial 5] Epoch 45/60, Training Loss: 0.1268, Validation Loss: 0.1913\n",
      "[Trial 13] Epoch 6/60, Training Loss: 2.8330, Validation Loss: 0.4528\n",
      "[Trial 9] Epoch 45/60, Training Loss: 0.1372, Validation Loss: 0.2158\n",
      "[Trial 13] Epoch 7/60, Training Loss: 2.0411, Validation Loss: 0.4104\n",
      "[Trial 10] Epoch 14/60, Training Loss: 0.1262, Validation Loss: 0.1371\n",
      "[Trial 11] Epoch 16/60, Training Loss: 0.1258, Validation Loss: 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:06:16,268] Trial 12 finished with value: inf and parameters: {'hidden_dim': 512, 'latent_dim': 64, 'learning_rate': 0.06887698572930995, 'batch_size': 8, 'patience': 10}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 12] Epoch 10/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 12] Early stopping after 10 epochs.\n",
      "[Trial 9] Epoch 46/60, Training Loss: 0.1365, Validation Loss: 0.2148\n",
      "[Trial 13] Epoch 8/60, Training Loss: 1.6000, Validation Loss: 0.3952\n",
      "[Trial 5] Epoch 46/60, Training Loss: 0.1265, Validation Loss: 0.1992\n",
      "[Trial 13] Epoch 9/60, Training Loss: 1.1097, Validation Loss: 0.4181\n",
      "[Trial 14] Epoch 1/60, Training Loss: 48.7466, Validation Loss: 0.2067\n",
      "[Trial 9] Epoch 47/60, Training Loss: 0.1361, Validation Loss: 0.2138\n",
      "[Trial 2] Epoch 32/60, Training Loss: 0.1281, Validation Loss: 0.2069\n",
      "[Trial 13] Epoch 10/60, Training Loss: 0.9181, Validation Loss: 0.3125\n",
      "[Trial 14] Epoch 2/60, Training Loss: 0.1288, Validation Loss: 0.1785\n",
      "[Trial 9] Epoch 48/60, Training Loss: 0.1358, Validation Loss: 0.2132\n",
      "[Trial 13] Epoch 11/60, Training Loss: 0.7352, Validation Loss: 0.2865\n",
      "[Trial 10] Epoch 15/60, Training Loss: 0.1262, Validation Loss: 0.1345\n",
      "[Trial 11] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1251\n",
      "[Trial 5] Epoch 47/60, Training Loss: 0.1273, Validation Loss: 0.1880\n",
      "[Trial 14] Epoch 3/60, Training Loss: 0.1287, Validation Loss: 0.1607\n",
      "[Trial 9] Epoch 49/60, Training Loss: 0.1353, Validation Loss: 0.2121\n",
      "[Trial 13] Epoch 12/60, Training Loss: 0.6624, Validation Loss: 0.2662\n",
      "[Trial 14] Epoch 4/60, Training Loss: 0.1274, Validation Loss: 0.1467\n",
      "[Trial 13] Epoch 13/60, Training Loss: 0.6402, Validation Loss: 0.2521\n",
      "[Trial 9] Epoch 50/60, Training Loss: 0.1350, Validation Loss: 0.2115\n",
      "[Trial 2] Epoch 33/60, Training Loss: 0.1280, Validation Loss: 0.2067\n",
      "[Trial 13] Epoch 14/60, Training Loss: 0.5041, Validation Loss: 0.2294\n",
      "[Trial 14] Epoch 5/60, Training Loss: 0.1312, Validation Loss: 0.1351\n",
      "[Trial 5] Epoch 48/60, Training Loss: 0.1265, Validation Loss: 0.1867\n",
      "[Trial 9] Epoch 51/60, Training Loss: 0.1346, Validation Loss: 0.2109\n",
      "[Trial 13] Epoch 15/60, Training Loss: 0.4778, Validation Loss: 0.2218\n",
      "[Trial 11] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 14] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1294\n",
      "[Trial 10] Epoch 16/60, Training Loss: 0.1261, Validation Loss: 0.1323\n",
      "[Trial 9] Epoch 52/60, Training Loss: 0.1340, Validation Loss: 0.2101\n",
      "[Trial 13] Epoch 16/60, Training Loss: 0.4100, Validation Loss: 0.2023\n",
      "[Trial 14] Epoch 7/60, Training Loss: 0.1265, Validation Loss: 0.1262\n",
      "[Trial 13] Epoch 17/60, Training Loss: 0.5427, Validation Loss: 0.4981\n",
      "[Trial 5] Epoch 49/60, Training Loss: 0.1261, Validation Loss: 0.1859\n",
      "[Trial 9] Epoch 53/60, Training Loss: 0.1339, Validation Loss: 0.2095\n",
      "[Trial 2] Epoch 34/60, Training Loss: 0.1278, Validation Loss: 0.2058\n",
      "[Trial 13] Epoch 18/60, Training Loss: 0.9577, Validation Loss: 0.1868\n",
      "[Trial 14] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 9] Epoch 54/60, Training Loss: 0.1334, Validation Loss: 0.2086\n",
      "[Trial 11] Epoch 19/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 13] Epoch 19/60, Training Loss: 0.4146, Validation Loss: 0.1833\n",
      "[Trial 14] Epoch 9/60, Training Loss: 0.1298, Validation Loss: 0.1253\n",
      "[Trial 10] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1308\n",
      "[Trial 5] Epoch 50/60, Training Loss: 0.1265, Validation Loss: 0.1848\n",
      "[Trial 9] Epoch 55/60, Training Loss: 0.1333, Validation Loss: 0.2080\n",
      "[Trial 13] Epoch 20/60, Training Loss: 1.0963, Validation Loss: 0.1756\n",
      "[Trial 14] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 21/60, Training Loss: 0.2903, Validation Loss: 0.1687\n",
      "[Trial 9] Epoch 56/60, Training Loss: 0.1330, Validation Loss: 0.2075\n",
      "[Trial 14] Epoch 11/60, Training Loss: 0.1261, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 22/60, Training Loss: 1.0795, Validation Loss: 0.2185\n",
      "[Trial 2] Epoch 35/60, Training Loss: 0.1278, Validation Loss: 0.2051\n",
      "[Trial 11] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 9] Epoch 57/60, Training Loss: 0.1328, Validation Loss: 0.2066\n",
      "[Trial 5] Epoch 51/60, Training Loss: 0.1263, Validation Loss: 0.1846\n",
      "[Trial 13] Epoch 23/60, Training Loss: 0.2696, Validation Loss: 0.1590\n",
      "[Trial 14] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1253\n",
      "[Trial 9] Epoch 58/60, Training Loss: 0.1325, Validation Loss: 0.2061\n",
      "[Trial 13] Epoch 24/60, Training Loss: 0.3964, Validation Loss: 0.1583\n",
      "[Trial 10] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1293\n",
      "[Trial 14] Epoch 13/60, Training Loss: 0.1604, Validation Loss: 0.1263\n",
      "[Trial 13] Epoch 25/60, Training Loss: 0.7552, Validation Loss: 0.1559\n",
      "[Trial 9] Epoch 59/60, Training Loss: 0.1322, Validation Loss: 0.2051\n",
      "[Trial 5] Epoch 52/60, Training Loss: 0.1264, Validation Loss: 0.1833\n",
      "[Trial 14] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 26/60, Training Loss: 0.2404, Validation Loss: 0.1839\n",
      "[Trial 2] Epoch 36/60, Training Loss: 0.1274, Validation Loss: 0.2048\n",
      "[Trial 11] Epoch 21/60, Training Loss: 0.1257, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:09:11,370] Trial 9 finished with value: 0.20469248543183008 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 2.884779625422642e-05, 'batch_size': 32, 'patience': 3}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 9] Epoch 60/60, Training Loss: 0.1320, Validation Loss: 0.2047\n",
      "[Trial 14] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 27/60, Training Loss: 2.9230, Validation Loss: 2.1119\n",
      "[Trial 15] Epoch 1/60, Training Loss: 2.9024, Validation Loss: 0.1885\n",
      "[Trial 13] Epoch 28/60, Training Loss: 0.5356, Validation Loss: 0.1559\n",
      "[Trial 5] Epoch 53/60, Training Loss: 0.1264, Validation Loss: 0.1830\n",
      "[Trial 10] Epoch 19/60, Training Loss: 0.1258, Validation Loss: 0.1286\n",
      "[Trial 14] Epoch 16/60, Training Loss: 0.1266, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 2/60, Training Loss: 0.1357, Validation Loss: 0.1572\n",
      "[Trial 13] Epoch 29/60, Training Loss: 0.1856, Validation Loss: 0.1448\n",
      "[Trial 14] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 3/60, Training Loss: 0.1306, Validation Loss: 0.1417\n",
      "[Trial 11] Epoch 22/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 13] Epoch 30/60, Training Loss: 0.1929, Validation Loss: 0.1442\n",
      "[Trial 2] Epoch 37/60, Training Loss: 0.1277, Validation Loss: 0.2038\n",
      "[Trial 14] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1253\n",
      "[Trial 5] Epoch 54/60, Training Loss: 0.1261, Validation Loss: 0.1810\n",
      "[Trial 13] Epoch 31/60, Training Loss: 0.1751, Validation Loss: 0.1420\n",
      "[Trial 15] Epoch 4/60, Training Loss: 0.1290, Validation Loss: 0.1350\n",
      "[Trial 13] Epoch 32/60, Training Loss: 0.1718, Validation Loss: 0.1424\n",
      "[Trial 14] Epoch 19/60, Training Loss: 0.1269, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 5/60, Training Loss: 0.1281, Validation Loss: 0.1289\n",
      "[Trial 10] Epoch 20/60, Training Loss: 0.1259, Validation Loss: 0.1276\n",
      "[Trial 13] Epoch 33/60, Training Loss: 0.1679, Validation Loss: 0.1415\n",
      "[Trial 15] Epoch 6/60, Training Loss: 0.1275, Validation Loss: 0.1272\n",
      "[Trial 14] Epoch 20/60, Training Loss: 0.1261, Validation Loss: 0.1253\n",
      "[Trial 11] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 5] Epoch 55/60, Training Loss: 0.1266, Validation Loss: 0.1795\n",
      "[Trial 13] Epoch 34/60, Training Loss: 0.1911, Validation Loss: 0.1392\n",
      "[Trial 2] Epoch 38/60, Training Loss: 0.1276, Validation Loss: 0.2035\n",
      "[Trial 15] Epoch 7/60, Training Loss: 0.1272, Validation Loss: 0.1260\n",
      "[Trial 14] Epoch 21/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 35/60, Training Loss: 0.5790, Validation Loss: 0.4285\n",
      "[Trial 15] Epoch 8/60, Training Loss: 0.1271, Validation Loss: 0.1260\n",
      "[Trial 14] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 36/60, Training Loss: 1.5062, Validation Loss: 0.1481\n",
      "[Trial 5] Epoch 56/60, Training Loss: 0.1259, Validation Loss: 0.1791\n",
      "[Trial 10] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1275\n",
      "[Trial 15] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1248\n",
      "[Trial 13] Epoch 37/60, Training Loss: 0.1602, Validation Loss: 0.1371\n",
      "[Trial 14] Epoch 23/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 11] Epoch 24/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 15] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1245\n",
      "[Trial 13] Epoch 38/60, Training Loss: 0.1721, Validation Loss: 0.1378\n",
      "[Trial 2] Epoch 39/60, Training Loss: 0.1272, Validation Loss: 0.2031\n",
      "[Trial 14] Epoch 24/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 39/60, Training Loss: 0.1552, Validation Loss: 0.1358\n",
      "[Trial 15] Epoch 11/60, Training Loss: 0.1266, Validation Loss: 0.1245\n",
      "[Trial 5] Epoch 57/60, Training Loss: 0.1262, Validation Loss: 0.1788\n",
      "[Trial 14] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 40/60, Training Loss: 0.1780, Validation Loss: 0.1354\n",
      "[Trial 15] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1240\n",
      "[Trial 10] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1270\n",
      "[Trial 13] Epoch 41/60, Training Loss: 1.1424, Validation Loss: 0.1694\n",
      "[Trial 11] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1250\n",
      "[Trial 14] Epoch 26/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1239\n",
      "[Trial 13] Epoch 42/60, Training Loss: 0.9677, Validation Loss: 0.1553\n",
      "[Trial 5] Epoch 58/60, Training Loss: 0.1262, Validation Loss: 0.1781\n",
      "[Trial 14] Epoch 27/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 2] Epoch 40/60, Training Loss: 0.1272, Validation Loss: 0.2024\n",
      "[Trial 15] Epoch 14/60, Training Loss: 0.1266, Validation Loss: 0.1245\n",
      "[Trial 13] Epoch 43/60, Training Loss: 0.1868, Validation Loss: 0.1386\n",
      "[Trial 15] Epoch 15/60, Training Loss: 0.1267, Validation Loss: 0.1239\n",
      "[Trial 14] Epoch 28/60, Training Loss: 0.1256, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 44/60, Training Loss: 0.1481, Validation Loss: 0.1342\n",
      "[Trial 11] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 15] Epoch 16/60, Training Loss: 0.1263, Validation Loss: 0.1240\n",
      "[Trial 5] Epoch 59/60, Training Loss: 0.1263, Validation Loss: 0.1770\n",
      "[Trial 13] Epoch 45/60, Training Loss: 0.1556, Validation Loss: 0.1340\n",
      "[Trial 14] Epoch 29/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 10] Epoch 23/60, Training Loss: 0.1255, Validation Loss: 0.1273\n",
      "[Trial 15] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1244\n",
      "[Trial 13] Epoch 46/60, Training Loss: 0.1451, Validation Loss: 0.1339\n",
      "[Trial 14] Epoch 30/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 2] Epoch 41/60, Training Loss: 0.1273, Validation Loss: 0.2019\n",
      "[Trial 13] Epoch 47/60, Training Loss: 0.1490, Validation Loss: 0.1335\n",
      "[Trial 15] Epoch 18/60, Training Loss: 0.1264, Validation Loss: 0.1247\n",
      "[Trial 14] Epoch 31/60, Training Loss: 0.1254, Validation Loss: 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:12:29,491] Trial 5 finished with value: 0.17697854315241177 and parameters: {'hidden_dim': 448, 'latent_dim': 128, 'learning_rate': 0.00014892883157847765, 'batch_size': 16, 'patience': 5}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 5] Epoch 60/60, Training Loss: 0.1262, Validation Loss: 0.1773\n",
      "[Trial 13] Epoch 48/60, Training Loss: 0.1453, Validation Loss: 0.1334\n",
      "[Trial 15] Epoch 19/60, Training Loss: 0.1262, Validation Loss: 0.1239\n",
      "[Trial 11] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 14] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 49/60, Training Loss: 0.1437, Validation Loss: 0.1332\n",
      "[Trial 10] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1266\n",
      "[Trial 15] Epoch 20/60, Training Loss: 0.1261, Validation Loss: 0.1238\n",
      "[Trial 16] Epoch 1/60, Training Loss: 1.1188, Validation Loss: 0.1713\n",
      "[Trial 13] Epoch 50/60, Training Loss: 0.1418, Validation Loss: 0.1329\n",
      "[Trial 14] Epoch 33/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 15] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1238\n",
      "[Trial 2] Epoch 42/60, Training Loss: 0.1274, Validation Loss: 0.2014\n",
      "[Trial 13] Epoch 51/60, Training Loss: 0.1481, Validation Loss: 0.1330\n",
      "[Trial 14] Epoch 34/60, Training Loss: 0.1256, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1237\n",
      "[Trial 16] Epoch 2/60, Training Loss: 0.1291, Validation Loss: 0.1446\n",
      "[Trial 13] Epoch 52/60, Training Loss: 0.1473, Validation Loss: 0.1326\n",
      "[Trial 11] Epoch 28/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 14] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1241[Trial 13] Epoch 53/60, Training Loss: 0.1357, Validation Loss: 0.1325\n",
      "\n",
      "[Trial 10] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1266\n",
      "[Trial 13] Epoch 54/60, Training Loss: 0.1402, Validation Loss: 0.1323\n",
      "[Trial 14] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 16] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1296\n",
      "[Trial 15] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1237\n",
      "[Trial 2] Epoch 43/60, Training Loss: 0.1271, Validation Loss: 0.2004\n",
      "[Trial 13] Epoch 55/60, Training Loss: 0.1386, Validation Loss: 0.1321\n",
      "[Trial 14] Epoch 37/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 15] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1239\n",
      "[Trial 13] Epoch 56/60, Training Loss: 0.1363, Validation Loss: 0.1319\n",
      "[Trial 11] Epoch 29/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 16] Epoch 4/60, Training Loss: 0.1262, Validation Loss: 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:13:49,912] Trial 15 finished with value: 0.12371130958199501 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.00226806972195251, 'batch_size': 32, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 15] Epoch 26/60, Training Loss: 0.1259, Validation Loss: 0.1238\n",
      "[Trial 15] Early stopping after 26 epochs.\n",
      "[Trial 14] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1253\n",
      "[Trial 13] Epoch 57/60, Training Loss: 0.1399, Validation Loss: 0.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:14:02,284] Trial 14 finished with value: 0.1252399963637193 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.01174946786204616, 'batch_size': 32, 'patience': 9}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 10] Epoch 26/60, Training Loss: 0.1255, Validation Loss: 0.1264\n",
      "[Trial 14] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 14] Early stopping after 39 epochs.\n",
      "[Trial 13] Epoch 58/60, Training Loss: 0.1417, Validation Loss: 0.1316\n",
      "[Trial 16] Epoch 5/60, Training Loss: 0.1260, Validation Loss: 0.1265\n",
      "[Trial 13] Epoch 59/60, Training Loss: 0.1380, Validation Loss: 0.1315\n",
      "[Trial 2] Epoch 44/60, Training Loss: 0.1269, Validation Loss: 0.2000\n",
      "[Trial 17] Epoch 1/60, Training Loss: 2951640497452.0923, Validation Loss: 237.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:14:21,907] Trial 13 finished with value: 0.1313388466835022 and parameters: {'hidden_dim': 384, 'latent_dim': 128, 'learning_rate': 0.017689424441510015, 'batch_size': 64, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 13] Epoch 60/60, Training Loss: 0.1418, Validation Loss: 0.1313\n",
      "[Trial 11] Epoch 30/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 18] Epoch 1/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 16] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1353\n",
      "[Trial 17] Epoch 2/60, Training Loss: 378.8627, Validation Loss: 296.8287\n",
      "[Trial 10] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1264\n",
      "[Trial 19] Epoch 1/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 16] Epoch 7/60, Training Loss: 0.1277, Validation Loss: 0.1263\n",
      "[Trial 18] Epoch 2/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 2] Epoch 45/60, Training Loss: 0.1272, Validation Loss: 0.1996\n",
      "[Trial 11] Epoch 31/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 17] Epoch 3/60, Training Loss: 381.6370, Validation Loss: 231.7981\n",
      "[Trial 16] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1264\n",
      "[Trial 19] Epoch 2/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 18] Epoch 3/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 10] Epoch 28/60, Training Loss: 0.1254, Validation Loss: 0.1264\n",
      "[Trial 17] Epoch 4/60, Training Loss: 385.7245, Validation Loss: 451.6664\n",
      "[Trial 16] Epoch 9/60, Training Loss: 0.1266, Validation Loss: 0.1513\n",
      "[Trial 19] Epoch 3/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 2] Epoch 46/60, Training Loss: 0.1271, Validation Loss: 0.1989\n",
      "[Trial 18] Epoch 4/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 11] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 16] Epoch 10/60, Training Loss: 0.1293, Validation Loss: 0.1263\n",
      "[Trial 17] Epoch 5/60, Training Loss: 375.1482, Validation Loss: 310.9774\n",
      "[Trial 19] Epoch 4/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 18] Epoch 5/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 10] Epoch 29/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 16] Epoch 11/60, Training Loss: 0.1255, Validation Loss: 0.1265\n",
      "[Trial 11] Epoch 33/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 2] Epoch 47/60, Training Loss: 0.1269, Validation Loss: 0.1987\n",
      "[Trial 17] Epoch 6/60, Training Loss: 363.2280, Validation Loss: 220.1629\n",
      "[Trial 19] Epoch 5/60, Training Loss: nan, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:16:09,947] Trial 18 finished with value: inf and parameters: {'hidden_dim': 384, 'latent_dim': 96, 'learning_rate': 0.02683019155579849, 'batch_size': 16, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 18] Epoch 6/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 18] Early stopping after 6 epochs.\n",
      "[Trial 16] Epoch 12/60, Training Loss: 0.1257, Validation Loss: 0.1263\n",
      "[Trial 17] Epoch 7/60, Training Loss: 371.8645, Validation Loss: 288.3321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:16:28,355] Trial 19 finished with value: inf and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.07459176771208186, 'batch_size': 16, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 19] Epoch 6/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 19] Early stopping after 6 epochs.\n",
      "[Trial 10] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 11] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 16] Epoch 13/60, Training Loss: 0.1255, Validation Loss: 0.1263\n",
      "[Trial 2] Epoch 48/60, Training Loss: 0.1267, Validation Loss: 0.1976\n",
      "[Trial 20] Epoch 1/60, Training Loss: 0.8180, Validation Loss: 0.1887\n",
      "[Trial 17] Epoch 8/60, Training Loss: 363.7259, Validation Loss: 265.3778\n",
      "[Trial 16] Epoch 14/60, Training Loss: 0.1254, Validation Loss: 0.1262\n",
      "[Trial 21] Epoch 1/60, Training Loss: 0.3386, Validation Loss: 0.1735\n",
      "[Trial 11] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 10] Epoch 31/60, Training Loss: 0.1255, Validation Loss: 0.1266\n",
      "[Trial 17] Epoch 9/60, Training Loss: 356.6054, Validation Loss: 250.9288\n",
      "[Trial 2] Epoch 49/60, Training Loss: 0.1269, Validation Loss: 0.1967\n",
      "[Trial 16] Epoch 15/60, Training Loss: 0.1251, Validation Loss: 0.1262\n",
      "[Trial 20] Epoch 2/60, Training Loss: 0.1330, Validation Loss: 0.1424\n",
      "[Trial 17] Epoch 10/60, Training Loss: 352.9800, Validation Loss: 245.6199\n",
      "[Trial 16] Epoch 16/60, Training Loss: 0.1251, Validation Loss: 0.1263\n",
      "[Trial 21] Epoch 2/60, Training Loss: 0.1305, Validation Loss: 0.1289\n",
      "[Trial 11] Epoch 36/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 10] Epoch 32/60, Training Loss: 0.1252, Validation Loss: 0.1264\n",
      "[Trial 2] Epoch 50/60, Training Loss: 0.1270, Validation Loss: 0.1966\n",
      "[Trial 16] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1262\n",
      "[Trial 17] Epoch 11/60, Training Loss: 353.0217, Validation Loss: 268.8012\n",
      "[Trial 20] Epoch 3/60, Training Loss: 0.1314, Validation Loss: 0.1308\n",
      "[Trial 21] Epoch 3/60, Training Loss: 0.1283, Validation Loss: 0.1250\n",
      "[Trial 16] Epoch 18/60, Training Loss: 0.1256, Validation Loss: 0.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:18:12,988] Trial 17 finished with value: 220.16294225056967 and parameters: {'hidden_dim': 384, 'latent_dim': 96, 'learning_rate': 0.022398296828546778, 'batch_size': 16, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 17] Epoch 12/60, Training Loss: 349.3630, Validation Loss: 253.5818\n",
      "[Trial 17] Early stopping after 12 epochs.\n",
      "[Trial 11] Epoch 37/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 10] Epoch 33/60, Training Loss: 0.1252, Validation Loss: 0.1264\n",
      "[Trial 2] Epoch 51/60, Training Loss: 0.1271, Validation Loss: 0.1960\n",
      "[Trial 16] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1262\n",
      "[Trial 20] Epoch 4/60, Training Loss: 0.1294, Validation Loss: 0.1292\n",
      "[Trial 21] Epoch 4/60, Training Loss: 0.1282, Validation Loss: 0.1248\n",
      "[Trial 22] Epoch 1/60, Training Loss: 0.2548, Validation Loss: 0.1663\n",
      "[Trial 16] Epoch 20/60, Training Loss: 0.1254, Validation Loss: 0.1263\n",
      "[Trial 11] Epoch 38/60, Training Loss: 0.1253, Validation Loss: 0.1250\n",
      "[Trial 10] Epoch 34/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 2] Epoch 52/60, Training Loss: 0.1267, Validation Loss: 0.1955\n",
      "[Trial 20] Epoch 5/60, Training Loss: 0.1292, Validation Loss: 0.1290\n",
      "[Trial 16] Epoch 21/60, Training Loss: 0.1253, Validation Loss: 0.1262\n",
      "[Trial 21] Epoch 5/60, Training Loss: 0.1283, Validation Loss: 0.1247\n",
      "[Trial 22] Epoch 2/60, Training Loss: 0.1297, Validation Loss: 0.1309\n",
      "[Trial 11] Epoch 39/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 16] Epoch 22/60, Training Loss: 0.1253, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 53/60, Training Loss: 0.1268, Validation Loss: 0.1952\n",
      "[Trial 10] Epoch 35/60, Training Loss: 0.1253, Validation Loss: 0.1264\n",
      "[Trial 20] Epoch 6/60, Training Loss: 0.1296, Validation Loss: 0.1309\n",
      "[Trial 16] Epoch 23/60, Training Loss: 0.1250, Validation Loss: 0.1262\n",
      "[Trial 21] Epoch 6/60, Training Loss: 0.1279, Validation Loss: 0.1249\n",
      "[Trial 11] Epoch 40/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 22] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1253\n",
      "[Trial 16] Epoch 24/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 54/60, Training Loss: 0.1270, Validation Loss: 0.1939\n",
      "[Trial 10] Epoch 36/60, Training Loss: 0.1251, Validation Loss: 0.1265\n",
      "[Trial 20] Epoch 7/60, Training Loss: 0.1282, Validation Loss: 0.1293\n",
      "[Trial 16] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1262\n",
      "[Trial 21] Epoch 7/60, Training Loss: 0.1274, Validation Loss: 0.1250\n",
      "[Trial 11] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1250\n",
      "[Trial 22] Epoch 4/60, Training Loss: 0.1276, Validation Loss: 0.1247\n",
      "[Trial 16] Epoch 26/60, Training Loss: 0.1252, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 55/60, Training Loss: 0.1267, Validation Loss: 0.1937\n",
      "[Trial 10] Epoch 37/60, Training Loss: 0.1253, Validation Loss: 0.1264\n",
      "[Trial 20] Epoch 8/60, Training Loss: 0.1287, Validation Loss: 0.1293\n",
      "[Trial 21] Epoch 8/60, Training Loss: 0.1301, Validation Loss: 0.1247\n",
      "[Trial 16] Epoch 27/60, Training Loss: 0.1249, Validation Loss: 0.1262\n",
      "[Trial 11] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 22] Epoch 5/60, Training Loss: 0.1275, Validation Loss: 0.1247\n",
      "[Trial 16] Epoch 28/60, Training Loss: 0.1252, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 56/60, Training Loss: 0.1268, Validation Loss: 0.1933\n",
      "[Trial 20] Epoch 9/60, Training Loss: 0.1277, Validation Loss: 0.1298\n",
      "[Trial 10] Epoch 38/60, Training Loss: 0.1252, Validation Loss: 0.1263\n",
      "[Trial 21] Epoch 9/60, Training Loss: 0.1269, Validation Loss: 0.1248\n",
      "[Trial 11] Epoch 43/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 16] Epoch 29/60, Training Loss: 0.1252, Validation Loss: 0.1262\n",
      "[Trial 22] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1249\n",
      "[Trial 16] Epoch 30/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 57/60, Training Loss: 0.1267, Validation Loss: 0.1927\n",
      "[Trial 20] Epoch 10/60, Training Loss: 0.1257, Validation Loss: 0.1289\n",
      "[Trial 10] Epoch 39/60, Training Loss: 0.1252, Validation Loss: 0.1264\n",
      "[Trial 21] Epoch 10/60, Training Loss: 0.1260, Validation Loss: 0.1247\n",
      "[Trial 11] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 22] Epoch 7/60, Training Loss: 0.1384, Validation Loss: 0.1273\n",
      "[Trial 16] Epoch 31/60, Training Loss: 0.1253, Validation Loss: 0.1262\n",
      "[Trial 16] Epoch 32/60, Training Loss: 0.1254, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 58/60, Training Loss: 0.1267, Validation Loss: 0.1920\n",
      "[Trial 20] Epoch 11/60, Training Loss: 0.1251, Validation Loss: 0.1289\n",
      "[Trial 10] Epoch 40/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 21] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1247\n",
      "[Trial 11] Epoch 45/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 22] Epoch 8/60, Training Loss: 0.1271, Validation Loss: 0.1247\n",
      "[Trial 16] Epoch 33/60, Training Loss: 0.1252, Validation Loss: 0.1262\n",
      "[Trial 2] Epoch 59/60, Training Loss: 0.1266, Validation Loss: 0.1913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:23:01,249] Trial 16 finished with value: 0.1261973281701406 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.006964789865080077, 'batch_size': 16, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 16] Epoch 34/60, Training Loss: 0.1253, Validation Loss: 0.1262\n",
      "[Trial 16] Early stopping after 34 epochs.\n",
      "[Trial 20] Epoch 12/60, Training Loss: 0.1251, Validation Loss: 0.1293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:23:04,969] Trial 21 finished with value: 0.12467839612315099 and parameters: {'hidden_dim': 128, 'latent_dim': 96, 'learning_rate': 0.006817116519395748, 'batch_size': 8, 'patience': 7}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 21] Epoch 12/60, Training Loss: 0.1258, Validation Loss: 0.1248\n",
      "[Trial 21] Early stopping after 12 epochs.\n",
      "[Trial 11] Epoch 46/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 10] Epoch 41/60, Training Loss: 0.1251, Validation Loss: 0.1264\n",
      "[Trial 22] Epoch 9/60, Training Loss: 0.1271, Validation Loss: 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:23:34,377] Trial 2 finished with value: 0.19126425919433435 and parameters: {'hidden_dim': 192, 'latent_dim': 128, 'learning_rate': 3.117197407632978e-05, 'batch_size': 8, 'patience': 10}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 2] Epoch 60/60, Training Loss: 0.1269, Validation Loss: 0.1913\n",
      "[Trial 20] Epoch 13/60, Training Loss: 0.1261, Validation Loss: 0.1292\n",
      "[Trial 23] Epoch 1/60, Training Loss: 0.1907, Validation Loss: 0.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:23:38,116] Trial 11 finished with value: 0.12503580475846926 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.00797383311344546, 'batch_size': 8, 'patience': 10}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 11] Epoch 47/60, Training Loss: 0.1253, Validation Loss: 0.1250\n",
      "[Trial 11] Early stopping after 47 epochs.\n",
      "[Trial 24] Epoch 1/60, Training Loss: 0.1778, Validation Loss: 0.2058\n",
      "[Trial 25] Epoch 1/60, Training Loss: 3.2372, Validation Loss: 0.2386\n",
      "[Trial 22] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1247\n",
      "[Trial 10] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1263\n",
      "[Trial 26] Epoch 1/60, Training Loss: 2.7041, Validation Loss: 0.2241\n",
      "[Trial 25] Epoch 2/60, Training Loss: 0.1397, Validation Loss: 0.2185\n",
      "[Trial 26] Epoch 2/60, Training Loss: 0.1364, Validation Loss: 0.2089\n",
      "[Trial 25] Epoch 3/60, Training Loss: 0.1334, Validation Loss: 0.2071\n",
      "[Trial 26] Epoch 3/60, Training Loss: 0.1315, Validation Loss: 0.2008\n",
      "[Trial 25] Epoch 4/60, Training Loss: 0.1302, Validation Loss: 0.1962\n",
      "[Trial 26] Epoch 4/60, Training Loss: 0.1296, Validation Loss: 0.1924\n",
      "[Trial 20] Epoch 14/60, Training Loss: 0.1253, Validation Loss: 0.1292\n",
      "[Trial 25] Epoch 5/60, Training Loss: 0.1292, Validation Loss: 0.1916\n",
      "[Trial 23] Epoch 2/60, Training Loss: 0.1295, Validation Loss: 0.1634\n",
      "[Trial 24] Epoch 2/60, Training Loss: 0.1304, Validation Loss: 0.1995\n",
      "[Trial 22] Epoch 11/60, Training Loss: 0.1267, Validation Loss: 0.1249\n",
      "[Trial 26] Epoch 5/60, Training Loss: 0.1284, Validation Loss: 0.1847\n",
      "[Trial 10] Epoch 43/60, Training Loss: 0.1256, Validation Loss: 0.1264\n",
      "[Trial 25] Epoch 6/60, Training Loss: 0.1288, Validation Loss: 0.1833\n",
      "[Trial 26] Epoch 6/60, Training Loss: 0.1283, Validation Loss: 0.1806\n",
      "[Trial 25] Epoch 7/60, Training Loss: 0.1279, Validation Loss: 0.1813\n",
      "[Trial 26] Epoch 7/60, Training Loss: 0.1280, Validation Loss: 0.1730\n",
      "[Trial 25] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1742\n",
      "[Trial 26] Epoch 8/60, Training Loss: 0.1272, Validation Loss: 0.1678\n",
      "[Trial 25] Epoch 9/60, Training Loss: 0.1272, Validation Loss: 0.1658\n",
      "[Trial 20] Epoch 15/60, Training Loss: 0.1249, Validation Loss: 0.1291\n",
      "[Trial 22] Epoch 12/60, Training Loss: 0.1265, Validation Loss: 0.1247\n",
      "[Trial 26] Epoch 9/60, Training Loss: 0.1270, Validation Loss: 0.1614\n",
      "[Trial 24] Epoch 3/60, Training Loss: 0.1294, Validation Loss: 0.1911\n",
      "[Trial 23] Epoch 3/60, Training Loss: 0.1283, Validation Loss: 0.1402\n",
      "[Trial 25] Epoch 10/60, Training Loss: 0.1272, Validation Loss: 0.1629\n",
      "[Trial 10] Epoch 44/60, Training Loss: 0.1252, Validation Loss: 0.1264\n",
      "[Trial 26] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1576\n",
      "[Trial 25] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1552\n",
      "[Trial 26] Epoch 11/60, Training Loss: 0.1269, Validation Loss: 0.1564\n",
      "[Trial 25] Epoch 12/60, Training Loss: 0.1267, Validation Loss: 0.1519\n",
      "[Trial 26] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1517\n",
      "[Trial 25] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1491\n",
      "[Trial 22] Epoch 13/60, Training Loss: 0.1265, Validation Loss: 0.1253\n",
      "[Trial 26] Epoch 13/60, Training Loss: 0.1264, Validation Loss: 0.1462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:25:32,901] Trial 20 finished with value: 0.12891395427286625 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.005783946985621786, 'batch_size': 8, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 20] Epoch 16/60, Training Loss: 0.1248, Validation Loss: 0.1290\n",
      "[Trial 20] Early stopping after 16 epochs.\n",
      "[Trial 25] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1472\n",
      "[Trial 24] Epoch 4/60, Training Loss: 0.1290, Validation Loss: 0.1728\n",
      "[Trial 26] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1459\n",
      "[Trial 10] Epoch 45/60, Training Loss: 0.1253, Validation Loss: 0.1263\n",
      "[Trial 23] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1303\n",
      "[Trial 27] Epoch 1/60, Training Loss: 1.7803, Validation Loss: 0.2209\n",
      "[Trial 25] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1446\n",
      "[Trial 26] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1450\n",
      "[Trial 27] Epoch 2/60, Training Loss: 0.1366, Validation Loss: 0.1966\n",
      "[Trial 25] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1430\n",
      "[Trial 26] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1441\n",
      "[Trial 25] Epoch 17/60, Training Loss: 0.1263, Validation Loss: 0.1403\n",
      "[Trial 27] Epoch 3/60, Training Loss: 0.1314, Validation Loss: 0.1828\n",
      "[Trial 22] Epoch 14/60, Training Loss: 0.1256, Validation Loss: 0.1247\n",
      "[Trial 26] Epoch 17/60, Training Loss: 0.1269, Validation Loss: 0.1405\n",
      "[Trial 25] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1395\n",
      "[Trial 27] Epoch 4/60, Training Loss: 0.1299, Validation Loss: 0.1711\n",
      "[Trial 26] Epoch 18/60, Training Loss: 0.1264, Validation Loss: 0.1412\n",
      "[Trial 24] Epoch 5/60, Training Loss: 0.1284, Validation Loss: 0.1605\n",
      "[Trial 10] Epoch 46/60, Training Loss: 0.1252, Validation Loss: 0.1263\n",
      "[Trial 25] Epoch 19/60, Training Loss: 0.1259, Validation Loss: 0.1374\n",
      "[Trial 23] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1269\n",
      "[Trial 26] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1396\n",
      "[Trial 27] Epoch 5/60, Training Loss: 0.1287, Validation Loss: 0.1621\n",
      "[Trial 25] Epoch 20/60, Training Loss: 0.1259, Validation Loss: 0.1377\n",
      "[Trial 26] Epoch 20/60, Training Loss: 0.1260, Validation Loss: 0.1378\n",
      "[Trial 27] Epoch 6/60, Training Loss: 0.1283, Validation Loss: 0.1526\n",
      "[Trial 25] Epoch 21/60, Training Loss: 0.1268, Validation Loss: 0.1366\n",
      "[Trial 22] Epoch 15/60, Training Loss: 0.1258, Validation Loss: 0.1247\n",
      "[Trial 26] Epoch 21/60, Training Loss: 0.1264, Validation Loss: 0.1359\n",
      "[Trial 27] Epoch 7/60, Training Loss: 0.1279, Validation Loss: 0.1464\n",
      "[Trial 25] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1353\n",
      "[Trial 26] Epoch 22/60, Training Loss: 0.1261, Validation Loss: 0.1348\n",
      "[Trial 24] Epoch 6/60, Training Loss: 0.1280, Validation Loss: 0.1495\n",
      "[Trial 25] Epoch 23/60, Training Loss: 0.1262, Validation Loss: 0.1344\n",
      "[Trial 27] Epoch 8/60, Training Loss: 0.1277, Validation Loss: 0.1429\n",
      "[Trial 10] Epoch 47/60, Training Loss: 0.1252, Validation Loss: 0.1263\n",
      "[Trial 26] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1340\n",
      "[Trial 23] Epoch 6/60, Training Loss: 0.1269, Validation Loss: 0.1262\n",
      "[Trial 25] Epoch 24/60, Training Loss: 0.1260, Validation Loss: 0.1338\n",
      "[Trial 26] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1338\n",
      "[Trial 27] Epoch 9/60, Training Loss: 0.1273, Validation Loss: 0.1383\n",
      "[Trial 25] Epoch 25/60, Training Loss: 0.1260, Validation Loss: 0.1347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:27:17,834] Trial 22 finished with value: 0.12466715313494206 and parameters: {'hidden_dim': 128, 'latent_dim': 96, 'learning_rate': 0.005156441278683721, 'batch_size': 8, 'patience': 7}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 22] Epoch 16/60, Training Loss: 0.1258, Validation Loss: 0.1247\n",
      "[Trial 22] Early stopping after 16 epochs.\n",
      "[Trial 26] Epoch 25/60, Training Loss: 0.1258, Validation Loss: 0.1341\n",
      "[Trial 27] Epoch 10/60, Training Loss: 0.1271, Validation Loss: 0.1372\n",
      "[Trial 25] Epoch 26/60, Training Loss: 0.1266, Validation Loss: 0.1335\n",
      "[Trial 26] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1326\n",
      "[Trial 28] Epoch 1/60, Training Loss: 1.6331, Validation Loss: 0.2250\n",
      "[Trial 27] Epoch 11/60, Training Loss: 0.1268, Validation Loss: 0.1317\n",
      "[Trial 25] Epoch 27/60, Training Loss: 0.1263, Validation Loss: 0.1333\n",
      "[Trial 26] Epoch 27/60, Training Loss: 0.1260, Validation Loss: 0.1320\n",
      "[Trial 24] Epoch 7/60, Training Loss: 0.1280, Validation Loss: 0.1395\n",
      "[Trial 10] Epoch 48/60, Training Loss: 0.1251, Validation Loss: 0.1263\n",
      "[Trial 28] Epoch 2/60, Training Loss: 0.1359, Validation Loss: 0.2037\n",
      "[Trial 25] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1333\n",
      "[Trial 27] Epoch 12/60, Training Loss: 0.1268, Validation Loss: 0.1333\n",
      "[Trial 26] Epoch 28/60, Training Loss: 0.1258, Validation Loss: 0.1312\n",
      "[Trial 23] Epoch 7/60, Training Loss: 0.1318, Validation Loss: 0.1261\n",
      "[Trial 25] Epoch 29/60, Training Loss: 0.1256, Validation Loss: 0.1320\n",
      "[Trial 27] Epoch 13/60, Training Loss: 0.1270, Validation Loss: 0.1298\n",
      "[Trial 28] Epoch 3/60, Training Loss: 0.1310, Validation Loss: 0.1913\n",
      "[Trial 26] Epoch 29/60, Training Loss: 0.1259, Validation Loss: 0.1313\n",
      "[Trial 25] Epoch 30/60, Training Loss: 0.1260, Validation Loss: 0.1315\n",
      "[Trial 26] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1303\n",
      "[Trial 27] Epoch 14/60, Training Loss: 0.1266, Validation Loss: 0.1312\n",
      "[Trial 28] Epoch 4/60, Training Loss: 0.1297, Validation Loss: 0.1801\n",
      "[Trial 25] Epoch 31/60, Training Loss: 0.1257, Validation Loss: 0.1306\n",
      "[Trial 26] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1303\n",
      "[Trial 27] Epoch 15/60, Training Loss: 0.1266, Validation Loss: 0.1293\n",
      "[Trial 28] Epoch 5/60, Training Loss: 0.1283, Validation Loss: 0.1701\n",
      "[Trial 24] Epoch 8/60, Training Loss: 0.1274, Validation Loss: 0.1352\n",
      "[Trial 25] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1304\n",
      "[Trial 10] Epoch 49/60, Training Loss: 0.1251, Validation Loss: 0.1263\n",
      "[Trial 26] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1297\n",
      "[Trial 27] Epoch 16/60, Training Loss: 0.1266, Validation Loss: 0.1288\n",
      "[Trial 25] Epoch 33/60, Training Loss: 0.1258, Validation Loss: 0.1310\n",
      "[Trial 23] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1261\n",
      "[Trial 28] Epoch 6/60, Training Loss: 0.1278, Validation Loss: 0.1616\n",
      "[Trial 26] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1297\n",
      "[Trial 25] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1300\n",
      "[Trial 27] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1276\n",
      "[Trial 26] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1297\n",
      "[Trial 28] Epoch 7/60, Training Loss: 0.1276, Validation Loss: 0.1543\n",
      "[Trial 25] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1304\n",
      "[Trial 26] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1290\n",
      "[Trial 27] Epoch 18/60, Training Loss: 0.1264, Validation Loss: 0.1275\n",
      "[Trial 28] Epoch 8/60, Training Loss: 0.1271, Validation Loss: 0.1498\n",
      "[Trial 25] Epoch 36/60, Training Loss: 0.1260, Validation Loss: 0.1305\n",
      "[Trial 24] Epoch 9/60, Training Loss: 0.1277, Validation Loss: 0.1332\n",
      "[Trial 26] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1293\n",
      "[Trial 10] Epoch 50/60, Training Loss: 0.1253, Validation Loss: 0.1264\n",
      "[Trial 27] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1266\n",
      "[Trial 25] Epoch 37/60, Training Loss: 0.1253, Validation Loss: 0.1302\n",
      "[Trial 28] Epoch 9/60, Training Loss: 0.1268, Validation Loss: 0.1451\n",
      "[Trial 26] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1290\n",
      "[Trial 23] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1260\n",
      "[Trial 27] Epoch 20/60, Training Loss: 0.1266, Validation Loss: 0.1261\n",
      "[Trial 25] Epoch 38/60, Training Loss: 0.1251, Validation Loss: 0.1291\n",
      "[Trial 28] Epoch 10/60, Training Loss: 0.1267, Validation Loss: 0.1400\n",
      "[Trial 26] Epoch 38/60, Training Loss: 0.1254, Validation Loss: 0.1293\n",
      "[Trial 25] Epoch 39/60, Training Loss: 0.1252, Validation Loss: 0.1298\n",
      "[Trial 27] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1258\n",
      "[Trial 26] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1289\n",
      "[Trial 28] Epoch 11/60, Training Loss: 0.1265, Validation Loss: 0.1397\n",
      "[Trial 25] Epoch 40/60, Training Loss: 0.1251, Validation Loss: 0.1292\n",
      "[Trial 26] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1284\n",
      "[Trial 27] Epoch 22/60, Training Loss: 0.1266, Validation Loss: 0.1258\n",
      "[Trial 24] Epoch 10/60, Training Loss: 0.1276, Validation Loss: 0.1282\n",
      "[Trial 10] Epoch 51/60, Training Loss: 0.1250, Validation Loss: 0.1264\n",
      "[Trial 25] Epoch 41/60, Training Loss: 0.1252, Validation Loss: 0.1291\n",
      "[Trial 28] Epoch 12/60, Training Loss: 0.1267, Validation Loss: 0.1357\n",
      "[Trial 26] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1285\n",
      "[Trial 27] Epoch 23/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 25] Epoch 42/60, Training Loss: 0.1252, Validation Loss: 0.1293\n",
      "[Trial 23] Epoch 10/60, Training Loss: 0.1266, Validation Loss: 0.1260\n",
      "[Trial 28] Epoch 13/60, Training Loss: 0.1275, Validation Loss: 0.1359\n",
      "[Trial 26] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1283\n",
      "[Trial 27] Epoch 24/60, Training Loss: 0.1268, Validation Loss: 0.1252\n",
      "[Trial 25] Epoch 43/60, Training Loss: 0.1250, Validation Loss: 0.1291\n",
      "[Trial 26] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1284\n",
      "[Trial 28] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1334\n",
      "[Trial 25] Epoch 44/60, Training Loss: 0.1250, Validation Loss: 0.1286\n",
      "[Trial 27] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1249\n",
      "[Trial 26] Epoch 44/60, Training Loss: 0.1256, Validation Loss: 0.1285\n",
      "[Trial 24] Epoch 11/60, Training Loss: 0.1273, Validation Loss: 0.1276\n",
      "[Trial 25] Epoch 45/60, Training Loss: 0.1250, Validation Loss: 0.1291\n",
      "[Trial 28] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1321\n",
      "[Trial 10] Epoch 52/60, Training Loss: 0.1251, Validation Loss: 0.1264\n",
      "[Trial 26] Epoch 45/60, Training Loss: 0.1259, Validation Loss: 0.1277\n",
      "[Trial 27] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1257\n",
      "[Trial 25] Epoch 46/60, Training Loss: 0.1250, Validation Loss: 0.1289\n",
      "[Trial 28] Epoch 16/60, Training Loss: 0.1269, Validation Loss: 0.1350\n",
      "[Trial 26] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1279\n",
      "[Trial 27] Epoch 27/60, Training Loss: 0.1277, Validation Loss: 0.1249\n",
      "[Trial 23] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1261\n",
      "[Trial 25] Epoch 47/60, Training Loss: 0.1250, Validation Loss: 0.1291\n",
      "[Trial 26] Epoch 47/60, Training Loss: 0.1255, Validation Loss: 0.1279\n",
      "[Trial 28] Epoch 17/60, Training Loss: 0.1270, Validation Loss: 0.1301\n",
      "[Trial 27] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1248\n",
      "[Trial 25] Epoch 48/60, Training Loss: 0.1249, Validation Loss: 0.1288\n",
      "[Trial 26] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:30:53,761] Trial 25 finished with value: 0.12857869664827984 and parameters: {'hidden_dim': 320, 'latent_dim': 64, 'learning_rate': 0.0019443908637972985, 'batch_size': 64, 'patience': 5}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 25] Epoch 49/60, Training Loss: 0.1249, Validation Loss: 0.1286\n",
      "[Trial 25] Early stopping after 49 epochs.\n",
      "[Trial 28] Epoch 18/60, Training Loss: 0.1261, Validation Loss: 0.1305\n",
      "[Trial 27] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1248\n",
      "[Trial 24] Epoch 12/60, Training Loss: 0.1272, Validation Loss: 0.1245\n",
      "[Trial 26] Epoch 49/60, Training Loss: 0.1254, Validation Loss: 0.1279\n",
      "[Trial 10] Epoch 53/60, Training Loss: 0.1251, Validation Loss: 0.1263\n",
      "[Trial 29] Epoch 1/60, Training Loss: 2.2286, Validation Loss: 0.2592\n",
      "[Trial 28] Epoch 19/60, Training Loss: 0.1262, Validation Loss: 0.1296\n",
      "[Trial 26] Epoch 50/60, Training Loss: 0.1254, Validation Loss: 0.1276\n",
      "[Trial 27] Epoch 30/60, Training Loss: 0.1266, Validation Loss: 0.1248\n",
      "[Trial 26] Epoch 51/60, Training Loss: 0.1251, Validation Loss: 0.1277\n",
      "[Trial 29] Epoch 2/60, Training Loss: 0.1489, Validation Loss: 0.2280\n",
      "[Trial 23] Epoch 12/60, Training Loss: 0.1260, Validation Loss: 0.1263\n",
      "[Trial 28] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1309\n",
      "[Trial 27] Epoch 31/60, Training Loss: 0.1262, Validation Loss: 0.1251\n",
      "[Trial 26] Epoch 52/60, Training Loss: 0.1252, Validation Loss: 0.1275\n",
      "[Trial 27] Epoch 32/60, Training Loss: 0.1260, Validation Loss: 0.1246\n",
      "[Trial 29] Epoch 3/60, Training Loss: 0.1384, Validation Loss: 0.2109\n",
      "[Trial 28] Epoch 21/60, Training Loss: 0.1269, Validation Loss: 0.1288\n",
      "[Trial 26] Epoch 53/60, Training Loss: 0.1253, Validation Loss: 0.1276\n",
      "[Trial 24] Epoch 13/60, Training Loss: 0.1289, Validation Loss: 0.1239\n",
      "[Trial 27] Epoch 33/60, Training Loss: 0.1260, Validation Loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:31:39,444] Trial 10 finished with value: 0.12633044527222712 and parameters: {'hidden_dim': 192, 'latent_dim': 96, 'learning_rate': 0.0011337878525403866, 'batch_size': 8, 'patience': 6}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 29] Epoch 4/60, Training Loss: 0.1343, Validation Loss: 0.1968\n",
      "[Trial 10] Epoch 54/60, Training Loss: 0.1250, Validation Loss: 0.1264\n",
      "[Trial 10] Early stopping after 54 epochs.\n",
      "[Trial 26] Epoch 54/60, Training Loss: 0.1252, Validation Loss: 0.1275\n",
      "[Trial 28] Epoch 22/60, Training Loss: 0.1260, Validation Loss: 0.1285\n",
      "[Trial 26] Epoch 55/60, Training Loss: 0.1252, Validation Loss: 0.1275\n",
      "[Trial 27] Epoch 34/60, Training Loss: 0.1260, Validation Loss: 0.1246\n",
      "[Trial 29] Epoch 5/60, Training Loss: 0.1319, Validation Loss: 0.1855\n",
      "[Trial 30] Epoch 1/60, Training Loss: 55016.2935, Validation Loss: 0.4634\n",
      "[Trial 28] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1288\n",
      "[Trial 26] Epoch 56/60, Training Loss: 0.1253, Validation Loss: 0.1274\n",
      "[Trial 23] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1260\n",
      "[Trial 27] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 29] Epoch 6/60, Training Loss: 0.1306, Validation Loss: 0.1777\n",
      "[Trial 30] Epoch 2/60, Training Loss: 1.3260, Validation Loss: 0.5399\n",
      "[Trial 28] Epoch 24/60, Training Loss: 0.1264, Validation Loss: 0.1277\n",
      "[Trial 26] Epoch 57/60, Training Loss: 0.1253, Validation Loss: 0.1274\n",
      "[Trial 27] Epoch 36/60, Training Loss: 0.1260, Validation Loss: 0.1245\n",
      "[Trial 24] Epoch 14/60, Training Loss: 0.1269, Validation Loss: 0.1232\n",
      "[Trial 29] Epoch 7/60, Training Loss: 0.1295, Validation Loss: 0.1679\n",
      "[Trial 26] Epoch 58/60, Training Loss: 0.1251, Validation Loss: 0.1274\n",
      "[Trial 30] Epoch 3/60, Training Loss: 0.4597, Validation Loss: 0.1864\n",
      "[Trial 28] Epoch 25/60, Training Loss: 0.1266, Validation Loss: 0.1283\n",
      "[Trial 27] Epoch 37/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 26] Epoch 59/60, Training Loss: 0.1252, Validation Loss: 0.1274\n",
      "[Trial 29] Epoch 8/60, Training Loss: 0.1290, Validation Loss: 0.1578\n",
      "[Trial 28] Epoch 26/60, Training Loss: 0.1260, Validation Loss: 0.1280\n",
      "[Trial 30] Epoch 4/60, Training Loss: 0.6138, Validation Loss: 0.1674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:32:31,907] Trial 26 finished with value: 0.12734540204207104 and parameters: {'hidden_dim': 320, 'latent_dim': 64, 'learning_rate': 0.0023177774715956133, 'batch_size': 64, 'patience': 5}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 26] Epoch 60/60, Training Loss: 0.1252, Validation Loss: 0.1273\n",
      "[Trial 27] Epoch 38/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 29] Epoch 9/60, Training Loss: 0.1284, Validation Loss: 0.1539\n",
      "[Trial 28] Epoch 27/60, Training Loss: 0.1260, Validation Loss: 0.1273\n",
      "[Trial 23] Epoch 14/60, Training Loss: 0.1263, Validation Loss: 0.1262\n",
      "[Trial 30] Epoch 5/60, Training Loss: 0.2545, Validation Loss: 0.1563\n",
      "[Trial 31] Epoch 1/60, Training Loss: 848202.3684, Validation Loss: 2.3504\n",
      "[Trial 27] Epoch 39/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 29] Epoch 10/60, Training Loss: 0.1282, Validation Loss: 0.1486\n",
      "[Trial 28] Epoch 28/60, Training Loss: 0.1263, Validation Loss: 0.1272\n",
      "[Trial 30] Epoch 6/60, Training Loss: 0.2288, Validation Loss: 0.1678\n",
      "[Trial 24] Epoch 15/60, Training Loss: 0.1269, Validation Loss: 0.1234\n",
      "[Trial 31] Epoch 2/60, Training Loss: 6.4259, Validation Loss: 0.4512\n",
      "[Trial 27] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 29] Epoch 11/60, Training Loss: 0.1280, Validation Loss: 0.1445\n",
      "[Trial 28] Epoch 29/60, Training Loss: 0.1268, Validation Loss: 0.1270\n",
      "[Trial 30] Epoch 7/60, Training Loss: 0.2162, Validation Loss: 0.1461\n",
      "[Trial 27] Epoch 41/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 31] Epoch 3/60, Training Loss: 1.5468, Validation Loss: 0.2874\n",
      "[Trial 29] Epoch 12/60, Training Loss: 0.1275, Validation Loss: 0.1419\n",
      "[Trial 28] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1271\n",
      "[Trial 30] Epoch 8/60, Training Loss: 0.2000, Validation Loss: 0.1432\n",
      "[Trial 27] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 31] Epoch 4/60, Training Loss: 0.8253, Validation Loss: 0.2171\n",
      "[Trial 23] Epoch 15/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 29] Epoch 13/60, Training Loss: 0.1275, Validation Loss: 0.1387\n",
      "[Trial 28] Epoch 31/60, Training Loss: 0.1267, Validation Loss: 0.1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:33:26,070] Trial 27 finished with value: 0.12439037511746089 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'learning_rate': 0.0016661604342968748, 'batch_size': 32, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 27] Epoch 43/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 27] Early stopping after 43 epochs.\n",
      "[Trial 30] Epoch 9/60, Training Loss: 0.1903, Validation Loss: 0.1405\n",
      "[Trial 31] Epoch 5/60, Training Loss: 0.5123, Validation Loss: 0.1807\n",
      "[Trial 24] Epoch 16/60, Training Loss: 0.1270, Validation Loss: 0.1235\n",
      "[Trial 29] Epoch 14/60, Training Loss: 0.1276, Validation Loss: 0.1364\n",
      "[Trial 28] Epoch 32/60, Training Loss: 0.1259, Validation Loss: 0.1273\n",
      "[Trial 32] Epoch 1/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 30] Epoch 10/60, Training Loss: 7.7312, Validation Loss: 31.4726\n",
      "[Trial 31] Epoch 6/60, Training Loss: 0.3583, Validation Loss: 0.1624\n",
      "[Trial 29] Epoch 15/60, Training Loss: 0.1275, Validation Loss: 0.1360\n",
      "[Trial 28] Epoch 33/60, Training Loss: 0.1253, Validation Loss: 0.1267\n",
      "[Trial 32] Epoch 2/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 31] Epoch 7/60, Training Loss: 0.3432, Validation Loss: 0.1544\n",
      "[Trial 30] Epoch 11/60, Training Loss: 1.7605, Validation Loss: 0.1329\n",
      "[Trial 29] Epoch 16/60, Training Loss: 0.1279, Validation Loss: 0.1346\n",
      "[Trial 23] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 28] Epoch 34/60, Training Loss: 0.1254, Validation Loss: 0.1272\n",
      "[Trial 32] Epoch 3/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 31] Epoch 8/60, Training Loss: 0.2655, Validation Loss: 0.1475\n",
      "[Trial 30] Epoch 12/60, Training Loss: 0.1515, Validation Loss: 0.1287\n",
      "[Trial 24] Epoch 17/60, Training Loss: 0.1270, Validation Loss: 0.1227\n",
      "[Trial 29] Epoch 17/60, Training Loss: 0.1270, Validation Loss: 0.1348\n",
      "[Trial 28] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:34:11,211] Trial 32 finished with value: inf and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.03682078837876353, 'batch_size': 32, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 32] Epoch 4/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 32] Early stopping after 4 epochs.\n",
      "[Trial 31] Epoch 9/60, Training Loss: 0.2090, Validation Loss: 0.1437\n",
      "[Trial 30] Epoch 13/60, Training Loss: 0.1469, Validation Loss: 0.1276\n",
      "[Trial 29] Epoch 18/60, Training Loss: 0.1274, Validation Loss: 0.1316\n",
      "[Trial 28] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1266\n",
      "[Trial 33] Epoch 1/60, Training Loss: 1.0582, Validation Loss: 0.2214\n",
      "[Trial 31] Epoch 10/60, Training Loss: 0.2018, Validation Loss: 0.1695\n",
      "[Trial 30] Epoch 14/60, Training Loss: 0.1464, Validation Loss: 0.1272\n",
      "[Trial 33] Epoch 2/60, Training Loss: 0.1463, Validation Loss: 0.1995\n",
      "[Trial 29] Epoch 19/60, Training Loss: 0.1267, Validation Loss: 0.1315\n",
      "[Trial 28] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1268\n",
      "[Trial 31] Epoch 11/60, Training Loss: 0.1882, Validation Loss: 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:34:37,573] Trial 23 finished with value: 0.1260109572360913 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.003771228758770986, 'batch_size': 8, 'patience': 7}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 23] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 23] Early stopping after 17 epochs.\n",
      "[Trial 30] Epoch 15/60, Training Loss: 0.1437, Validation Loss: 0.1270\n",
      "[Trial 33] Epoch 3/60, Training Loss: 0.1366, Validation Loss: 0.1843\n",
      "[Trial 29] Epoch 20/60, Training Loss: 0.1281, Validation Loss: 0.1324\n",
      "[Trial 28] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1265\n",
      "[Trial 24] Epoch 18/60, Training Loss: 0.1270, Validation Loss: 0.1227\n",
      "[Trial 31] Epoch 12/60, Training Loss: 0.1757, Validation Loss: 0.1348\n",
      "[Trial 34] Epoch 1/60, Training Loss: 1.7677, Validation Loss: 0.2462\n",
      "[Trial 30] Epoch 16/60, Training Loss: 0.1418, Validation Loss: 0.1268\n",
      "[Trial 33] Epoch 4/60, Training Loss: 0.1333, Validation Loss: 0.1734\n",
      "[Trial 29] Epoch 21/60, Training Loss: 0.1280, Validation Loss: 0.1309\n",
      "[Trial 28] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1267\n",
      "[Trial 31] Epoch 13/60, Training Loss: 0.1795, Validation Loss: 0.1320\n",
      "[Trial 34] Epoch 2/60, Training Loss: 0.1537, Validation Loss: 0.2204\n",
      "[Trial 30] Epoch 17/60, Training Loss: 0.1408, Validation Loss: 0.1267\n",
      "[Trial 33] Epoch 5/60, Training Loss: 0.1311, Validation Loss: 0.1639\n",
      "[Trial 28] Epoch 40/60, Training Loss: 0.1254, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 22/60, Training Loss: 0.1265, Validation Loss: 0.1302\n",
      "[Trial 31] Epoch 14/60, Training Loss: 0.1496, Validation Loss: 0.1295\n",
      "[Trial 34] Epoch 3/60, Training Loss: 0.1409, Validation Loss: 0.2057\n",
      "[Trial 33] Epoch 6/60, Training Loss: 0.1300, Validation Loss: 0.1572\n",
      "[Trial 30] Epoch 18/60, Training Loss: 0.1405, Validation Loss: 0.1267\n",
      "[Trial 28] Epoch 41/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1305\n",
      "[Trial 31] Epoch 15/60, Training Loss: 0.1570, Validation Loss: 0.1408\n",
      "[Trial 24] Epoch 19/60, Training Loss: 0.1268, Validation Loss: 0.1223\n",
      "[Trial 33] Epoch 7/60, Training Loss: 0.1292, Validation Loss: 0.1483\n",
      "[Trial 34] Epoch 4/60, Training Loss: 0.1358, Validation Loss: 0.1949\n",
      "[Trial 30] Epoch 19/60, Training Loss: 0.1390, Validation Loss: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:35:30,087] Trial 28 finished with value: 0.12654791921377181 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'learning_rate': 0.001460989722460908, 'batch_size': 32, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 28] Epoch 42/60, Training Loss: 0.1254, Validation Loss: 0.1267\n",
      "[Trial 28] Early stopping after 42 epochs.\n",
      "[Trial 29] Epoch 24/60, Training Loss: 0.1282, Validation Loss: 0.1288\n",
      "[Trial 31] Epoch 16/60, Training Loss: 0.1582, Validation Loss: 0.1300\n",
      "[Trial 33] Epoch 8/60, Training Loss: 0.1282, Validation Loss: 0.1441\n",
      "[Trial 34] Epoch 5/60, Training Loss: 0.1329, Validation Loss: 0.1875\n",
      "[Trial 30] Epoch 20/60, Training Loss: 0.1389, Validation Loss: 0.1267\n",
      "[Trial 35] Epoch 1/60, Training Loss: 1.2640, Validation Loss: 0.1434\n",
      "[Trial 29] Epoch 25/60, Training Loss: 0.1262, Validation Loss: 0.1289\n",
      "[Trial 33] Epoch 9/60, Training Loss: 0.1279, Validation Loss: 0.1424\n",
      "[Trial 31] Epoch 17/60, Training Loss: 0.1705, Validation Loss: 0.1277\n",
      "[Trial 34] Epoch 6/60, Training Loss: 0.1314, Validation Loss: 0.1809\n",
      "[Trial 30] Epoch 21/60, Training Loss: 0.1382, Validation Loss: 0.1267\n",
      "[Trial 35] Epoch 2/60, Training Loss: 0.1280, Validation Loss: 0.1328\n",
      "[Trial 29] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1283\n",
      "[Trial 33] Epoch 10/60, Training Loss: 0.1276, Validation Loss: 0.1374\n",
      "[Trial 31] Epoch 18/60, Training Loss: 0.1519, Validation Loss: 0.1263\n",
      "[Trial 34] Epoch 7/60, Training Loss: 0.1301, Validation Loss: 0.1724\n",
      "[Trial 24] Epoch 20/60, Training Loss: 0.1270, Validation Loss: 0.1224\n",
      "[Trial 30] Epoch 22/60, Training Loss: 0.1377, Validation Loss: 0.1267\n",
      "[Trial 35] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1262\n",
      "[Trial 29] Epoch 27/60, Training Loss: 0.1299, Validation Loss: 0.1283\n",
      "[Trial 33] Epoch 11/60, Training Loss: 0.1272, Validation Loss: 0.1337\n",
      "[Trial 31] Epoch 19/60, Training Loss: 0.1454, Validation Loss: 0.1263\n",
      "[Trial 34] Epoch 8/60, Training Loss: 0.1291, Validation Loss: 0.1640\n",
      "[Trial 35] Epoch 4/60, Training Loss: 0.1270, Validation Loss: 0.1245\n",
      "[Trial 30] Epoch 23/60, Training Loss: 0.1377, Validation Loss: 0.1267\n",
      "[Trial 33] Epoch 12/60, Training Loss: 0.1270, Validation Loss: 0.1324\n",
      "[Trial 29] Epoch 28/60, Training Loss: 0.1261, Validation Loss: 0.1278\n",
      "[Trial 31] Epoch 20/60, Training Loss: 0.1393, Validation Loss: 0.1260\n",
      "[Trial 34] Epoch 9/60, Training Loss: 0.1289, Validation Loss: 0.1580\n",
      "[Trial 35] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1246\n",
      "[Trial 30] Epoch 24/60, Training Loss: 0.1368, Validation Loss: 0.1266\n",
      "[Trial 33] Epoch 13/60, Training Loss: 0.1270, Validation Loss: 0.1302\n",
      "[Trial 29] Epoch 29/60, Training Loss: 0.1261, Validation Loss: 0.1279\n",
      "[Trial 31] Epoch 21/60, Training Loss: 0.1425, Validation Loss: 0.1259\n",
      "[Trial 35] Epoch 6/60, Training Loss: 0.1265, Validation Loss: 0.1242\n",
      "[Trial 34] Epoch 10/60, Training Loss: 0.1281, Validation Loss: 0.1547\n",
      "[Trial 33] Epoch 14/60, Training Loss: 0.1268, Validation Loss: 0.1297\n",
      "[Trial 30] Epoch 25/60, Training Loss: 0.1373, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 30/60, Training Loss: 0.1261, Validation Loss: 0.1271\n",
      "[Trial 24] Epoch 21/60, Training Loss: 0.1270, Validation Loss: 0.1222\n",
      "[Trial 31] Epoch 22/60, Training Loss: 0.1342, Validation Loss: 0.1260\n",
      "[Trial 35] Epoch 7/60, Training Loss: 0.1263, Validation Loss: 0.1242\n",
      "[Trial 34] Epoch 11/60, Training Loss: 0.1278, Validation Loss: 0.1492\n",
      "[Trial 33] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1289\n",
      "[Trial 30] Epoch 26/60, Training Loss: 0.1368, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 31/60, Training Loss: 0.1278, Validation Loss: 0.1272\n",
      "[Trial 31] Epoch 23/60, Training Loss: 0.1377, Validation Loss: 0.1266\n",
      "[Trial 35] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 34] Epoch 12/60, Training Loss: 0.1273, Validation Loss: 0.1458\n",
      "[Trial 33] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1297\n",
      "[Trial 30] Epoch 27/60, Training Loss: 0.1369, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 32/60, Training Loss: 0.1260, Validation Loss: 0.1271\n",
      "[Trial 31] Epoch 24/60, Training Loss: 0.1324, Validation Loss: 0.1258\n",
      "[Trial 35] Epoch 9/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 33] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1284\n",
      "[Trial 34] Epoch 13/60, Training Loss: 0.1270, Validation Loss: 0.1422\n",
      "[Trial 30] Epoch 28/60, Training Loss: 0.1364, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 33/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 31] Epoch 25/60, Training Loss: 0.1319, Validation Loss: 0.1258\n",
      "[Trial 35] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 24] Epoch 22/60, Training Loss: 0.1269, Validation Loss: 0.1224\n",
      "[Trial 33] Epoch 18/60, Training Loss: 0.1265, Validation Loss: 0.1275\n",
      "[Trial 34] Epoch 14/60, Training Loss: 0.1270, Validation Loss: 0.1394\n",
      "[Trial 30] Epoch 29/60, Training Loss: 0.1366, Validation Loss: 0.1266\n",
      "[Trial 29] Epoch 34/60, Training Loss: 0.1270, Validation Loss: 0.1275\n",
      "[Trial 31] Epoch 26/60, Training Loss: 0.1431, Validation Loss: 0.1299\n",
      "[Trial 35] Epoch 11/60, Training Loss: 0.1263, Validation Loss: 0.1243\n",
      "[Trial 33] Epoch 19/60, Training Loss: 0.1264, Validation Loss: 0.1276\n",
      "[Trial 34] Epoch 15/60, Training Loss: 0.1266, Validation Loss: 0.1380\n",
      "[Trial 29] Epoch 35/60, Training Loss: 0.1264, Validation Loss: 0.1267\n",
      "[Trial 30] Epoch 30/60, Training Loss: 0.1364, Validation Loss: 0.1267\n",
      "[Trial 35] Epoch 12/60, Training Loss: 0.1263, Validation Loss: 0.1240\n",
      "[Trial 31] Epoch 27/60, Training Loss: 0.1324, Validation Loss: 0.1262\n",
      "[Trial 33] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1278\n",
      "[Trial 34] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1371\n",
      "[Trial 29] Epoch 36/60, Training Loss: 0.1287, Validation Loss: 0.1269\n",
      "[Trial 30] Epoch 31/60, Training Loss: 0.1362, Validation Loss: 0.1266\n",
      "[Trial 35] Epoch 13/60, Training Loss: 0.1261, Validation Loss: 0.1239\n",
      "[Trial 31] Epoch 28/60, Training Loss: 0.1321, Validation Loss: 0.1257\n",
      "[Trial 33] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1271\n",
      "[Trial 24] Epoch 23/60, Training Loss: 0.1267, Validation Loss: 0.1222\n",
      "[Trial 34] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1355\n",
      "[Trial 29] Epoch 37/60, Training Loss: 0.1258, Validation Loss: 0.1267\n",
      "[Trial 35] Epoch 14/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 30] Epoch 32/60, Training Loss: 0.1360, Validation Loss: 0.1266\n",
      "[Trial 33] Epoch 22/60, Training Loss: 0.1261, Validation Loss: 0.1273\n",
      "[Trial 31] Epoch 29/60, Training Loss: 0.1293, Validation Loss: 0.1257\n",
      "[Trial 34] Epoch 18/60, Training Loss: 0.1265, Validation Loss: 0.1338\n",
      "[Trial 35] Epoch 15/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 29] Epoch 38/60, Training Loss: 0.1258, Validation Loss: 0.1262\n",
      "[Trial 33] Epoch 23/60, Training Loss: 0.1259, Validation Loss: 0.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:38:13,962] Trial 30 finished with value: 0.12656374077002208 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'learning_rate': 0.03776592173939602, 'batch_size': 32, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 30] Epoch 33/60, Training Loss: 0.1357, Validation Loss: 0.1266\n",
      "[Trial 30] Early stopping after 33 epochs.\n",
      "[Trial 31] Epoch 30/60, Training Loss: 0.1298, Validation Loss: 0.1257\n",
      "[Trial 34] Epoch 19/60, Training Loss: 0.1264, Validation Loss: 0.1324\n",
      "[Trial 35] Epoch 16/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 33] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1273\n",
      "[Trial 29] Epoch 39/60, Training Loss: 0.1259, Validation Loss: 0.1264\n",
      "[Trial 31] Epoch 31/60, Training Loss: 0.1297, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 1/60, Training Loss: 1.4805, Validation Loss: 0.1961\n",
      "[Trial 34] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1324\n",
      "[Trial 35] Epoch 17/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 33] Epoch 25/60, Training Loss: 0.1265, Validation Loss: 0.1270\n",
      "[Trial 24] Epoch 24/60, Training Loss: 0.1266, Validation Loss: 0.1227\n",
      "[Trial 29] Epoch 40/60, Training Loss: 0.1270, Validation Loss: 0.1268\n",
      "[Trial 31] Epoch 32/60, Training Loss: 0.1287, Validation Loss: 0.1259\n",
      "[Trial 35] Epoch 18/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 33] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1268\n",
      "[Trial 34] Epoch 21/60, Training Loss: 0.1260, Validation Loss: 0.1309\n",
      "[Trial 29] Epoch 41/60, Training Loss: 0.1259, Validation Loss: 0.1266\n",
      "[Trial 31] Epoch 33/60, Training Loss: 0.1285, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 2/60, Training Loss: 0.1370, Validation Loss: 0.1680\n",
      "[Trial 33] Epoch 27/60, Training Loss: 0.1260, Validation Loss: 0.1271\n",
      "[Trial 35] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1240\n",
      "[Trial 34] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1305\n",
      "[Trial 29] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1260\n",
      "[Trial 31] Epoch 34/60, Training Loss: 0.1285, Validation Loss: 0.1257\n",
      "[Trial 33] Epoch 28/60, Training Loss: 0.1263, Validation Loss: 0.1265\n",
      "[Trial 35] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1244\n",
      "[Trial 34] Epoch 23/60, Training Loss: 0.1259, Validation Loss: 0.1307\n",
      "[Trial 36] Epoch 3/60, Training Loss: 0.1317, Validation Loss: 0.1484\n",
      "[Trial 29] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 31] Epoch 35/60, Training Loss: 0.1278, Validation Loss: 0.1257\n",
      "[Trial 24] Epoch 25/60, Training Loss: 0.1269, Validation Loss: 0.1222\n",
      "[Trial 33] Epoch 29/60, Training Loss: 0.1261, Validation Loss: 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:39:17,260] Trial 35 finished with value: 0.12393681704998016 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.012401173915514334, 'batch_size': 32, 'patience': 8}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 35] Epoch 21/60, Training Loss: 0.1260, Validation Loss: 0.1242\n",
      "[Trial 35] Early stopping after 21 epochs.\n",
      "[Trial 34] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1300\n",
      "[Trial 29] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 31] Epoch 36/60, Training Loss: 0.1302, Validation Loss: 0.1257\n",
      "[Trial 33] Epoch 30/60, Training Loss: 0.1264, Validation Loss: 0.1264\n",
      "[Trial 37] Epoch 1/60, Training Loss: 1.2290, Validation Loss: 0.1377\n",
      "[Trial 36] Epoch 4/60, Training Loss: 0.1295, Validation Loss: 0.1407\n",
      "[Trial 34] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1292\n",
      "[Trial 29] Epoch 45/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 31] Epoch 37/60, Training Loss: 0.1283, Validation Loss: 0.1257\n",
      "[Trial 33] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1267\n",
      "[Trial 37] Epoch 2/60, Training Loss: 0.1274, Validation Loss: 0.1306\n",
      "[Trial 34] Epoch 26/60, Training Loss: 0.1258, Validation Loss: 0.1289\n",
      "[Trial 29] Epoch 46/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 31] Epoch 38/60, Training Loss: 0.1284, Validation Loss: 0.1257\n",
      "[Trial 33] Epoch 32/60, Training Loss: 0.1261, Validation Loss: 0.1261\n",
      "[Trial 36] Epoch 5/60, Training Loss: 0.1287, Validation Loss: 0.1326\n",
      "[Trial 37] Epoch 3/60, Training Loss: 0.1266, Validation Loss: 0.1295\n",
      "[Trial 24] Epoch 26/60, Training Loss: 0.1268, Validation Loss: 0.1223\n",
      "[Trial 34] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1293\n",
      "[Trial 29] Epoch 47/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 33] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 31] Epoch 39/60, Training Loss: 0.1297, Validation Loss: 0.1257\n",
      "[Trial 37] Epoch 4/60, Training Loss: 0.1264, Validation Loss: 0.1273\n",
      "[Trial 34] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1288\n",
      "[Trial 36] Epoch 6/60, Training Loss: 0.1285, Validation Loss: 0.1289\n",
      "[Trial 33] Epoch 34/60, Training Loss: 0.1266, Validation Loss: 0.1264\n",
      "[Trial 29] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 31] Epoch 40/60, Training Loss: 0.1281, Validation Loss: 0.1256\n",
      "[Trial 37] Epoch 5/60, Training Loss: 0.1259, Validation Loss: 0.1263\n",
      "[Trial 34] Epoch 29/60, Training Loss: 0.1256, Validation Loss: 0.1285\n",
      "[Trial 33] Epoch 35/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 29] Epoch 49/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 37] Epoch 6/60, Training Loss: 0.1257, Validation Loss: 0.1263\n",
      "[Trial 31] Epoch 41/60, Training Loss: 0.1288, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 7/60, Training Loss: 0.1277, Validation Loss: 0.1260\n",
      "[Trial 24] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:40:29,905] Trial 33 finished with value: 0.126131655027469 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.0007274061743297105, 'batch_size': 32, 'patience': 4}. Best is trial 8 with value: 0.12338427988191446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 33] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1262\n",
      "[Trial 33] Early stopping after 36 epochs.\n",
      "[Trial 34] Epoch 30/60, Training Loss: 0.1257, Validation Loss: 0.1285\n",
      "[Trial 37] Epoch 7/60, Training Loss: 0.1256, Validation Loss: 0.1259\n",
      "[Trial 29] Epoch 50/60, Training Loss: 0.1255, Validation Loss: 0.1260\n",
      "[Trial 31] Epoch 42/60, Training Loss: 0.1290, Validation Loss: 0.1256\n",
      "[Trial 38] Epoch 1/60, Training Loss: 1.4863, Validation Loss: 0.1429\n",
      "[Trial 34] Epoch 31/60, Training Loss: 0.1257, Validation Loss: 0.1282\n",
      "[Trial 37] Epoch 8/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 29] Epoch 51/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 31] Epoch 43/60, Training Loss: 0.1295, Validation Loss: 0.1256\n",
      "[Trial 36] Epoch 8/60, Training Loss: 0.1275, Validation Loss: 0.1251\n",
      "[Trial 38] Epoch 2/60, Training Loss: 0.1271, Validation Loss: 0.1321\n",
      "[Trial 37] Epoch 9/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 34] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1280\n",
      "[Trial 29] Epoch 52/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 31] Epoch 44/60, Training Loss: 0.1276, Validation Loss: 0.1256\n",
      "[Trial 38] Epoch 3/60, Training Loss: 0.1265, Validation Loss: 0.1292\n",
      "[Trial 37] Epoch 10/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 36] Epoch 9/60, Training Loss: 0.1272, Validation Loss: 0.1240\n",
      "[Trial 34] Epoch 33/60, Training Loss: 0.1257, Validation Loss: 0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:41:06,864] Trial 24 finished with value: 0.12217055708169937 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.0013573987421900472, 'batch_size': 8, 'patience': 5}. Best is trial 24 with value: 0.12217055708169937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 24] Epoch 28/60, Training Loss: 0.1264, Validation Loss: 0.1222\n",
      "[Trial 24] Early stopping after 28 epochs.\n",
      "[Trial 29] Epoch 53/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 31] Epoch 45/60, Training Loss: 0.1297, Validation Loss: 0.1256\n",
      "[Trial 38] Epoch 4/60, Training Loss: 0.1260, Validation Loss: 0.1282\n",
      "[Trial 37] Epoch 11/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 34] Epoch 34/60, Training Loss: 0.1254, Validation Loss: 0.1280\n",
      "[Trial 29] Epoch 54/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 38] Epoch 5/60, Training Loss: 0.1260, Validation Loss: 0.1281\n",
      "[Trial 31] Epoch 46/60, Training Loss: 0.1287, Validation Loss: 0.1256\n",
      "[Trial 36] Epoch 10/60, Training Loss: 0.1274, Validation Loss: 0.1246\n",
      "[Trial 37] Epoch 12/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 34] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1282\n",
      "[Trial 39] Epoch 1/60, Training Loss: 0.2320, Validation Loss: 0.2133\n",
      "[Trial 38] Epoch 6/60, Training Loss: 0.1256, Validation Loss: 0.1278\n",
      "[Trial 29] Epoch 55/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 31] Epoch 47/60, Training Loss: 0.1281, Validation Loss: 0.1256\n",
      "[Trial 37] Epoch 13/60, Training Loss: 0.1257, Validation Loss: 0.1259\n",
      "[Trial 38] Epoch 7/60, Training Loss: 0.1253, Validation Loss: 0.1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:41:41,784] Trial 34 finished with value: 0.127963758011659 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.0005408054797629247, 'batch_size': 32, 'patience': 4}. Best is trial 24 with value: 0.12217055708169937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 34] Epoch 36/60, Training Loss: 0.1252, Validation Loss: 0.1280\n",
      "[Trial 34] Early stopping after 36 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:41:43,717] Trial 29 finished with value: 0.12591044306755067 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'learning_rate': 0.0011410171206277064, 'batch_size': 32, 'patience': 4}. Best is trial 24 with value: 0.12217055708169937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 36] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1233\n",
      "[Trial 29] Epoch 56/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 29] Early stopping after 56 epochs.\n",
      "[Trial 31] Epoch 48/60, Training Loss: 0.1291, Validation Loss: 0.1256\n",
      "[Trial 37] Epoch 14/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 38] Epoch 8/60, Training Loss: 0.1254, Validation Loss: 0.1279\n",
      "[Trial 39] Epoch 2/60, Training Loss: 0.1350, Validation Loss: 0.1947\n",
      "[Trial 31] Epoch 49/60, Training Loss: 0.1279, Validation Loss: 0.1256\n",
      "[Trial 37] Epoch 15/60, Training Loss: 0.1255, Validation Loss: 0.1257\n",
      "[Trial 38] Epoch 9/60, Training Loss: 0.1250, Validation Loss: 0.1278\n",
      "[Trial 36] Epoch 12/60, Training Loss: 0.1270, Validation Loss: 0.1230\n",
      "[Trial 40] Epoch 1/60, Training Loss: 4.2403, Validation Loss: 0.4094\n",
      "[Trial 41] Epoch 1/60, Training Loss: 0.5873, Validation Loss: 0.2347\n",
      "[Trial 31] Epoch 50/60, Training Loss: 0.1297, Validation Loss: 0.1256\n",
      "[Trial 38] Epoch 10/60, Training Loss: 0.1252, Validation Loss: 0.1277\n",
      "[Trial 37] Epoch 16/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 39] Epoch 3/60, Training Loss: 0.1317, Validation Loss: 0.1869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:42:19,284] Trial 31 finished with value: 0.12564444492260615 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.03863639922919607, 'batch_size': 32, 'patience': 4}. Best is trial 24 with value: 0.12217055708169937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 31] Epoch 51/60, Training Loss: 0.1283, Validation Loss: 0.1256\n",
      "[Trial 31] Early stopping after 51 epochs.\n",
      "[Trial 38] Epoch 11/60, Training Loss: 0.1251, Validation Loss: 0.1277\n",
      "[Trial 36] Epoch 13/60, Training Loss: 0.1276, Validation Loss: 0.1228\n",
      "[Trial 37] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 40] Epoch 2/60, Training Loss: 0.2638, Validation Loss: 0.3136\n",
      "[Trial 41] Epoch 2/60, Training Loss: 0.1311, Validation Loss: 0.2217\n",
      "[Trial 38] Epoch 12/60, Training Loss: 0.1251, Validation Loss: 0.1276\n",
      "[Trial 37] Epoch 18/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 39] Epoch 4/60, Training Loss: 0.1301, Validation Loss: 0.1717\n",
      "[Trial 36] Epoch 14/60, Training Loss: 0.1272, Validation Loss: 0.1233\n",
      "[Trial 38] Epoch 13/60, Training Loss: 0.1250, Validation Loss: 0.1281\n",
      "[Trial 42] Epoch 1/60, Training Loss: 1.4949, Validation Loss: 0.2577\n",
      "[Trial 37] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 40] Epoch 3/60, Training Loss: 0.2005, Validation Loss: 0.2856\n",
      "[Trial 41] Epoch 3/60, Training Loss: 0.1311, Validation Loss: 0.2010\n",
      "[Trial 38] Epoch 14/60, Training Loss: 0.1251, Validation Loss: 0.1276\n",
      "[Trial 37] Epoch 20/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1225\n",
      "[Trial 39] Epoch 5/60, Training Loss: 0.1300, Validation Loss: 0.1653\n",
      "[Trial 38] Epoch 15/60, Training Loss: 0.1274, Validation Loss: 0.1276\n",
      "[Trial 37] Epoch 21/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 42] Epoch 2/60, Training Loss: 0.1304, Validation Loss: 0.2461\n",
      "[Trial 38] Epoch 16/60, Training Loss: 0.1250, Validation Loss: 0.1276\n",
      "[Trial 40] Epoch 4/60, Training Loss: 0.1759, Validation Loss: 0.2711\n",
      "[Trial 41] Epoch 4/60, Training Loss: 0.1302, Validation Loss: 0.1798\n",
      "[Trial 37] Epoch 22/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 16/60, Training Loss: 0.1270, Validation Loss: 0.1227\n",
      "[Trial 38] Epoch 17/60, Training Loss: 0.1252, Validation Loss: 0.1276\n",
      "[Trial 39] Epoch 6/60, Training Loss: 0.1290, Validation Loss: 0.1575\n",
      "[Trial 37] Epoch 23/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 42] Epoch 3/60, Training Loss: 0.1307, Validation Loss: 0.2259\n",
      "[Trial 38] Epoch 18/60, Training Loss: 0.1250, Validation Loss: 0.1276\n",
      "[Trial 40] Epoch 5/60, Training Loss: 0.1634, Validation Loss: 0.2622\n",
      "[Trial 41] Epoch 5/60, Training Loss: 0.1301, Validation Loss: 0.1620\n",
      "[Trial 36] Epoch 17/60, Training Loss: 0.1270, Validation Loss: 0.1239\n",
      "[Trial 37] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 38] Epoch 19/60, Training Loss: 0.1252, Validation Loss: 0.1280\n",
      "[Trial 39] Epoch 7/60, Training Loss: 0.1284, Validation Loss: 0.1527\n",
      "[Trial 37] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 38] Epoch 20/60, Training Loss: 0.1251, Validation Loss: 0.1276\n",
      "[Trial 42] Epoch 4/60, Training Loss: 0.1304, Validation Loss: 0.2060\n",
      "[Trial 36] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1219\n",
      "[Trial 40] Epoch 6/60, Training Loss: 0.1553, Validation Loss: 0.2548\n",
      "[Trial 41] Epoch 6/60, Training Loss: 0.1297, Validation Loss: 0.1499\n",
      "[Trial 37] Epoch 26/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 38] Epoch 21/60, Training Loss: 0.1249, Validation Loss: 0.1277\n",
      "[Trial 39] Epoch 8/60, Training Loss: 0.1284, Validation Loss: 0.1478\n",
      "[Trial 37] Epoch 27/60, Training Loss: 0.1252, Validation Loss: 0.1258\n",
      "[Trial 38] Epoch 22/60, Training Loss: 0.1249, Validation Loss: 0.1278\n",
      "[Trial 36] Epoch 19/60, Training Loss: 0.1271, Validation Loss: 0.1218\n",
      "[Trial 42] Epoch 5/60, Training Loss: 0.1305, Validation Loss: 0.1916\n",
      "[Trial 37] Epoch 28/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 41] Epoch 7/60, Training Loss: 0.1295, Validation Loss: 0.1400\n",
      "[Trial 40] Epoch 7/60, Training Loss: 0.1497, Validation Loss: 0.2497\n",
      "[Trial 38] Epoch 23/60, Training Loss: 0.1250, Validation Loss: 0.1276\n",
      "[Trial 37] Epoch 29/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 36] Epoch 20/60, Training Loss: 0.1268, Validation Loss: 0.1229\n",
      "[Trial 39] Epoch 9/60, Training Loss: 0.1281, Validation Loss: 0.1436\n",
      "[Trial 38] Epoch 24/60, Training Loss: 0.1248, Validation Loss: 0.1275\n",
      "[Trial 37] Epoch 30/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 38] Epoch 25/60, Training Loss: 0.1250, Validation Loss: 0.1275\n",
      "[Trial 42] Epoch 6/60, Training Loss: 0.1307, Validation Loss: 0.1721\n",
      "[Trial 41] Epoch 8/60, Training Loss: 0.1289, Validation Loss: 0.1324\n",
      "[Trial 40] Epoch 8/60, Training Loss: 0.1456, Validation Loss: 0.2459\n",
      "[Trial 36] Epoch 21/60, Training Loss: 0.1271, Validation Loss: 0.1686\n",
      "[Trial 37] Epoch 31/60, Training Loss: 0.1251, Validation Loss: 0.1257\n",
      "[Trial 38] Epoch 26/60, Training Loss: 0.1249, Validation Loss: 0.1277\n",
      "[Trial 39] Epoch 10/60, Training Loss: 0.1282, Validation Loss: 0.1417\n",
      "[Trial 38] Epoch 27/60, Training Loss: 0.1249, Validation Loss: 0.1277\n",
      "[Trial 37] Epoch 32/60, Training Loss: 0.1252, Validation Loss: 0.1258\n",
      "[Trial 41] Epoch 9/60, Training Loss: 0.1289, Validation Loss: 0.1278\n",
      "[Trial 40] Epoch 9/60, Training Loss: 0.1426, Validation Loss: 0.2432\n",
      "[Trial 42] Epoch 7/60, Training Loss: 0.1302, Validation Loss: 0.1587\n",
      "[Trial 36] Epoch 22/60, Training Loss: 0.1289, Validation Loss: 0.1220\n",
      "[Trial 38] Epoch 28/60, Training Loss: 0.1249, Validation Loss: 0.1279\n",
      "[Trial 37] Epoch 33/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 39] Epoch 11/60, Training Loss: 0.1278, Validation Loss: 0.1369\n",
      "[Trial 38] Epoch 29/60, Training Loss: 0.1248, Validation Loss: 0.1276\n",
      "[Trial 37] Epoch 34/60, Training Loss: 0.1251, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1218\n",
      "[Trial 41] Epoch 10/60, Training Loss: 0.1284, Validation Loss: 0.1241\n",
      "[Trial 40] Epoch 10/60, Training Loss: 0.1406, Validation Loss: 0.2398\n",
      "[Trial 42] Epoch 8/60, Training Loss: 0.1298, Validation Loss: 0.1484\n",
      "[Trial 38] Epoch 30/60, Training Loss: 0.1249, Validation Loss: 0.1276\n",
      "[Trial 37] Epoch 35/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 39] Epoch 12/60, Training Loss: 0.1279, Validation Loss: 0.1360\n",
      "[Trial 38] Epoch 31/60, Training Loss: 0.1249, Validation Loss: 0.1277\n",
      "[Trial 37] Epoch 36/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 36] Epoch 24/60, Training Loss: 0.1271, Validation Loss: 0.1222\n",
      "[Trial 41] Epoch 11/60, Training Loss: 0.1282, Validation Loss: 0.1227\n",
      "[Trial 40] Epoch 11/60, Training Loss: 0.1380, Validation Loss: 0.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:45:59,920] Trial 38 finished with value: 0.1275303932527701 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.012242489143373479, 'batch_size': 32, 'patience': 8}. Best is trial 24 with value: 0.12217055708169937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 38] Epoch 32/60, Training Loss: 0.1247, Validation Loss: 0.1276\n",
      "[Trial 38] Early stopping after 32 epochs.\n",
      "[Trial 37] Epoch 37/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 42] Epoch 9/60, Training Loss: 0.1306, Validation Loss: 0.1398\n",
      "[Trial 39] Epoch 13/60, Training Loss: 0.1277, Validation Loss: 0.1341\n",
      "[Trial 36] Epoch 25/60, Training Loss: 0.1270, Validation Loss: 0.1220\n",
      "[Trial 37] Epoch 38/60, Training Loss: 0.1252, Validation Loss: 0.1257\n",
      "[Trial 41] Epoch 12/60, Training Loss: 0.3546, Validation Loss: 0.1228\n",
      "[Trial 40] Epoch 12/60, Training Loss: 0.1371, Validation Loss: 0.2362\n",
      "[Trial 43] Epoch 1/60, Training Loss: 1.5050, Validation Loss: 0.2414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:46:25,658] Trial 37 finished with value: 0.1257164103289445 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.010963773444910414, 'batch_size': 32, 'patience': 8}. Best is trial 24 with value: 0.12217055708169937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 37] Epoch 39/60, Training Loss: 0.1252, Validation Loss: 0.1257\n",
      "[Trial 37] Early stopping after 39 epochs.\n",
      "[Trial 42] Epoch 10/60, Training Loss: 0.1569, Validation Loss: 0.1335\n",
      "[Trial 36] Epoch 26/60, Training Loss: 0.1266, Validation Loss: 0.1219\n",
      "[Trial 39] Epoch 14/60, Training Loss: 0.1278, Validation Loss: 0.1335\n",
      "[Trial 44] Epoch 1/60, Training Loss: 29.9958, Validation Loss: 0.2580\n",
      "[Trial 44] Epoch 2/60, Training Loss: 0.1351, Validation Loss: 0.2449\n",
      "[Trial 41] Epoch 13/60, Training Loss: 0.1276, Validation Loss: 0.1218\n",
      "[Trial 36] Epoch 27/60, Training Loss: 0.1270, Validation Loss: 0.1231\n",
      "[Trial 40] Epoch 13/60, Training Loss: 0.1357, Validation Loss: 0.2343\n",
      "[Trial 43] Epoch 2/60, Training Loss: 0.1308, Validation Loss: 0.2306\n",
      "[Trial 44] Epoch 3/60, Training Loss: 0.1304, Validation Loss: 0.2394\n",
      "[Trial 42] Epoch 11/60, Training Loss: 0.1282, Validation Loss: 0.1296\n",
      "[Trial 39] Epoch 15/60, Training Loss: 0.1277, Validation Loss: 0.1321\n",
      "[Trial 36] Epoch 28/60, Training Loss: 0.1265, Validation Loss: 0.1218\n",
      "[Trial 44] Epoch 4/60, Training Loss: 0.1294, Validation Loss: 0.2350\n",
      "[Trial 41] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1217\n",
      "[Trial 40] Epoch 14/60, Training Loss: 0.1345, Validation Loss: 0.2332\n",
      "[Trial 44] Epoch 5/60, Training Loss: 0.1288, Validation Loss: 0.2293\n",
      "[Trial 43] Epoch 3/60, Training Loss: 0.1305, Validation Loss: 0.2073\n",
      "[Trial 42] Epoch 12/60, Training Loss: 0.1284, Validation Loss: 0.1272\n",
      "[Trial 36] Epoch 29/60, Training Loss: 0.1267, Validation Loss: 0.1222\n",
      "[Trial 39] Epoch 16/60, Training Loss: 0.1273, Validation Loss: 0.1313\n",
      "[Trial 44] Epoch 6/60, Training Loss: 0.1281, Validation Loss: 0.2260\n",
      "[Trial 41] Epoch 15/60, Training Loss: 0.1278, Validation Loss: 0.1218\n",
      "[Trial 44] Epoch 7/60, Training Loss: 0.1283, Validation Loss: 0.2217\n",
      "[Trial 40] Epoch 15/60, Training Loss: 0.1337, Validation Loss: 0.2313\n",
      "[Trial 36] Epoch 30/60, Training Loss: 0.1263, Validation Loss: 0.1218\n",
      "[Trial 43] Epoch 4/60, Training Loss: 0.1305, Validation Loss: 0.1882\n",
      "[Trial 44] Epoch 8/60, Training Loss: 0.1284, Validation Loss: 0.2187\n",
      "[Trial 39] Epoch 17/60, Training Loss: 0.1273, Validation Loss: 0.1335\n",
      "[Trial 42] Epoch 13/60, Training Loss: 0.1282, Validation Loss: 0.1259\n",
      "[Trial 44] Epoch 9/60, Training Loss: 0.1286, Validation Loss: 0.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:48:09,351] Trial 36 finished with value: 0.12179954821864764 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0007583320545319638, 'batch_size': 16, 'patience': 8}. Best is trial 36 with value: 0.12179954821864764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 36] Epoch 31/60, Training Loss: 0.1271, Validation Loss: 0.1222\n",
      "[Trial 36] Early stopping after 31 epochs.\n",
      "[Trial 41] Epoch 16/60, Training Loss: 0.1277, Validation Loss: 0.1217\n",
      "[Trial 40] Epoch 16/60, Training Loss: 0.1327, Validation Loss: 0.2300\n",
      "[Trial 44] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.2122\n",
      "[Trial 43] Epoch 5/60, Training Loss: 0.1298, Validation Loss: 0.1689\n",
      "[Trial 39] Epoch 18/60, Training Loss: 0.1274, Validation Loss: 0.1317\n",
      "[Trial 44] Epoch 11/60, Training Loss: 0.1286, Validation Loss: 0.2108\n",
      "[Trial 42] Epoch 14/60, Training Loss: 0.1276, Validation Loss: 0.1256\n",
      "[Trial 41] Epoch 17/60, Training Loss: 0.1272, Validation Loss: 0.1219\n",
      "[Trial 45] Epoch 1/60, Training Loss: 3.9022, Validation Loss: 0.4303\n",
      "[Trial 40] Epoch 17/60, Training Loss: 0.1322, Validation Loss: 0.2284\n",
      "[Trial 44] Epoch 12/60, Training Loss: 0.1280, Validation Loss: 0.2062\n",
      "[Trial 43] Epoch 6/60, Training Loss: 0.1286, Validation Loss: 0.1560\n",
      "[Trial 44] Epoch 13/60, Training Loss: 0.1282, Validation Loss: 0.2038\n",
      "[Trial 39] Epoch 19/60, Training Loss: 0.1273, Validation Loss: 0.1301\n",
      "[Trial 42] Epoch 15/60, Training Loss: 0.1277, Validation Loss: 0.1254\n",
      "[Trial 44] Epoch 14/60, Training Loss: 0.1283, Validation Loss: 0.1989\n",
      "[Trial 41] Epoch 18/60, Training Loss: 0.1267, Validation Loss: 0.1219\n",
      "[Trial 45] Epoch 2/60, Training Loss: 0.2819, Validation Loss: 0.3254\n",
      "[Trial 40] Epoch 18/60, Training Loss: 0.1318, Validation Loss: 0.2278\n",
      "[Trial 44] Epoch 15/60, Training Loss: 0.1298, Validation Loss: 0.1960\n",
      "[Trial 43] Epoch 7/60, Training Loss: 0.1353, Validation Loss: 0.1460\n",
      "[Trial 39] Epoch 20/60, Training Loss: 0.1273, Validation Loss: 0.1302\n",
      "[Trial 44] Epoch 16/60, Training Loss: 0.1286, Validation Loss: 0.1925\n",
      "[Trial 42] Epoch 16/60, Training Loss: 0.1275, Validation Loss: 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:49:31,328] Trial 41 finished with value: 0.1217108176400264 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.003341935932330103, 'batch_size': 16, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 41] Epoch 19/60, Training Loss: 0.1266, Validation Loss: 0.1219\n",
      "[Trial 41] Early stopping after 19 epochs.\n",
      "[Trial 45] Epoch 3/60, Training Loss: 0.2125, Validation Loss: 0.2940\n",
      "[Trial 40] Epoch 19/60, Training Loss: 0.1313, Validation Loss: 0.2266\n",
      "[Trial 44] Epoch 17/60, Training Loss: 0.1305, Validation Loss: 0.1894\n",
      "[Trial 39] Epoch 21/60, Training Loss: 0.1272, Validation Loss: 0.1297\n",
      "[Trial 43] Epoch 8/60, Training Loss: 0.1284, Validation Loss: 0.1390\n",
      "[Trial 44] Epoch 18/60, Training Loss: 0.1291, Validation Loss: 0.1846\n",
      "[Trial 42] Epoch 17/60, Training Loss: 0.1280, Validation Loss: 0.1252\n",
      "[Trial 46] Epoch 1/60, Training Loss: 1.1614, Validation Loss: 0.2668\n",
      "[Trial 44] Epoch 19/60, Training Loss: 0.1273, Validation Loss: 0.1804\n",
      "[Trial 45] Epoch 4/60, Training Loss: 0.1849, Validation Loss: 0.2790\n",
      "[Trial 40] Epoch 20/60, Training Loss: 0.1309, Validation Loss: 0.2254\n",
      "[Trial 44] Epoch 20/60, Training Loss: 0.1285, Validation Loss: 0.1771\n",
      "[Trial 39] Epoch 22/60, Training Loss: 0.1272, Validation Loss: 0.1307\n",
      "[Trial 43] Epoch 9/60, Training Loss: 0.1283, Validation Loss: 0.1338\n",
      "[Trial 44] Epoch 21/60, Training Loss: 0.1285, Validation Loss: 0.1723\n",
      "[Trial 46] Epoch 2/60, Training Loss: 0.1508, Validation Loss: 0.2503\n",
      "[Trial 42] Epoch 18/60, Training Loss: 0.1278, Validation Loss: 0.1252\n",
      "[Trial 45] Epoch 5/60, Training Loss: 0.1699, Validation Loss: 0.2696\n",
      "[Trial 40] Epoch 21/60, Training Loss: 0.1305, Validation Loss: 0.2250\n",
      "[Trial 44] Epoch 22/60, Training Loss: 0.1295, Validation Loss: 0.1679\n",
      "[Trial 39] Epoch 23/60, Training Loss: 0.1264, Validation Loss: 0.1302\n",
      "[Trial 44] Epoch 23/60, Training Loss: 0.1281, Validation Loss: 0.1647\n",
      "[Trial 43] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.1306\n",
      "[Trial 46] Epoch 3/60, Training Loss: 0.1397, Validation Loss: 0.2432\n",
      "[Trial 45] Epoch 6/60, Training Loss: 0.1608, Validation Loss: 0.2628\n",
      "[Trial 42] Epoch 19/60, Training Loss: 0.1276, Validation Loss: 0.1251\n",
      "[Trial 44] Epoch 24/60, Training Loss: 0.1309, Validation Loss: 0.1612\n",
      "[Trial 40] Epoch 22/60, Training Loss: 0.1296, Validation Loss: 0.2244\n",
      "[Trial 44] Epoch 25/60, Training Loss: 0.1281, Validation Loss: 0.1578\n",
      "[Trial 39] Epoch 24/60, Training Loss: 0.1273, Validation Loss: 0.1303\n",
      "[Trial 43] Epoch 11/60, Training Loss: 0.1283, Validation Loss: 0.1286\n",
      "[Trial 46] Epoch 4/60, Training Loss: 0.1349, Validation Loss: 0.2381\n",
      "[Trial 44] Epoch 26/60, Training Loss: 0.1278, Validation Loss: 0.1550\n",
      "[Trial 45] Epoch 7/60, Training Loss: 0.1544, Validation Loss: 0.2588\n",
      "[Trial 40] Epoch 23/60, Training Loss: 0.1294, Validation Loss: 0.2229\n",
      "[Trial 42] Epoch 20/60, Training Loss: 0.1277, Validation Loss: 0.1251\n",
      "[Trial 44] Epoch 27/60, Training Loss: 0.1277, Validation Loss: 0.1521\n",
      "[Trial 39] Epoch 25/60, Training Loss: 0.1262, Validation Loss: 0.1297\n",
      "[Trial 44] Epoch 28/60, Training Loss: 0.1383, Validation Loss: 0.1503\n",
      "[Trial 43] Epoch 12/60, Training Loss: 0.1286, Validation Loss: 0.1275\n",
      "[Trial 46] Epoch 5/60, Training Loss: 0.1328, Validation Loss: 0.2341\n",
      "[Trial 45] Epoch 8/60, Training Loss: 0.1490, Validation Loss: 0.2547\n",
      "[Trial 40] Epoch 24/60, Training Loss: 0.1291, Validation Loss: 0.2212\n",
      "[Trial 44] Epoch 29/60, Training Loss: 0.1289, Validation Loss: 0.1471\n",
      "[Trial 42] Epoch 21/60, Training Loss: 0.1280, Validation Loss: 0.1251\n",
      "[Trial 39] Epoch 26/60, Training Loss: 0.1259, Validation Loss: 0.1295\n",
      "[Trial 44] Epoch 30/60, Training Loss: 0.1279, Validation Loss: 0.1441\n",
      "[Trial 46] Epoch 6/60, Training Loss: 0.1317, Validation Loss: 0.2303\n",
      "[Trial 43] Epoch 13/60, Training Loss: 0.1282, Validation Loss: 0.1269\n",
      "[Trial 44] Epoch 31/60, Training Loss: 0.1272, Validation Loss: 0.1419\n",
      "[Trial 45] Epoch 9/60, Training Loss: 0.1454, Validation Loss: 0.2517\n",
      "[Trial 40] Epoch 25/60, Training Loss: 0.1289, Validation Loss: 0.2212\n",
      "[Trial 42] Epoch 22/60, Training Loss: 0.1285, Validation Loss: 0.1250\n",
      "[Trial 44] Epoch 32/60, Training Loss: 0.1268, Validation Loss: 0.1396\n",
      "[Trial 39] Epoch 27/60, Training Loss: 0.1265, Validation Loss: 0.1293\n",
      "[Trial 46] Epoch 7/60, Training Loss: 0.1301, Validation Loss: 0.2290\n",
      "[Trial 44] Epoch 33/60, Training Loss: 0.1269, Validation Loss: 0.1377\n",
      "[Trial 43] Epoch 14/60, Training Loss: 0.1280, Validation Loss: 0.1335\n",
      "[Trial 40] Epoch 26/60, Training Loss: 0.1291, Validation Loss: 0.2197\n",
      "[Trial 45] Epoch 10/60, Training Loss: 0.1426, Validation Loss: 0.2493\n",
      "[Trial 44] Epoch 34/60, Training Loss: 0.1268, Validation Loss: 0.1362\n",
      "[Trial 42] Epoch 23/60, Training Loss: 0.1274, Validation Loss: 0.1251\n",
      "[Trial 39] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1290\n",
      "[Trial 44] Epoch 35/60, Training Loss: 0.1297, Validation Loss: 0.1347\n",
      "[Trial 46] Epoch 8/60, Training Loss: 0.1298, Validation Loss: 0.2266\n",
      "[Trial 44] Epoch 36/60, Training Loss: 0.1364, Validation Loss: 0.1336\n",
      "[Trial 43] Epoch 15/60, Training Loss: 0.1292, Validation Loss: 0.1264\n",
      "[Trial 40] Epoch 27/60, Training Loss: 0.1286, Validation Loss: 0.2192\n",
      "[Trial 45] Epoch 11/60, Training Loss: 0.1404, Validation Loss: 0.2474\n",
      "[Trial 42] Epoch 24/60, Training Loss: 0.1275, Validation Loss: 0.1251\n",
      "[Trial 39] Epoch 29/60, Training Loss: 0.1259, Validation Loss: 0.1297\n",
      "[Trial 44] Epoch 37/60, Training Loss: 0.1273, Validation Loss: 0.1321\n",
      "[Trial 46] Epoch 9/60, Training Loss: 0.1293, Validation Loss: 0.2235\n",
      "[Trial 44] Epoch 38/60, Training Loss: 0.1270, Validation Loss: 0.1310\n",
      "[Trial 43] Epoch 16/60, Training Loss: 0.1275, Validation Loss: 0.1264\n",
      "[Trial 40] Epoch 28/60, Training Loss: 0.1286, Validation Loss: 0.2182\n",
      "[Trial 45] Epoch 12/60, Training Loss: 0.1389, Validation Loss: 0.2451\n",
      "[Trial 44] Epoch 39/60, Training Loss: 0.1268, Validation Loss: 0.1299\n",
      "[Trial 39] Epoch 30/60, Training Loss: 0.1261, Validation Loss: 0.1295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:53:54,637] Trial 42 finished with value: 0.12504227235913276 and parameters: {'hidden_dim': 448, 'latent_dim': 128, 'learning_rate': 0.0031899489244990122, 'batch_size': 16, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 42] Epoch 25/60, Training Loss: 0.1262, Validation Loss: 0.1251\n",
      "[Trial 42] Early stopping after 25 epochs.\n",
      "[Trial 44] Epoch 40/60, Training Loss: 0.1268, Validation Loss: 0.1292\n",
      "[Trial 46] Epoch 10/60, Training Loss: 0.1286, Validation Loss: 0.2197\n",
      "[Trial 44] Epoch 41/60, Training Loss: 0.1303, Validation Loss: 0.1284\n",
      "[Trial 43] Epoch 17/60, Training Loss: 0.1278, Validation Loss: 0.1263\n",
      "[Trial 40] Epoch 29/60, Training Loss: 0.1281, Validation Loss: 0.2180\n",
      "[Trial 45] Epoch 13/60, Training Loss: 0.1373, Validation Loss: 0.2433\n",
      "[Trial 39] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1289\n",
      "[Trial 47] Epoch 1/60, Training Loss: 1.2310, Validation Loss: 0.2729\n",
      "[Trial 44] Epoch 42/60, Training Loss: 0.1266, Validation Loss: 0.1278\n",
      "[Trial 46] Epoch 11/60, Training Loss: 0.1285, Validation Loss: 0.2178\n",
      "[Trial 44] Epoch 43/60, Training Loss: 0.1286, Validation Loss: 0.1324\n",
      "[Trial 43] Epoch 18/60, Training Loss: 0.1272, Validation Loss: 0.1264\n",
      "[Trial 40] Epoch 30/60, Training Loss: 0.1281, Validation Loss: 0.2165\n",
      "[Trial 45] Epoch 14/60, Training Loss: 0.1358, Validation Loss: 0.2417\n",
      "[Trial 39] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1292\n",
      "[Trial 44] Epoch 44/60, Training Loss: 0.4245, Validation Loss: 0.1343\n",
      "[Trial 47] Epoch 2/60, Training Loss: 0.1524, Validation Loss: 0.2537\n",
      "[Trial 46] Epoch 12/60, Training Loss: 0.1282, Validation Loss: 0.2147\n",
      "[Trial 44] Epoch 45/60, Training Loss: 0.1273, Validation Loss: 0.1266\n",
      "[Trial 44] Epoch 46/60, Training Loss: 0.1260, Validation Loss: 0.1263\n",
      "[Trial 40] Epoch 31/60, Training Loss: 0.1280, Validation Loss: 0.2160\n",
      "[Trial 43] Epoch 19/60, Training Loss: 0.1283, Validation Loss: 0.1263\n",
      "[Trial 45] Epoch 15/60, Training Loss: 0.1350, Validation Loss: 0.2396\n",
      "[Trial 47] Epoch 3/60, Training Loss: 0.1403, Validation Loss: 0.2459\n",
      "[Trial 39] Epoch 33/60, Training Loss: 0.1261, Validation Loss: 0.1284\n",
      "[Trial 44] Epoch 47/60, Training Loss: 0.1259, Validation Loss: 0.1261\n",
      "[Trial 46] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.2128\n",
      "[Trial 44] Epoch 48/60, Training Loss: 0.1261, Validation Loss: 0.1260\n",
      "[Trial 40] Epoch 32/60, Training Loss: 0.1281, Validation Loss: 0.2147\n",
      "[Trial 45] Epoch 16/60, Training Loss: 0.1341, Validation Loss: 0.2391\n",
      "[Trial 43] Epoch 20/60, Training Loss: 0.1270, Validation Loss: 0.1263\n",
      "[Trial 39] Epoch 34/60, Training Loss: 0.1260, Validation Loss: 0.1294\n",
      "[Trial 47] Epoch 4/60, Training Loss: 0.1355, Validation Loss: 0.2416\n",
      "[Trial 44] Epoch 49/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 46] Epoch 14/60, Training Loss: 0.1281, Validation Loss: 0.2134\n",
      "[Trial 44] Epoch 50/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 40] Epoch 33/60, Training Loss: 0.1275, Validation Loss: 0.2133\n",
      "[Trial 45] Epoch 17/60, Training Loss: 0.1337, Validation Loss: 0.2374\n",
      "[Trial 43] Epoch 21/60, Training Loss: 0.1276, Validation Loss: 0.1265\n",
      "[Trial 44] Epoch 51/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 47] Epoch 5/60, Training Loss: 0.1330, Validation Loss: 0.2379\n",
      "[Trial 39] Epoch 35/60, Training Loss: 0.1263, Validation Loss: 0.1284\n",
      "[Trial 46] Epoch 15/60, Training Loss: 0.1279, Validation Loss: 0.2073\n",
      "[Trial 44] Epoch 52/60, Training Loss: 0.1258, Validation Loss: 0.1255\n",
      "[Trial 44] Epoch 53/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 40] Epoch 34/60, Training Loss: 0.1279, Validation Loss: 0.2131\n",
      "[Trial 45] Epoch 18/60, Training Loss: 0.1326, Validation Loss: 0.2362\n",
      "[Trial 47] Epoch 6/60, Training Loss: 0.1316, Validation Loss: 0.2360\n",
      "[Trial 39] Epoch 36/60, Training Loss: 0.1264, Validation Loss: 0.1280\n",
      "[Trial 43] Epoch 22/60, Training Loss: 0.1271, Validation Loss: 0.1264\n",
      "[Trial 44] Epoch 54/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 46] Epoch 16/60, Training Loss: 0.1280, Validation Loss: 0.2095\n",
      "[Trial 44] Epoch 55/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 40] Epoch 35/60, Training Loss: 0.1275, Validation Loss: 0.2124\n",
      "[Trial 47] Epoch 7/60, Training Loss: 0.1304, Validation Loss: 0.2320\n",
      "[Trial 39] Epoch 37/60, Training Loss: 0.1261, Validation Loss: 0.1285\n",
      "[Trial 45] Epoch 19/60, Training Loss: 0.1323, Validation Loss: 0.2350\n",
      "[Trial 43] Epoch 23/60, Training Loss: 0.1274, Validation Loss: 0.1284\n",
      "[Trial 44] Epoch 56/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 46] Epoch 17/60, Training Loss: 0.1275, Validation Loss: 0.2047\n",
      "[Trial 44] Epoch 57/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 44] Epoch 58/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 47] Epoch 8/60, Training Loss: 0.1294, Validation Loss: 0.2299\n",
      "[Trial 40] Epoch 36/60, Training Loss: 0.1275, Validation Loss: 0.2115\n",
      "[Trial 39] Epoch 38/60, Training Loss: 0.1261, Validation Loss: 0.1288\n",
      "[Trial 45] Epoch 20/60, Training Loss: 0.1314, Validation Loss: 0.2337\n",
      "[Trial 43] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1263\n",
      "[Trial 46] Epoch 18/60, Training Loss: 0.1276, Validation Loss: 0.2011\n",
      "[Trial 44] Epoch 59/60, Training Loss: 0.1258, Validation Loss: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:57:47,192] Trial 44 finished with value: 0.12497361352046331 and parameters: {'hidden_dim': 448, 'latent_dim': 128, 'learning_rate': 0.003475671835131112, 'batch_size': 64, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 44] Epoch 60/60, Training Loss: 0.1259, Validation Loss: 0.1250\n",
      "[Trial 47] Epoch 9/60, Training Loss: 0.1290, Validation Loss: 0.2264\n",
      "[Trial 40] Epoch 37/60, Training Loss: 0.1274, Validation Loss: 0.2114\n",
      "[Trial 39] Epoch 39/60, Training Loss: 0.1262, Validation Loss: 0.1285\n",
      "[Trial 45] Epoch 21/60, Training Loss: 0.1311, Validation Loss: 0.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:57:56,881] Trial 43 finished with value: 0.12632756556073824 and parameters: {'hidden_dim': 448, 'latent_dim': 128, 'learning_rate': 0.0031549703972053493, 'batch_size': 16, 'patience': 5}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 43] Epoch 25/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 43] Early stopping after 25 epochs.\n",
      "[Trial 46] Epoch 19/60, Training Loss: 0.1276, Validation Loss: 0.2005\n",
      "[Trial 48] Epoch 1/60, Training Loss: 3.4883, Validation Loss: 0.3559\n",
      "[Trial 47] Epoch 10/60, Training Loss: 0.1289, Validation Loss: 0.2239\n",
      "[Trial 40] Epoch 38/60, Training Loss: 0.1272, Validation Loss: 0.2104\n",
      "[Trial 39] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1286\n",
      "[Trial 45] Epoch 22/60, Training Loss: 0.1308, Validation Loss: 0.2318\n",
      "[Trial 49] Epoch 1/60, Training Loss: 3.7195, Validation Loss: 0.3748\n",
      "[Trial 46] Epoch 20/60, Training Loss: 0.1282, Validation Loss: 0.1959\n",
      "[Trial 48] Epoch 2/60, Training Loss: 0.2359, Validation Loss: 0.2903\n",
      "[Trial 47] Epoch 11/60, Training Loss: 0.1284, Validation Loss: 0.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 11:58:40,523] Trial 39 finished with value: 0.12796198924382526 and parameters: {'hidden_dim': 448, 'latent_dim': 32, 'learning_rate': 0.0003718686540277649, 'batch_size': 16, 'patience': 5}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 49] Epoch 2/60, Training Loss: 0.2409, Validation Loss: 0.2906\n",
      "[Trial 39] Epoch 41/60, Training Loss: 0.1258, Validation Loss: 0.1281\n",
      "[Trial 39] Early stopping after 41 epochs.\n",
      "[Trial 40] Epoch 39/60, Training Loss: 0.1269, Validation Loss: 0.2097\n",
      "[Trial 45] Epoch 23/60, Training Loss: 0.1304, Validation Loss: 0.2302\n",
      "[Trial 46] Epoch 21/60, Training Loss: 0.1270, Validation Loss: 0.1948\n",
      "[Trial 48] Epoch 3/60, Training Loss: 0.1882, Validation Loss: 0.2698\n",
      "[Trial 49] Epoch 3/60, Training Loss: 0.1877, Validation Loss: 0.2677\n",
      "[Trial 47] Epoch 12/60, Training Loss: 0.1289, Validation Loss: 0.2172\n",
      "[Trial 50] Epoch 1/60, Training Loss: 1.9063, Validation Loss: 0.3001\n",
      "[Trial 40] Epoch 40/60, Training Loss: 0.1272, Validation Loss: 0.2085\n",
      "[Trial 45] Epoch 24/60, Training Loss: 0.1298, Validation Loss: 0.2293\n",
      "[Trial 46] Epoch 22/60, Training Loss: 0.1267, Validation Loss: 0.1932\n",
      "[Trial 48] Epoch 4/60, Training Loss: 0.1682, Validation Loss: 0.2590\n",
      "[Trial 49] Epoch 4/60, Training Loss: 0.1677, Validation Loss: 0.2568\n",
      "[Trial 50] Epoch 2/60, Training Loss: 0.1727, Validation Loss: 0.2666\n",
      "[Trial 47] Epoch 13/60, Training Loss: 0.1278, Validation Loss: 0.2148\n",
      "[Trial 40] Epoch 41/60, Training Loss: 0.1272, Validation Loss: 0.2093\n",
      "[Trial 45] Epoch 25/60, Training Loss: 0.1297, Validation Loss: 0.2287\n",
      "[Trial 46] Epoch 23/60, Training Loss: 0.1274, Validation Loss: 0.1911\n",
      "[Trial 48] Epoch 5/60, Training Loss: 0.1577, Validation Loss: 0.2523\n",
      "[Trial 49] Epoch 5/60, Training Loss: 0.1571, Validation Loss: 0.2502\n",
      "[Trial 50] Epoch 3/60, Training Loss: 0.1511, Validation Loss: 0.2553\n",
      "[Trial 47] Epoch 14/60, Training Loss: 0.1278, Validation Loss: 0.2133\n",
      "[Trial 40] Epoch 42/60, Training Loss: 0.1274, Validation Loss: 0.2071\n",
      "[Trial 45] Epoch 26/60, Training Loss: 0.1295, Validation Loss: 0.2274\n",
      "[Trial 46] Epoch 24/60, Training Loss: 0.1271, Validation Loss: 0.1883\n",
      "[Trial 48] Epoch 6/60, Training Loss: 0.1505, Validation Loss: 0.2473\n",
      "[Trial 49] Epoch 6/60, Training Loss: 0.1507, Validation Loss: 0.2445\n",
      "[Trial 50] Epoch 4/60, Training Loss: 0.1426, Validation Loss: 0.2480\n",
      "[Trial 47] Epoch 15/60, Training Loss: 0.1274, Validation Loss: 0.2119\n",
      "[Trial 40] Epoch 43/60, Training Loss: 0.1269, Validation Loss: 0.2059\n",
      "[Trial 45] Epoch 27/60, Training Loss: 0.1294, Validation Loss: 0.2263\n",
      "[Trial 46] Epoch 25/60, Training Loss: 0.1272, Validation Loss: 0.1882\n",
      "[Trial 48] Epoch 7/60, Training Loss: 0.1462, Validation Loss: 0.2436\n",
      "[Trial 49] Epoch 7/60, Training Loss: 0.1460, Validation Loss: 0.2411\n",
      "[Trial 50] Epoch 5/60, Training Loss: 0.1376, Validation Loss: 0.2433\n",
      "[Trial 47] Epoch 16/60, Training Loss: 0.1278, Validation Loss: 0.2076\n",
      "[Trial 45] Epoch 28/60, Training Loss: 0.1291, Validation Loss: 0.2248\n",
      "[Trial 40] Epoch 44/60, Training Loss: 0.1273, Validation Loss: 0.2052\n",
      "[Trial 46] Epoch 26/60, Training Loss: 0.1266, Validation Loss: 0.1847\n",
      "[Trial 48] Epoch 8/60, Training Loss: 0.1427, Validation Loss: 0.2409\n",
      "[Trial 49] Epoch 8/60, Training Loss: 0.1429, Validation Loss: 0.2385\n",
      "[Trial 50] Epoch 6/60, Training Loss: 0.1349, Validation Loss: 0.2392\n",
      "[Trial 47] Epoch 17/60, Training Loss: 0.1274, Validation Loss: 0.2060\n",
      "[Trial 45] Epoch 29/60, Training Loss: 0.1290, Validation Loss: 0.2237\n",
      "[Trial 40] Epoch 45/60, Training Loss: 0.1268, Validation Loss: 0.2054\n",
      "[Trial 46] Epoch 27/60, Training Loss: 0.1273, Validation Loss: 0.1828\n",
      "[Trial 49] Epoch 9/60, Training Loss: 0.1403, Validation Loss: 0.2359\n",
      "[Trial 48] Epoch 9/60, Training Loss: 0.1395, Validation Loss: 0.2381\n",
      "[Trial 50] Epoch 7/60, Training Loss: 0.1333, Validation Loss: 0.2367\n",
      "[Trial 47] Epoch 18/60, Training Loss: 0.1276, Validation Loss: 0.2054\n",
      "[Trial 45] Epoch 30/60, Training Loss: 0.1285, Validation Loss: 0.2228\n",
      "[Trial 40] Epoch 46/60, Training Loss: 0.1268, Validation Loss: 0.2039\n",
      "[Trial 46] Epoch 28/60, Training Loss: 0.1268, Validation Loss: 0.1803\n",
      "[Trial 49] Epoch 10/60, Training Loss: 0.1388, Validation Loss: 0.2333\n",
      "[Trial 48] Epoch 10/60, Training Loss: 0.1376, Validation Loss: 0.2360\n",
      "[Trial 50] Epoch 8/60, Training Loss: 0.1315, Validation Loss: 0.2341\n",
      "[Trial 47] Epoch 19/60, Training Loss: 0.1273, Validation Loss: 0.2016\n",
      "[Trial 45] Epoch 31/60, Training Loss: 0.1285, Validation Loss: 0.2219\n",
      "[Trial 40] Epoch 47/60, Training Loss: 0.1267, Validation Loss: 0.2047\n",
      "[Trial 46] Epoch 29/60, Training Loss: 0.1270, Validation Loss: 0.1790\n",
      "[Trial 49] Epoch 11/60, Training Loss: 0.1372, Validation Loss: 0.2319\n",
      "[Trial 48] Epoch 11/60, Training Loss: 0.1360, Validation Loss: 0.2343\n",
      "[Trial 50] Epoch 9/60, Training Loss: 0.1310, Validation Loss: 0.2307\n",
      "[Trial 47] Epoch 20/60, Training Loss: 0.1269, Validation Loss: 0.1988\n",
      "[Trial 45] Epoch 32/60, Training Loss: 0.1286, Validation Loss: 0.2213\n",
      "[Trial 40] Epoch 48/60, Training Loss: 0.1265, Validation Loss: 0.2021\n",
      "[Trial 46] Epoch 30/60, Training Loss: 0.1267, Validation Loss: 0.1750\n",
      "[Trial 49] Epoch 12/60, Training Loss: 0.1357, Validation Loss: 0.2299\n",
      "[Trial 48] Epoch 12/60, Training Loss: 0.1349, Validation Loss: 0.2329\n",
      "[Trial 50] Epoch 10/60, Training Loss: 0.1302, Validation Loss: 0.2284\n",
      "[Trial 47] Epoch 21/60, Training Loss: 0.1279, Validation Loss: 0.1978\n",
      "[Trial 45] Epoch 33/60, Training Loss: 0.1284, Validation Loss: 0.2207\n",
      "[Trial 40] Epoch 49/60, Training Loss: 0.1264, Validation Loss: 0.2028\n",
      "[Trial 46] Epoch 31/60, Training Loss: 0.1267, Validation Loss: 0.1742\n",
      "[Trial 49] Epoch 13/60, Training Loss: 0.1352, Validation Loss: 0.2281\n",
      "[Trial 48] Epoch 13/60, Training Loss: 0.1338, Validation Loss: 0.2315\n",
      "[Trial 50] Epoch 11/60, Training Loss: 0.1296, Validation Loss: 0.2254\n",
      "[Trial 47] Epoch 22/60, Training Loss: 0.1266, Validation Loss: 0.1963\n",
      "[Trial 45] Epoch 34/60, Training Loss: 0.1284, Validation Loss: 0.2196\n",
      "[Trial 40] Epoch 50/60, Training Loss: 0.1269, Validation Loss: 0.2019\n",
      "[Trial 46] Epoch 32/60, Training Loss: 0.1269, Validation Loss: 0.1712\n",
      "[Trial 49] Epoch 14/60, Training Loss: 0.1341, Validation Loss: 0.2272\n",
      "[Trial 48] Epoch 14/60, Training Loss: 0.1328, Validation Loss: 0.2293\n",
      "[Trial 50] Epoch 12/60, Training Loss: 0.1288, Validation Loss: 0.2239\n",
      "[Trial 47] Epoch 23/60, Training Loss: 0.1273, Validation Loss: 0.1941\n",
      "[Trial 45] Epoch 35/60, Training Loss: 0.1278, Validation Loss: 0.2194\n",
      "[Trial 49] Epoch 15/60, Training Loss: 0.1333, Validation Loss: 0.2256\n",
      "[Trial 40] Epoch 51/60, Training Loss: 0.1265, Validation Loss: 0.2007\n",
      "[Trial 46] Epoch 33/60, Training Loss: 0.1263, Validation Loss: 0.1701\n",
      "[Trial 48] Epoch 15/60, Training Loss: 0.1319, Validation Loss: 0.2281\n",
      "[Trial 50] Epoch 13/60, Training Loss: 0.1288, Validation Loss: 0.2210\n",
      "[Trial 47] Epoch 24/60, Training Loss: 0.1267, Validation Loss: 0.1925\n",
      "[Trial 49] Epoch 16/60, Training Loss: 0.1330, Validation Loss: 0.2239\n",
      "[Trial 45] Epoch 36/60, Training Loss: 0.1282, Validation Loss: 0.2179\n",
      "[Trial 46] Epoch 34/60, Training Loss: 0.1266, Validation Loss: 0.1779\n",
      "[Trial 40] Epoch 52/60, Training Loss: 0.1267, Validation Loss: 0.2003\n",
      "[Trial 48] Epoch 16/60, Training Loss: 0.1315, Validation Loss: 0.2273\n",
      "[Trial 50] Epoch 14/60, Training Loss: 0.1282, Validation Loss: 0.2200\n",
      "[Trial 47] Epoch 25/60, Training Loss: 0.1270, Validation Loss: 0.1904\n",
      "[Trial 49] Epoch 17/60, Training Loss: 0.1320, Validation Loss: 0.2230\n",
      "[Trial 45] Epoch 37/60, Training Loss: 0.1275, Validation Loss: 0.2168\n",
      "[Trial 46] Epoch 35/60, Training Loss: 0.1278, Validation Loss: 0.1672\n",
      "[Trial 40] Epoch 53/60, Training Loss: 0.1263, Validation Loss: 0.1996\n",
      "[Trial 48] Epoch 17/60, Training Loss: 0.1311, Validation Loss: 0.2261\n",
      "[Trial 50] Epoch 15/60, Training Loss: 0.1283, Validation Loss: 0.2177\n",
      "[Trial 47] Epoch 26/60, Training Loss: 0.1266, Validation Loss: 0.1873\n",
      "[Trial 49] Epoch 18/60, Training Loss: 0.1320, Validation Loss: 0.2221\n",
      "[Trial 46] Epoch 36/60, Training Loss: 0.1260, Validation Loss: 0.1660\n",
      "[Trial 45] Epoch 38/60, Training Loss: 0.1277, Validation Loss: 0.2158\n",
      "[Trial 40] Epoch 54/60, Training Loss: 0.1268, Validation Loss: 0.1986\n",
      "[Trial 48] Epoch 18/60, Training Loss: 0.1306, Validation Loss: 0.2249\n",
      "[Trial 50] Epoch 16/60, Training Loss: 0.1277, Validation Loss: 0.2162\n",
      "[Trial 47] Epoch 27/60, Training Loss: 0.1271, Validation Loss: 0.1850\n",
      "[Trial 49] Epoch 19/60, Training Loss: 0.1312, Validation Loss: 0.2209\n",
      "[Trial 46] Epoch 37/60, Training Loss: 0.1262, Validation Loss: 0.1681\n",
      "[Trial 45] Epoch 39/60, Training Loss: 0.1274, Validation Loss: 0.2157\n",
      "[Trial 40] Epoch 55/60, Training Loss: 0.1263, Validation Loss: 0.1978\n",
      "[Trial 48] Epoch 19/60, Training Loss: 0.1302, Validation Loss: 0.2238\n",
      "[Trial 50] Epoch 17/60, Training Loss: 0.1281, Validation Loss: 0.2126\n",
      "[Trial 47] Epoch 28/60, Training Loss: 0.1274, Validation Loss: 0.1824\n",
      "[Trial 49] Epoch 20/60, Training Loss: 0.1307, Validation Loss: 0.2197\n",
      "[Trial 46] Epoch 38/60, Training Loss: 0.1265, Validation Loss: 0.1627\n",
      "[Trial 45] Epoch 40/60, Training Loss: 0.1276, Validation Loss: 0.2150\n",
      "[Trial 40] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1975\n",
      "[Trial 48] Epoch 20/60, Training Loss: 0.1296, Validation Loss: 0.2229\n",
      "[Trial 50] Epoch 18/60, Training Loss: 0.1276, Validation Loss: 0.2118\n",
      "[Trial 49] Epoch 21/60, Training Loss: 0.1303, Validation Loss: 0.2185\n",
      "[Trial 47] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1833\n",
      "[Trial 46] Epoch 39/60, Training Loss: 0.1267, Validation Loss: 0.1674\n",
      "[Trial 45] Epoch 41/60, Training Loss: 0.1274, Validation Loss: 0.2138\n",
      "[Trial 40] Epoch 57/60, Training Loss: 0.1261, Validation Loss: 0.1970\n",
      "[Trial 48] Epoch 21/60, Training Loss: 0.1296, Validation Loss: 0.2219\n",
      "[Trial 50] Epoch 19/60, Training Loss: 0.1274, Validation Loss: 0.2105\n",
      "[Trial 49] Epoch 22/60, Training Loss: 0.1303, Validation Loss: 0.2174\n",
      "[Trial 47] Epoch 30/60, Training Loss: 0.1265, Validation Loss: 0.1794\n",
      "[Trial 46] Epoch 40/60, Training Loss: 0.1271, Validation Loss: 0.1604\n",
      "[Trial 45] Epoch 42/60, Training Loss: 0.1275, Validation Loss: 0.2135\n",
      "[Trial 40] Epoch 58/60, Training Loss: 0.1262, Validation Loss: 0.1963\n",
      "[Trial 48] Epoch 22/60, Training Loss: 0.1292, Validation Loss: 0.2208\n",
      "[Trial 50] Epoch 20/60, Training Loss: 0.1276, Validation Loss: 0.2073\n",
      "[Trial 49] Epoch 23/60, Training Loss: 0.1305, Validation Loss: 0.2166\n",
      "[Trial 47] Epoch 31/60, Training Loss: 0.1270, Validation Loss: 0.1840\n",
      "[Trial 46] Epoch 41/60, Training Loss: 0.1267, Validation Loss: 0.1600\n",
      "[Trial 45] Epoch 43/60, Training Loss: 0.1276, Validation Loss: 0.2125\n",
      "[Trial 40] Epoch 59/60, Training Loss: 0.1266, Validation Loss: 0.1965\n",
      "[Trial 48] Epoch 23/60, Training Loss: 0.1288, Validation Loss: 0.2198\n",
      "[Trial 50] Epoch 21/60, Training Loss: 0.1274, Validation Loss: 0.2051\n",
      "[Trial 49] Epoch 24/60, Training Loss: 0.1298, Validation Loss: 0.2160\n",
      "[Trial 47] Epoch 32/60, Training Loss: 0.1277, Validation Loss: 0.1763\n",
      "[Trial 46] Epoch 42/60, Training Loss: 0.1262, Validation Loss: 0.1566\n",
      "[Trial 45] Epoch 44/60, Training Loss: 0.1270, Validation Loss: 0.2111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:06:48,864] Trial 40 finished with value: 0.1948637622098128 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 6.323650813429166e-05, 'batch_size': 16, 'patience': 5}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 48] Epoch 24/60, Training Loss: 0.1287, Validation Loss: 0.2191\n",
      "[Trial 40] Epoch 60/60, Training Loss: 0.1262, Validation Loss: 0.1949\n",
      "[Trial 50] Epoch 22/60, Training Loss: 0.1272, Validation Loss: 0.2035\n",
      "[Trial 49] Epoch 25/60, Training Loss: 0.1296, Validation Loss: 0.2138\n",
      "[Trial 47] Epoch 33/60, Training Loss: 0.1261, Validation Loss: 0.1741\n",
      "[Trial 46] Epoch 43/60, Training Loss: 0.1264, Validation Loss: 0.1558\n",
      "[Trial 45] Epoch 45/60, Training Loss: 0.1271, Validation Loss: 0.2110\n",
      "[Trial 51] Epoch 1/60, Training Loss: 1.3158, Validation Loss: 0.2719\n",
      "[Trial 48] Epoch 25/60, Training Loss: 0.1285, Validation Loss: 0.2183\n",
      "[Trial 49] Epoch 26/60, Training Loss: 0.1292, Validation Loss: 0.2140\n",
      "[Trial 50] Epoch 23/60, Training Loss: 0.1276, Validation Loss: 0.2015\n",
      "[Trial 47] Epoch 34/60, Training Loss: 0.1265, Validation Loss: 0.1744\n",
      "[Trial 46] Epoch 44/60, Training Loss: 0.1268, Validation Loss: 0.1550\n",
      "[Trial 51] Epoch 2/60, Training Loss: 0.1464, Validation Loss: 0.2549\n",
      "[Trial 45] Epoch 46/60, Training Loss: 0.1272, Validation Loss: 0.2106\n",
      "[Trial 48] Epoch 26/60, Training Loss: 0.1284, Validation Loss: 0.2168\n",
      "[Trial 49] Epoch 27/60, Training Loss: 0.1291, Validation Loss: 0.2126\n",
      "[Trial 50] Epoch 24/60, Training Loss: 0.1273, Validation Loss: 0.2000\n",
      "[Trial 47] Epoch 35/60, Training Loss: 0.1267, Validation Loss: 0.1730\n",
      "[Trial 46] Epoch 45/60, Training Loss: 0.1264, Validation Loss: 0.1535\n",
      "[Trial 51] Epoch 3/60, Training Loss: 0.1361, Validation Loss: 0.2477\n",
      "[Trial 45] Epoch 47/60, Training Loss: 0.1272, Validation Loss: 0.2099\n",
      "[Trial 48] Epoch 27/60, Training Loss: 0.1280, Validation Loss: 0.2157\n",
      "[Trial 49] Epoch 28/60, Training Loss: 0.1290, Validation Loss: 0.2115\n",
      "[Trial 50] Epoch 25/60, Training Loss: 0.1268, Validation Loss: 0.1988\n",
      "[Trial 47] Epoch 36/60, Training Loss: 0.1261, Validation Loss: 0.1710\n",
      "[Trial 46] Epoch 46/60, Training Loss: 0.1262, Validation Loss: 0.1533\n",
      "[Trial 51] Epoch 4/60, Training Loss: 0.1322, Validation Loss: 0.2390\n",
      "[Trial 49] Epoch 29/60, Training Loss: 0.1289, Validation Loss: 0.2102\n",
      "[Trial 45] Epoch 48/60, Training Loss: 0.1268, Validation Loss: 0.2087\n",
      "[Trial 48] Epoch 28/60, Training Loss: 0.1278, Validation Loss: 0.2150\n",
      "[Trial 50] Epoch 26/60, Training Loss: 0.1269, Validation Loss: 0.1954\n",
      "[Trial 47] Epoch 37/60, Training Loss: 0.1261, Validation Loss: 0.1781\n",
      "[Trial 51] Epoch 5/60, Training Loss: 0.1307, Validation Loss: 0.2366\n",
      "[Trial 46] Epoch 47/60, Training Loss: 0.1262, Validation Loss: 0.1505\n",
      "[Trial 49] Epoch 30/60, Training Loss: 0.1291, Validation Loss: 0.2101\n",
      "[Trial 48] Epoch 29/60, Training Loss: 0.1276, Validation Loss: 0.2140\n",
      "[Trial 45] Epoch 49/60, Training Loss: 0.1269, Validation Loss: 0.2083\n",
      "[Trial 50] Epoch 27/60, Training Loss: 0.1271, Validation Loss: 0.1942\n",
      "[Trial 47] Epoch 38/60, Training Loss: 0.1270, Validation Loss: 0.1683\n",
      "[Trial 51] Epoch 6/60, Training Loss: 0.1294, Validation Loss: 0.2327\n",
      "[Trial 49] Epoch 31/60, Training Loss: 0.1288, Validation Loss: 0.2088\n",
      "[Trial 46] Epoch 48/60, Training Loss: 0.1261, Validation Loss: 0.1515\n",
      "[Trial 48] Epoch 30/60, Training Loss: 0.1278, Validation Loss: 0.2128\n",
      "[Trial 45] Epoch 50/60, Training Loss: 0.1270, Validation Loss: 0.2073\n",
      "[Trial 50] Epoch 28/60, Training Loss: 0.1268, Validation Loss: 0.1914\n",
      "[Trial 47] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1673\n",
      "[Trial 51] Epoch 7/60, Training Loss: 0.1284, Validation Loss: 0.2269\n",
      "[Trial 49] Epoch 32/60, Training Loss: 0.1284, Validation Loss: 0.2076\n",
      "[Trial 46] Epoch 49/60, Training Loss: 0.1266, Validation Loss: 0.1498\n",
      "[Trial 48] Epoch 31/60, Training Loss: 0.1272, Validation Loss: 0.2126\n",
      "[Trial 45] Epoch 51/60, Training Loss: 0.1268, Validation Loss: 0.2068\n",
      "[Trial 50] Epoch 29/60, Training Loss: 0.1268, Validation Loss: 0.1903\n",
      "[Trial 47] Epoch 40/60, Training Loss: 0.1263, Validation Loss: 0.1667\n",
      "[Trial 49] Epoch 33/60, Training Loss: 0.1285, Validation Loss: 0.2073\n",
      "[Trial 51] Epoch 8/60, Training Loss: 0.1285, Validation Loss: 0.2254\n",
      "[Trial 46] Epoch 50/60, Training Loss: 0.1258, Validation Loss: 0.1479\n",
      "[Trial 48] Epoch 32/60, Training Loss: 0.1278, Validation Loss: 0.2113\n",
      "[Trial 45] Epoch 52/60, Training Loss: 0.1267, Validation Loss: 0.2058\n",
      "[Trial 50] Epoch 30/60, Training Loss: 0.1265, Validation Loss: 0.1893\n",
      "[Trial 47] Epoch 41/60, Training Loss: 0.1262, Validation Loss: 0.1619\n",
      "[Trial 49] Epoch 34/60, Training Loss: 0.1281, Validation Loss: 0.2064\n",
      "[Trial 51] Epoch 9/60, Training Loss: 0.1274, Validation Loss: 0.2219\n",
      "[Trial 46] Epoch 51/60, Training Loss: 0.1261, Validation Loss: 0.1468\n",
      "[Trial 48] Epoch 33/60, Training Loss: 0.1274, Validation Loss: 0.2109\n",
      "[Trial 50] Epoch 31/60, Training Loss: 0.1266, Validation Loss: 0.1864\n",
      "[Trial 45] Epoch 53/60, Training Loss: 0.1267, Validation Loss: 0.2050\n",
      "[Trial 47] Epoch 42/60, Training Loss: 0.1262, Validation Loss: 0.1637\n",
      "[Trial 49] Epoch 35/60, Training Loss: 0.1281, Validation Loss: 0.2056\n",
      "[Trial 51] Epoch 10/60, Training Loss: 0.1276, Validation Loss: 0.2176\n",
      "[Trial 46] Epoch 52/60, Training Loss: 0.1263, Validation Loss: 0.1466\n",
      "[Trial 48] Epoch 34/60, Training Loss: 0.1276, Validation Loss: 0.2095\n",
      "[Trial 50] Epoch 32/60, Training Loss: 0.1264, Validation Loss: 0.1876\n",
      "[Trial 45] Epoch 54/60, Training Loss: 0.1267, Validation Loss: 0.2038\n",
      "[Trial 47] Epoch 43/60, Training Loss: 0.1272, Validation Loss: 0.1619\n",
      "[Trial 49] Epoch 36/60, Training Loss: 0.1282, Validation Loss: 0.2052\n",
      "[Trial 51] Epoch 11/60, Training Loss: 0.1276, Validation Loss: 0.2121\n",
      "[Trial 46] Epoch 53/60, Training Loss: 0.1261, Validation Loss: 0.1450\n",
      "[Trial 48] Epoch 35/60, Training Loss: 0.1272, Validation Loss: 0.2085\n",
      "[Trial 50] Epoch 33/60, Training Loss: 0.1273, Validation Loss: 0.1835\n",
      "[Trial 45] Epoch 55/60, Training Loss: 0.1269, Validation Loss: 0.2044\n",
      "[Trial 47] Epoch 44/60, Training Loss: 0.1260, Validation Loss: 0.1593\n",
      "[Trial 49] Epoch 37/60, Training Loss: 0.1280, Validation Loss: 0.2032\n",
      "[Trial 51] Epoch 12/60, Training Loss: 0.1273, Validation Loss: 0.2076\n",
      "[Trial 46] Epoch 54/60, Training Loss: 0.1262, Validation Loss: 0.1462\n",
      "[Trial 48] Epoch 36/60, Training Loss: 0.1270, Validation Loss: 0.2076\n",
      "[Trial 50] Epoch 34/60, Training Loss: 0.1263, Validation Loss: 0.1816\n",
      "[Trial 45] Epoch 56/60, Training Loss: 0.1268, Validation Loss: 0.2025\n",
      "[Trial 49] Epoch 38/60, Training Loss: 0.1280, Validation Loss: 0.2021\n",
      "[Trial 47] Epoch 45/60, Training Loss: 0.1263, Validation Loss: 0.1604\n",
      "[Trial 51] Epoch 13/60, Training Loss: 0.1271, Validation Loss: 0.2054\n",
      "[Trial 46] Epoch 55/60, Training Loss: 0.1261, Validation Loss: 0.1444\n",
      "[Trial 48] Epoch 37/60, Training Loss: 0.1268, Validation Loss: 0.2076\n",
      "[Trial 50] Epoch 35/60, Training Loss: 0.1263, Validation Loss: 0.1796\n",
      "[Trial 45] Epoch 57/60, Training Loss: 0.1267, Validation Loss: 0.2025\n",
      "[Trial 49] Epoch 39/60, Training Loss: 0.1277, Validation Loss: 0.2016\n",
      "[Trial 47] Epoch 46/60, Training Loss: 0.1262, Validation Loss: 0.1591\n",
      "[Trial 51] Epoch 14/60, Training Loss: 0.1269, Validation Loss: 0.2020\n",
      "[Trial 48] Epoch 38/60, Training Loss: 0.1272, Validation Loss: 0.2064\n",
      "[Trial 46] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1435\n",
      "[Trial 50] Epoch 36/60, Training Loss: 0.1266, Validation Loss: 0.1827\n",
      "[Trial 45] Epoch 58/60, Training Loss: 0.1264, Validation Loss: 0.2014\n",
      "[Trial 49] Epoch 40/60, Training Loss: 0.1276, Validation Loss: 0.2009\n",
      "[Trial 47] Epoch 47/60, Training Loss: 0.1265, Validation Loss: 0.1574\n",
      "[Trial 51] Epoch 15/60, Training Loss: 0.1276, Validation Loss: 0.1993\n",
      "[Trial 48] Epoch 39/60, Training Loss: 0.1265, Validation Loss: 0.2057\n",
      "[Trial 46] Epoch 57/60, Training Loss: 0.1261, Validation Loss: 0.1421\n",
      "[Trial 50] Epoch 37/60, Training Loss: 0.1270, Validation Loss: 0.1772\n",
      "[Trial 45] Epoch 59/60, Training Loss: 0.1264, Validation Loss: 0.2014\n",
      "[Trial 49] Epoch 41/60, Training Loss: 0.1280, Validation Loss: 0.2002\n",
      "[Trial 47] Epoch 48/60, Training Loss: 0.1261, Validation Loss: 0.1556\n",
      "[Trial 51] Epoch 16/60, Training Loss: 0.1266, Validation Loss: 0.1949\n",
      "[Trial 48] Epoch 40/60, Training Loss: 0.1267, Validation Loss: 0.2050\n",
      "[Trial 46] Epoch 58/60, Training Loss: 0.1261, Validation Loss: 0.1436\n",
      "[Trial 50] Epoch 38/60, Training Loss: 0.1265, Validation Loss: 0.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:12:51,910] Trial 45 finished with value: 0.2000380590558052 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 5.752874329147863e-05, 'batch_size': 16, 'patience': 5}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 45] Epoch 60/60, Training Loss: 0.1269, Validation Loss: 0.2000\n",
      "[Trial 49] Epoch 42/60, Training Loss: 0.1279, Validation Loss: 0.1991\n",
      "[Trial 47] Epoch 49/60, Training Loss: 0.1259, Validation Loss: 0.1571\n",
      "[Trial 51] Epoch 17/60, Training Loss: 0.1269, Validation Loss: 0.1929\n",
      "[Trial 48] Epoch 41/60, Training Loss: 0.1269, Validation Loss: 0.2030\n",
      "[Trial 50] Epoch 39/60, Training Loss: 0.1262, Validation Loss: 0.1739\n",
      "[Trial 46] Epoch 59/60, Training Loss: 0.1262, Validation Loss: 0.1407\n",
      "[Trial 49] Epoch 43/60, Training Loss: 0.1278, Validation Loss: 0.1989\n",
      "[Trial 52] Epoch 1/60, Training Loss: 24.0758, Validation Loss: 8.2071\n",
      "[Trial 47] Epoch 50/60, Training Loss: 0.1264, Validation Loss: 0.1533\n",
      "[Trial 51] Epoch 18/60, Training Loss: 0.1273, Validation Loss: 0.1921\n",
      "[Trial 48] Epoch 42/60, Training Loss: 0.1264, Validation Loss: 0.2036\n",
      "[Trial 50] Epoch 40/60, Training Loss: 0.1262, Validation Loss: 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:13:32,075] Trial 46 finished with value: 0.13950512893497943 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.0002954480994275112, 'batch_size': 16, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 46] Epoch 60/60, Training Loss: 0.1263, Validation Loss: 0.1395\n",
      "[Trial 49] Epoch 44/60, Training Loss: 0.1275, Validation Loss: 0.1974\n",
      "[Trial 52] Epoch 2/60, Training Loss: 3.2998, Validation Loss: 1.3055\n",
      "[Trial 53] Epoch 1/60, Training Loss: 4.5746, Validation Loss: 0.3195\n",
      "[Trial 51] Epoch 19/60, Training Loss: 0.1263, Validation Loss: 0.1861\n",
      "[Trial 47] Epoch 51/60, Training Loss: 0.1261, Validation Loss: 0.1536\n",
      "[Trial 53] Epoch 2/60, Training Loss: 0.1937, Validation Loss: 0.2496\n",
      "[Trial 48] Epoch 43/60, Training Loss: 0.1265, Validation Loss: 0.2025\n",
      "[Trial 50] Epoch 41/60, Training Loss: 0.1262, Validation Loss: 0.1719\n",
      "[Trial 53] Epoch 3/60, Training Loss: 0.1583, Validation Loss: 0.2303\n",
      "[Trial 49] Epoch 45/60, Training Loss: 0.1276, Validation Loss: 0.1971\n",
      "[Trial 52] Epoch 3/60, Training Loss: 0.8773, Validation Loss: 0.7564\n",
      "[Trial 53] Epoch 4/60, Training Loss: 0.1468, Validation Loss: 0.2197\n",
      "[Trial 51] Epoch 20/60, Training Loss: 0.1271, Validation Loss: 0.1847\n",
      "[Trial 47] Epoch 52/60, Training Loss: 0.1265, Validation Loss: 0.1531\n",
      "[Trial 53] Epoch 5/60, Training Loss: 0.1413, Validation Loss: 0.2127\n",
      "[Trial 48] Epoch 44/60, Training Loss: 0.1265, Validation Loss: 0.2012\n",
      "[Trial 50] Epoch 42/60, Training Loss: 0.1261, Validation Loss: 0.1698\n",
      "[Trial 49] Epoch 46/60, Training Loss: 0.1278, Validation Loss: 0.1955\n",
      "[Trial 52] Epoch 4/60, Training Loss: 0.5977, Validation Loss: 0.6091\n",
      "[Trial 53] Epoch 6/60, Training Loss: 0.1379, Validation Loss: 0.2047\n",
      "[Trial 53] Epoch 7/60, Training Loss: 0.1362, Validation Loss: 0.1986\n",
      "[Trial 51] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1804\n",
      "[Trial 47] Epoch 53/60, Training Loss: 0.1263, Validation Loss: 0.1523\n",
      "[Trial 53] Epoch 8/60, Training Loss: 0.1346, Validation Loss: 0.1943\n",
      "[Trial 48] Epoch 45/60, Training Loss: 0.1268, Validation Loss: 0.2004\n",
      "[Trial 49] Epoch 47/60, Training Loss: 0.1276, Validation Loss: 0.1955\n",
      "[Trial 50] Epoch 43/60, Training Loss: 0.1266, Validation Loss: 0.1718\n",
      "[Trial 52] Epoch 5/60, Training Loss: 0.4823, Validation Loss: 0.5243\n",
      "[Trial 53] Epoch 9/60, Training Loss: 0.1331, Validation Loss: 0.1876\n",
      "[Trial 51] Epoch 22/60, Training Loss: 0.1263, Validation Loss: 0.1768\n",
      "[Trial 53] Epoch 10/60, Training Loss: 0.1324, Validation Loss: 0.1836\n",
      "[Trial 47] Epoch 54/60, Training Loss: 0.1261, Validation Loss: 0.1502\n",
      "[Trial 53] Epoch 11/60, Training Loss: 0.1315, Validation Loss: 0.1781\n",
      "[Trial 48] Epoch 46/60, Training Loss: 0.1269, Validation Loss: 0.2014\n",
      "[Trial 49] Epoch 48/60, Training Loss: 0.1276, Validation Loss: 0.1946\n",
      "[Trial 52] Epoch 6/60, Training Loss: 0.4078, Validation Loss: 0.4687\n",
      "[Trial 50] Epoch 44/60, Training Loss: 0.1264, Validation Loss: 0.1680\n",
      "[Trial 53] Epoch 12/60, Training Loss: 0.1311, Validation Loss: 0.1751\n",
      "[Trial 51] Epoch 23/60, Training Loss: 0.1264, Validation Loss: 0.1776\n",
      "[Trial 47] Epoch 55/60, Training Loss: 0.1260, Validation Loss: 0.1501\n",
      "[Trial 53] Epoch 13/60, Training Loss: 0.1303, Validation Loss: 0.1708\n",
      "[Trial 49] Epoch 49/60, Training Loss: 0.1276, Validation Loss: 0.1942\n",
      "[Trial 53] Epoch 14/60, Training Loss: 0.1299, Validation Loss: 0.1682\n",
      "[Trial 48] Epoch 47/60, Training Loss: 0.1267, Validation Loss: 0.1991\n",
      "[Trial 52] Epoch 7/60, Training Loss: 0.3572, Validation Loss: 0.4301\n",
      "[Trial 50] Epoch 45/60, Training Loss: 0.1260, Validation Loss: 0.1664\n",
      "[Trial 53] Epoch 15/60, Training Loss: 0.1298, Validation Loss: 0.1635\n",
      "[Trial 51] Epoch 24/60, Training Loss: 0.1277, Validation Loss: 0.1734\n",
      "[Trial 47] Epoch 56/60, Training Loss: 0.1259, Validation Loss: 0.1492\n",
      "[Trial 53] Epoch 16/60, Training Loss: 0.1294, Validation Loss: 0.1622\n",
      "[Trial 49] Epoch 50/60, Training Loss: 0.1273, Validation Loss: 0.1930\n",
      "[Trial 48] Epoch 48/60, Training Loss: 0.1261, Validation Loss: 0.1981\n",
      "[Trial 52] Epoch 8/60, Training Loss: 0.3198, Validation Loss: 0.4021\n",
      "[Trial 53] Epoch 17/60, Training Loss: 0.1292, Validation Loss: 0.1581\n",
      "[Trial 50] Epoch 46/60, Training Loss: 0.1261, Validation Loss: 0.1645\n",
      "[Trial 53] Epoch 18/60, Training Loss: 0.1291, Validation Loss: 0.1544\n",
      "[Trial 51] Epoch 25/60, Training Loss: 0.1259, Validation Loss: 0.1711\n",
      "[Trial 47] Epoch 57/60, Training Loss: 0.1262, Validation Loss: 0.1483\n",
      "[Trial 53] Epoch 19/60, Training Loss: 0.1285, Validation Loss: 0.1520\n",
      "[Trial 49] Epoch 51/60, Training Loss: 0.1274, Validation Loss: 0.1922\n",
      "[Trial 52] Epoch 9/60, Training Loss: 0.2922, Validation Loss: 0.3800\n",
      "[Trial 48] Epoch 49/60, Training Loss: 0.1262, Validation Loss: 0.1974\n",
      "[Trial 50] Epoch 47/60, Training Loss: 0.1265, Validation Loss: 0.1638\n",
      "[Trial 53] Epoch 20/60, Training Loss: 0.1284, Validation Loss: 0.1489\n",
      "[Trial 53] Epoch 21/60, Training Loss: 0.1285, Validation Loss: 0.1458\n",
      "[Trial 51] Epoch 26/60, Training Loss: 0.1268, Validation Loss: 0.1705\n",
      "[Trial 47] Epoch 58/60, Training Loss: 0.1261, Validation Loss: 0.1470\n",
      "[Trial 53] Epoch 22/60, Training Loss: 0.1283, Validation Loss: 0.1451\n",
      "[Trial 49] Epoch 52/60, Training Loss: 0.1275, Validation Loss: 0.1918\n",
      "[Trial 52] Epoch 10/60, Training Loss: 0.2695, Validation Loss: 0.3627\n",
      "[Trial 48] Epoch 50/60, Training Loss: 0.1265, Validation Loss: 0.1966\n",
      "[Trial 50] Epoch 48/60, Training Loss: 0.1261, Validation Loss: 0.1639\n",
      "[Trial 53] Epoch 23/60, Training Loss: 0.1280, Validation Loss: 0.1420\n",
      "[Trial 51] Epoch 27/60, Training Loss: 0.1265, Validation Loss: 0.1661\n",
      "[Trial 53] Epoch 24/60, Training Loss: 0.1281, Validation Loss: 0.1435\n",
      "[Trial 47] Epoch 59/60, Training Loss: 0.1266, Validation Loss: 0.1468\n",
      "[Trial 49] Epoch 53/60, Training Loss: 0.1272, Validation Loss: 0.1897\n",
      "[Trial 53] Epoch 25/60, Training Loss: 0.1281, Validation Loss: 0.1416\n",
      "[Trial 52] Epoch 11/60, Training Loss: 0.2517, Validation Loss: 0.3486\n",
      "[Trial 48] Epoch 51/60, Training Loss: 0.1266, Validation Loss: 0.1962\n",
      "[Trial 50] Epoch 49/60, Training Loss: 0.1263, Validation Loss: 0.1614\n",
      "[Trial 53] Epoch 26/60, Training Loss: 0.1289, Validation Loss: 0.1397\n",
      "[Trial 51] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1636\n",
      "[Trial 53] Epoch 27/60, Training Loss: 0.1275, Validation Loss: 0.1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:17:08,366] Trial 47 finished with value: 0.1453121657172839 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.0002668302659950614, 'batch_size': 16, 'patience': 5}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 47] Epoch 60/60, Training Loss: 0.1265, Validation Loss: 0.1453\n",
      "[Trial 49] Epoch 54/60, Training Loss: 0.1277, Validation Loss: 0.1895\n",
      "[Trial 52] Epoch 12/60, Training Loss: 0.2368, Validation Loss: 0.3372\n",
      "[Trial 53] Epoch 28/60, Training Loss: 0.1276, Validation Loss: 0.1366\n",
      "[Trial 54] Epoch 1/60, Training Loss: 7.2350, Validation Loss: 0.4499\n",
      "[Trial 48] Epoch 52/60, Training Loss: 0.1262, Validation Loss: 0.1950\n",
      "[Trial 50] Epoch 50/60, Training Loss: 0.1257, Validation Loss: 0.1605\n",
      "[Trial 53] Epoch 29/60, Training Loss: 0.1279, Validation Loss: 0.1366\n",
      "[Trial 54] Epoch 2/60, Training Loss: 0.2709, Validation Loss: 0.3157\n",
      "[Trial 51] Epoch 29/60, Training Loss: 0.1264, Validation Loss: 0.1641\n",
      "[Trial 53] Epoch 30/60, Training Loss: 0.1288, Validation Loss: 0.1357\n",
      "[Trial 49] Epoch 55/60, Training Loss: 0.1273, Validation Loss: 0.1891\n",
      "[Trial 54] Epoch 3/60, Training Loss: 0.1971, Validation Loss: 0.2786\n",
      "[Trial 52] Epoch 13/60, Training Loss: 0.2251, Validation Loss: 0.3276\n",
      "[Trial 53] Epoch 31/60, Training Loss: 0.1275, Validation Loss: 0.1340\n",
      "[Trial 48] Epoch 53/60, Training Loss: 0.1266, Validation Loss: 0.1945\n",
      "[Trial 54] Epoch 4/60, Training Loss: 0.1729, Validation Loss: 0.2611\n",
      "[Trial 50] Epoch 51/60, Training Loss: 0.1266, Validation Loss: 0.1594\n",
      "[Trial 53] Epoch 32/60, Training Loss: 0.1290, Validation Loss: 0.1342\n",
      "[Trial 54] Epoch 5/60, Training Loss: 0.1605, Validation Loss: 0.2500\n",
      "[Trial 51] Epoch 30/60, Training Loss: 0.1277, Validation Loss: 0.1604\n",
      "[Trial 49] Epoch 56/60, Training Loss: 0.1275, Validation Loss: 0.1883\n",
      "[Trial 53] Epoch 33/60, Training Loss: 0.1282, Validation Loss: 0.1326\n",
      "[Trial 54] Epoch 6/60, Training Loss: 0.1529, Validation Loss: 0.2402\n",
      "[Trial 52] Epoch 14/60, Training Loss: 0.2148, Validation Loss: 0.3194\n",
      "[Trial 53] Epoch 34/60, Training Loss: 0.1270, Validation Loss: 0.1311\n",
      "[Trial 48] Epoch 54/60, Training Loss: 0.1266, Validation Loss: 0.1938\n",
      "[Trial 54] Epoch 7/60, Training Loss: 0.1480, Validation Loss: 0.2358\n",
      "[Trial 50] Epoch 52/60, Training Loss: 0.1259, Validation Loss: 0.1587\n",
      "[Trial 53] Epoch 35/60, Training Loss: 0.1272, Validation Loss: 0.1310\n",
      "[Trial 51] Epoch 31/60, Training Loss: 0.1256, Validation Loss: 0.1572\n",
      "[Trial 54] Epoch 8/60, Training Loss: 0.1446, Validation Loss: 0.2301\n",
      "[Trial 49] Epoch 57/60, Training Loss: 0.1271, Validation Loss: 0.1874\n",
      "[Trial 53] Epoch 36/60, Training Loss: 0.1278, Validation Loss: 0.1309\n",
      "[Trial 52] Epoch 15/60, Training Loss: 0.2061, Validation Loss: 0.3127\n",
      "[Trial 54] Epoch 9/60, Training Loss: 0.1420, Validation Loss: 0.2255\n",
      "[Trial 48] Epoch 55/60, Training Loss: 0.1263, Validation Loss: 0.1929\n",
      "[Trial 53] Epoch 37/60, Training Loss: 0.1275, Validation Loss: 0.1302\n",
      "[Trial 54] Epoch 10/60, Training Loss: 0.1399, Validation Loss: 0.2198\n",
      "[Trial 50] Epoch 53/60, Training Loss: 0.1261, Validation Loss: 0.1587\n",
      "[Trial 51] Epoch 32/60, Training Loss: 0.1259, Validation Loss: 0.1567\n",
      "[Trial 53] Epoch 38/60, Training Loss: 0.1295, Validation Loss: 0.1300\n",
      "[Trial 54] Epoch 11/60, Training Loss: 0.1383, Validation Loss: 0.2163\n",
      "[Trial 49] Epoch 58/60, Training Loss: 0.1273, Validation Loss: 0.1865\n",
      "[Trial 52] Epoch 16/60, Training Loss: 0.1983, Validation Loss: 0.3066\n",
      "[Trial 53] Epoch 39/60, Training Loss: 0.1277, Validation Loss: 0.1300\n",
      "[Trial 54] Epoch 12/60, Training Loss: 0.1370, Validation Loss: 0.2123\n",
      "[Trial 48] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1923\n",
      "[Trial 53] Epoch 40/60, Training Loss: 0.1269, Validation Loss: 0.1289\n",
      "[Trial 54] Epoch 13/60, Training Loss: 0.1358, Validation Loss: 0.2084\n",
      "[Trial 50] Epoch 54/60, Training Loss: 0.1268, Validation Loss: 0.1573\n",
      "[Trial 51] Epoch 33/60, Training Loss: 0.1259, Validation Loss: 0.1555\n",
      "[Trial 49] Epoch 59/60, Training Loss: 0.1271, Validation Loss: 0.1861\n",
      "[Trial 53] Epoch 41/60, Training Loss: 0.1273, Validation Loss: 0.1297\n",
      "[Trial 54] Epoch 14/60, Training Loss: 0.1350, Validation Loss: 0.2053\n",
      "[Trial 52] Epoch 17/60, Training Loss: 0.1925, Validation Loss: 0.3016\n",
      "[Trial 53] Epoch 42/60, Training Loss: 0.1281, Validation Loss: 0.1286\n",
      "[Trial 54] Epoch 15/60, Training Loss: 0.1344, Validation Loss: 0.2009\n",
      "[Trial 48] Epoch 57/60, Training Loss: 0.1264, Validation Loss: 0.1927\n",
      "[Trial 50] Epoch 55/60, Training Loss: 0.1256, Validation Loss: 0.1552\n",
      "[Trial 53] Epoch 43/60, Training Loss: 0.1281, Validation Loss: 0.1286\n",
      "[Trial 54] Epoch 16/60, Training Loss: 0.1337, Validation Loss: 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:19:21,732] Trial 49 finished with value: 0.184931243956089 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 7.389901345700854e-05, 'batch_size': 16, 'patience': 9}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 49] Epoch 60/60, Training Loss: 0.1272, Validation Loss: 0.1849\n",
      "[Trial 51] Epoch 34/60, Training Loss: 0.1265, Validation Loss: 0.1631\n",
      "[Trial 53] Epoch 44/60, Training Loss: 0.1272, Validation Loss: 0.1281\n",
      "[Trial 54] Epoch 17/60, Training Loss: 0.1331, Validation Loss: 0.1940\n",
      "[Trial 52] Epoch 18/60, Training Loss: 0.1861, Validation Loss: 0.2969\n",
      "[Trial 55] Epoch 1/60, Training Loss: 75.2181, Validation Loss: 51.8798\n",
      "[Trial 53] Epoch 45/60, Training Loss: 0.1281, Validation Loss: 0.1269\n",
      "[Trial 54] Epoch 18/60, Training Loss: 0.1324, Validation Loss: 0.1910\n",
      "[Trial 55] Epoch 2/60, Training Loss: 38.3251, Validation Loss: 24.8056\n",
      "[Trial 48] Epoch 58/60, Training Loss: 0.1264, Validation Loss: 0.1908\n",
      "[Trial 50] Epoch 56/60, Training Loss: 0.1257, Validation Loss: 0.1551\n",
      "[Trial 53] Epoch 46/60, Training Loss: 0.1267, Validation Loss: 0.1280\n",
      "[Trial 54] Epoch 19/60, Training Loss: 0.1323, Validation Loss: 0.1877\n",
      "[Trial 55] Epoch 3/60, Training Loss: 15.9611, Validation Loss: 8.7775\n",
      "[Trial 51] Epoch 35/60, Training Loss: 0.1265, Validation Loss: 0.1508\n",
      "[Trial 53] Epoch 47/60, Training Loss: 0.1343, Validation Loss: 0.1287\n",
      "[Trial 54] Epoch 20/60, Training Loss: 0.1317, Validation Loss: 0.1839\n",
      "[Trial 55] Epoch 4/60, Training Loss: 5.7174, Validation Loss: 3.6206\n",
      "[Trial 52] Epoch 19/60, Training Loss: 0.1815, Validation Loss: 0.2931\n",
      "[Trial 53] Epoch 48/60, Training Loss: 0.1271, Validation Loss: 0.1270\n",
      "[Trial 55] Epoch 5/60, Training Loss: 2.7430, Validation Loss: 2.1689\n",
      "[Trial 54] Epoch 21/60, Training Loss: 0.1313, Validation Loss: 0.1804\n",
      "[Trial 48] Epoch 59/60, Training Loss: 0.1261, Validation Loss: 0.1900\n",
      "[Trial 50] Epoch 57/60, Training Loss: 0.1261, Validation Loss: 0.1554\n",
      "[Trial 55] Epoch 6/60, Training Loss: 1.8260, Validation Loss: 1.6425\n",
      "[Trial 53] Epoch 49/60, Training Loss: 0.1265, Validation Loss: 0.1277\n",
      "[Trial 54] Epoch 22/60, Training Loss: 0.1311, Validation Loss: 0.1791\n",
      "[Trial 51] Epoch 36/60, Training Loss: 0.1257, Validation Loss: 0.1514\n",
      "[Trial 55] Epoch 7/60, Training Loss: 1.4383, Validation Loss: 1.3726\n",
      "[Trial 53] Epoch 50/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 52] Epoch 20/60, Training Loss: 0.1765, Validation Loss: 0.2892\n",
      "[Trial 54] Epoch 23/60, Training Loss: 0.1307, Validation Loss: 0.1752\n",
      "[Trial 55] Epoch 8/60, Training Loss: 1.2112, Validation Loss: 1.1964\n",
      "[Trial 53] Epoch 51/60, Training Loss: 0.1263, Validation Loss: 0.1258\n",
      "[Trial 54] Epoch 24/60, Training Loss: 0.1304, Validation Loss: 0.1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:20:29,904] Trial 48 finished with value: 0.1900342697898547 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 7.379760555557267e-05, 'batch_size': 16, 'patience': 5}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 50] Epoch 58/60, Training Loss: 0.1261, Validation Loss: 0.1531\n",
      "[Trial 48] Epoch 60/60, Training Loss: 0.1263, Validation Loss: 0.1923\n",
      "[Trial 55] Epoch 9/60, Training Loss: 1.0551, Validation Loss: 1.0694\n",
      "[Trial 53] Epoch 52/60, Training Loss: 0.1268, Validation Loss: 0.1266\n",
      "[Trial 54] Epoch 25/60, Training Loss: 0.1302, Validation Loss: 0.1698\n",
      "[Trial 51] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1502\n",
      "[Trial 55] Epoch 10/60, Training Loss: 0.9389, Validation Loss: 0.9722\n",
      "[Trial 56] Epoch 1/60, Training Loss: 21.7609, Validation Loss: 1.2728\n",
      "[Trial 52] Epoch 21/60, Training Loss: 0.1726, Validation Loss: 0.2858\n",
      "[Trial 53] Epoch 53/60, Training Loss: 0.1264, Validation Loss: 0.1260\n",
      "[Trial 54] Epoch 26/60, Training Loss: 0.1302, Validation Loss: 0.1667\n",
      "[Trial 55] Epoch 11/60, Training Loss: 0.8478, Validation Loss: 0.8938\n",
      "[Trial 56] Epoch 2/60, Training Loss: 0.8183, Validation Loss: 0.6419\n",
      "[Trial 53] Epoch 54/60, Training Loss: 0.1269, Validation Loss: 0.1268\n",
      "[Trial 54] Epoch 27/60, Training Loss: 0.1298, Validation Loss: 0.1658\n",
      "[Trial 50] Epoch 59/60, Training Loss: 0.1260, Validation Loss: 0.1518\n",
      "[Trial 55] Epoch 12/60, Training Loss: 0.7733, Validation Loss: 0.8295\n",
      "[Trial 56] Epoch 3/60, Training Loss: 0.4831, Validation Loss: 0.4691\n",
      "[Trial 51] Epoch 38/60, Training Loss: 0.1260, Validation Loss: 0.1492\n",
      "[Trial 53] Epoch 55/60, Training Loss: 0.1305, Validation Loss: 0.1269\n",
      "[Trial 54] Epoch 28/60, Training Loss: 0.1294, Validation Loss: 0.1627\n",
      "[Trial 55] Epoch 13/60, Training Loss: 0.7126, Validation Loss: 0.7764\n",
      "[Trial 56] Epoch 4/60, Training Loss: 0.3546, Validation Loss: 0.3872\n",
      "[Trial 52] Epoch 22/60, Training Loss: 0.1690, Validation Loss: 0.2828\n",
      "[Trial 53] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1267\n",
      "[Trial 54] Epoch 29/60, Training Loss: 0.1294, Validation Loss: 0.1597\n",
      "[Trial 55] Epoch 14/60, Training Loss: 0.6617, Validation Loss: 0.7323\n",
      "[Trial 56] Epoch 5/60, Training Loss: 0.2876, Validation Loss: 0.3404\n",
      "[Trial 53] Epoch 57/60, Training Loss: 0.1262, Validation Loss: 0.1259\n",
      "[Trial 54] Epoch 30/60, Training Loss: 0.1292, Validation Loss: 0.1576\n",
      "[Trial 55] Epoch 15/60, Training Loss: 0.6178, Validation Loss: 0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:21:18,975] Trial 50 finished with value: 0.15169738133748373 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.0001871641171306353, 'batch_size': 16, 'patience': 9}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 50] Epoch 60/60, Training Loss: 0.1258, Validation Loss: 0.1517\n",
      "[Trial 56] Epoch 6/60, Training Loss: 0.2474, Validation Loss: 0.3118\n",
      "[Trial 51] Epoch 39/60, Training Loss: 0.1260, Validation Loss: 0.1484\n",
      "[Trial 53] Epoch 58/60, Training Loss: 0.1260, Validation Loss: 0.1258\n",
      "[Trial 55] Epoch 16/60, Training Loss: 0.5791, Validation Loss: 0.6595\n",
      "[Trial 54] Epoch 31/60, Training Loss: 0.1291, Validation Loss: 0.1555\n",
      "[Trial 57] Epoch 1/60, Training Loss: 6.0225, Validation Loss: 0.3918\n",
      "[Trial 52] Epoch 23/60, Training Loss: 0.1659, Validation Loss: 0.2801\n",
      "[Trial 56] Epoch 7/60, Training Loss: 0.2222, Validation Loss: 0.2929\n",
      "[Trial 55] Epoch 17/60, Training Loss: 0.5457, Validation Loss: 0.6299\n",
      "[Trial 53] Epoch 59/60, Training Loss: 0.1260, Validation Loss: 0.1258\n",
      "[Trial 54] Epoch 32/60, Training Loss: 0.1291, Validation Loss: 0.1549\n",
      "[Trial 57] Epoch 2/60, Training Loss: 0.2282, Validation Loss: 0.2848\n",
      "[Trial 56] Epoch 8/60, Training Loss: 0.2054, Validation Loss: 0.2792\n",
      "[Trial 55] Epoch 18/60, Training Loss: 0.5164, Validation Loss: 0.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:21:44,507] Trial 53 finished with value: 0.12571726044019063 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.0008773281154043574, 'batch_size': 64, 'patience': 9}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 53] Epoch 60/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 54] Epoch 33/60, Training Loss: 0.1287, Validation Loss: 0.1513\n",
      "[Trial 57] Epoch 3/60, Training Loss: 0.1744, Validation Loss: 0.2578\n",
      "[Trial 51] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1457\n",
      "[Trial 56] Epoch 9/60, Training Loss: 0.1932, Validation Loss: 0.2688\n",
      "[Trial 55] Epoch 19/60, Training Loss: 0.4902, Validation Loss: 0.5802\n",
      "[Trial 57] Epoch 4/60, Training Loss: 0.1576, Validation Loss: 0.2440\n",
      "[Trial 54] Epoch 34/60, Training Loss: 0.1290, Validation Loss: 0.1510\n",
      "[Trial 52] Epoch 24/60, Training Loss: 0.1632, Validation Loss: 0.2775\n",
      "[Trial 56] Epoch 10/60, Training Loss: 0.1839, Validation Loss: 0.2609\n",
      "[Trial 55] Epoch 20/60, Training Loss: 0.4665, Validation Loss: 0.5588\n",
      "[Trial 57] Epoch 5/60, Training Loss: 0.1486, Validation Loss: 0.2341\n",
      "[Trial 54] Epoch 35/60, Training Loss: 0.1297, Validation Loss: 0.1495\n",
      "[Trial 55] Epoch 21/60, Training Loss: 0.4446, Validation Loss: 0.5392\n",
      "[Trial 56] Epoch 11/60, Training Loss: 0.1769, Validation Loss: 0.2542\n",
      "[Trial 57] Epoch 6/60, Training Loss: 0.1437, Validation Loss: 0.2266\n",
      "[Trial 54] Epoch 36/60, Training Loss: 0.1285, Validation Loss: 0.1476\n",
      "[Trial 51] Epoch 41/60, Training Loss: 0.1264, Validation Loss: 0.1452\n",
      "[Trial 55] Epoch 22/60, Training Loss: 0.4254, Validation Loss: 0.5218\n",
      "[Trial 56] Epoch 12/60, Training Loss: 0.1713, Validation Loss: 0.2488\n",
      "[Trial 57] Epoch 7/60, Training Loss: 0.1404, Validation Loss: 0.2196\n",
      "[Trial 54] Epoch 37/60, Training Loss: 0.1284, Validation Loss: 0.1463\n",
      "[Trial 58] Epoch 1/60, Training Loss: 0.5216, Validation Loss: 0.1797\n",
      "[Trial 52] Epoch 25/60, Training Loss: 0.1601, Validation Loss: 0.2750\n",
      "[Trial 55] Epoch 23/60, Training Loss: 0.4078, Validation Loss: 0.5060\n",
      "[Trial 56] Epoch 13/60, Training Loss: 0.1668, Validation Loss: 0.2437\n",
      "[Trial 57] Epoch 8/60, Training Loss: 0.1381, Validation Loss: 0.2137\n",
      "[Trial 54] Epoch 38/60, Training Loss: 0.1283, Validation Loss: 0.1442\n",
      "[Trial 55] Epoch 24/60, Training Loss: 0.3920, Validation Loss: 0.4916\n",
      "[Trial 56] Epoch 14/60, Training Loss: 0.1626, Validation Loss: 0.2397\n",
      "[Trial 57] Epoch 9/60, Training Loss: 0.1362, Validation Loss: 0.2087\n",
      "[Trial 54] Epoch 39/60, Training Loss: 0.1290, Validation Loss: 0.1436\n",
      "[Trial 55] Epoch 25/60, Training Loss: 0.3771, Validation Loss: 0.4781\n",
      "[Trial 51] Epoch 42/60, Training Loss: 0.1258, Validation Loss: 0.1434\n",
      "[Trial 56] Epoch 15/60, Training Loss: 0.1593, Validation Loss: 0.2358\n",
      "[Trial 57] Epoch 10/60, Training Loss: 0.1349, Validation Loss: 0.2049\n",
      "[Trial 54] Epoch 40/60, Training Loss: 0.1281, Validation Loss: 0.1420\n",
      "[Trial 52] Epoch 26/60, Training Loss: 0.1579, Validation Loss: 0.2731\n",
      "[Trial 55] Epoch 26/60, Training Loss: 0.3637, Validation Loss: 0.4662\n",
      "[Trial 56] Epoch 16/60, Training Loss: 0.1564, Validation Loss: 0.2329\n",
      "[Trial 57] Epoch 11/60, Training Loss: 0.1339, Validation Loss: 0.2004\n",
      "[Trial 54] Epoch 41/60, Training Loss: 0.1301, Validation Loss: 0.1414\n",
      "[Trial 55] Epoch 27/60, Training Loss: 0.3514, Validation Loss: 0.4550\n",
      "[Trial 58] Epoch 2/60, Training Loss: 0.1324, Validation Loss: 0.1605\n",
      "[Trial 56] Epoch 17/60, Training Loss: 0.1538, Validation Loss: 0.2295\n",
      "[Trial 57] Epoch 12/60, Training Loss: 0.1336, Validation Loss: 0.1948\n",
      "[Trial 55] Epoch 28/60, Training Loss: 0.3401, Validation Loss: 0.4449\n",
      "[Trial 54] Epoch 42/60, Training Loss: 0.1279, Validation Loss: 0.1410\n",
      "[Trial 51] Epoch 43/60, Training Loss: 0.1262, Validation Loss: 0.1437\n",
      "[Trial 56] Epoch 18/60, Training Loss: 0.1519, Validation Loss: 0.2255\n",
      "[Trial 57] Epoch 13/60, Training Loss: 0.1325, Validation Loss: 0.1916\n",
      "[Trial 52] Epoch 27/60, Training Loss: 0.1559, Validation Loss: 0.2710\n",
      "[Trial 55] Epoch 29/60, Training Loss: 0.3299, Validation Loss: 0.4355\n",
      "[Trial 54] Epoch 43/60, Training Loss: 0.1277, Validation Loss: 0.1391\n",
      "[Trial 56] Epoch 19/60, Training Loss: 0.1501, Validation Loss: 0.2239\n",
      "[Trial 57] Epoch 14/60, Training Loss: 0.1319, Validation Loss: 0.1867\n",
      "[Trial 55] Epoch 30/60, Training Loss: 0.3201, Validation Loss: 0.4267\n",
      "[Trial 54] Epoch 44/60, Training Loss: 0.1279, Validation Loss: 0.1389\n",
      "[Trial 56] Epoch 20/60, Training Loss: 0.1482, Validation Loss: 0.2213\n",
      "[Trial 57] Epoch 15/60, Training Loss: 0.1325, Validation Loss: 0.1837\n",
      "[Trial 55] Epoch 31/60, Training Loss: 0.3115, Validation Loss: 0.4188\n",
      "[Trial 51] Epoch 44/60, Training Loss: 0.1262, Validation Loss: 0.1436\n",
      "[Trial 54] Epoch 45/60, Training Loss: 0.1291, Validation Loss: 0.1373\n",
      "[Trial 58] Epoch 3/60, Training Loss: 0.1289, Validation Loss: 0.1475\n",
      "[Trial 56] Epoch 21/60, Training Loss: 0.1470, Validation Loss: 0.2190\n",
      "[Trial 57] Epoch 16/60, Training Loss: 0.1317, Validation Loss: 0.1792\n",
      "[Trial 52] Epoch 28/60, Training Loss: 0.1537, Validation Loss: 0.2692\n",
      "[Trial 55] Epoch 32/60, Training Loss: 0.3033, Validation Loss: 0.4109\n",
      "[Trial 54] Epoch 46/60, Training Loss: 0.1321, Validation Loss: 0.1361\n",
      "[Trial 56] Epoch 22/60, Training Loss: 0.1457, Validation Loss: 0.2169\n",
      "[Trial 57] Epoch 17/60, Training Loss: 0.1308, Validation Loss: 0.1749\n",
      "[Trial 55] Epoch 33/60, Training Loss: 0.2959, Validation Loss: 0.4040\n",
      "[Trial 54] Epoch 47/60, Training Loss: 0.1276, Validation Loss: 0.1354\n",
      "[Trial 56] Epoch 23/60, Training Loss: 0.1446, Validation Loss: 0.2150\n",
      "[Trial 57] Epoch 18/60, Training Loss: 0.1302, Validation Loss: 0.1728\n",
      "[Trial 55] Epoch 34/60, Training Loss: 0.2886, Validation Loss: 0.3973\n",
      "[Trial 51] Epoch 45/60, Training Loss: 0.1257, Validation Loss: 0.1405\n",
      "[Trial 54] Epoch 48/60, Training Loss: 0.1276, Validation Loss: 0.1342\n",
      "[Trial 52] Epoch 29/60, Training Loss: 0.1519, Validation Loss: 0.2675\n",
      "[Trial 56] Epoch 24/60, Training Loss: 0.1434, Validation Loss: 0.2129\n",
      "[Trial 55] Epoch 35/60, Training Loss: 0.2818, Validation Loss: 0.3911\n",
      "[Trial 57] Epoch 19/60, Training Loss: 0.1300, Validation Loss: 0.1666\n",
      "[Trial 54] Epoch 49/60, Training Loss: 0.1274, Validation Loss: 0.1336\n",
      "[Trial 55] Epoch 36/60, Training Loss: 0.2756, Validation Loss: 0.3851\n",
      "[Trial 57] Epoch 20/60, Training Loss: 0.1303, Validation Loss: 0.1646\n",
      "[Trial 56] Epoch 25/60, Training Loss: 0.1425, Validation Loss: 0.2108\n",
      "[Trial 58] Epoch 4/60, Training Loss: 0.1276, Validation Loss: 0.1388\n",
      "[Trial 54] Epoch 50/60, Training Loss: 0.1277, Validation Loss: 0.1340\n",
      "[Trial 55] Epoch 37/60, Training Loss: 0.2697, Validation Loss: 0.3797\n",
      "[Trial 57] Epoch 21/60, Training Loss: 0.1300, Validation Loss: 0.1614\n",
      "[Trial 56] Epoch 26/60, Training Loss: 0.1416, Validation Loss: 0.2093\n",
      "[Trial 51] Epoch 46/60, Training Loss: 0.1259, Validation Loss: 0.1427\n",
      "[Trial 54] Epoch 51/60, Training Loss: 0.1277, Validation Loss: 0.1331\n",
      "[Trial 52] Epoch 30/60, Training Loss: 0.1501, Validation Loss: 0.2657\n",
      "[Trial 55] Epoch 38/60, Training Loss: 0.2641, Validation Loss: 0.3746\n",
      "[Trial 57] Epoch 22/60, Training Loss: 0.1295, Validation Loss: 0.1582\n",
      "[Trial 56] Epoch 27/60, Training Loss: 0.1408, Validation Loss: 0.2079\n",
      "[Trial 54] Epoch 52/60, Training Loss: 0.1284, Validation Loss: 0.1330\n",
      "[Trial 55] Epoch 39/60, Training Loss: 0.2589, Validation Loss: 0.3696\n",
      "[Trial 57] Epoch 23/60, Training Loss: 0.1299, Validation Loss: 0.1567\n",
      "[Trial 56] Epoch 28/60, Training Loss: 0.1400, Validation Loss: 0.2060\n",
      "[Trial 55] Epoch 40/60, Training Loss: 0.2539, Validation Loss: 0.3651\n",
      "[Trial 54] Epoch 53/60, Training Loss: 0.1276, Validation Loss: 0.1322\n",
      "[Trial 51] Epoch 47/60, Training Loss: 0.1261, Validation Loss: 0.1404\n",
      "[Trial 57] Epoch 24/60, Training Loss: 0.1293, Validation Loss: 0.1526\n",
      "[Trial 56] Epoch 29/60, Training Loss: 0.1394, Validation Loss: 0.2048\n",
      "[Trial 58] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1334\n",
      "[Trial 52] Epoch 31/60, Training Loss: 0.1487, Validation Loss: 0.2642\n",
      "[Trial 55] Epoch 41/60, Training Loss: 0.2493, Validation Loss: 0.3605\n",
      "[Trial 54] Epoch 54/60, Training Loss: 0.1284, Validation Loss: 0.1327\n",
      "[Trial 57] Epoch 25/60, Training Loss: 0.1286, Validation Loss: 0.1507\n",
      "[Trial 56] Epoch 30/60, Training Loss: 0.1388, Validation Loss: 0.2028\n",
      "[Trial 55] Epoch 42/60, Training Loss: 0.2445, Validation Loss: 0.3564\n",
      "[Trial 54] Epoch 55/60, Training Loss: 0.1285, Validation Loss: 0.1312\n",
      "[Trial 57] Epoch 26/60, Training Loss: 0.1285, Validation Loss: 0.1488\n",
      "[Trial 56] Epoch 31/60, Training Loss: 0.1382, Validation Loss: 0.2006\n",
      "[Trial 55] Epoch 43/60, Training Loss: 0.2409, Validation Loss: 0.3523\n",
      "[Trial 54] Epoch 56/60, Training Loss: 0.1275, Validation Loss: 0.1310\n",
      "[Trial 51] Epoch 48/60, Training Loss: 0.1256, Validation Loss: 0.1391\n",
      "[Trial 57] Epoch 27/60, Training Loss: 0.1297, Validation Loss: 0.1460\n",
      "[Trial 56] Epoch 32/60, Training Loss: 0.1378, Validation Loss: 0.2000\n",
      "[Trial 52] Epoch 32/60, Training Loss: 0.1474, Validation Loss: 0.2628\n",
      "[Trial 55] Epoch 44/60, Training Loss: 0.2368, Validation Loss: 0.3486\n",
      "[Trial 54] Epoch 57/60, Training Loss: 0.1298, Validation Loss: 0.1301\n",
      "[Trial 57] Epoch 28/60, Training Loss: 0.1284, Validation Loss: 0.1446\n",
      "[Trial 56] Epoch 33/60, Training Loss: 0.1371, Validation Loss: 0.1985\n",
      "[Trial 55] Epoch 45/60, Training Loss: 0.2332, Validation Loss: 0.3449\n",
      "[Trial 58] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1312\n",
      "[Trial 54] Epoch 58/60, Training Loss: 0.1273, Validation Loss: 0.1295\n",
      "[Trial 57] Epoch 29/60, Training Loss: 0.1293, Validation Loss: 0.1423\n",
      "[Trial 56] Epoch 34/60, Training Loss: 0.1366, Validation Loss: 0.1973\n",
      "[Trial 55] Epoch 46/60, Training Loss: 0.2295, Validation Loss: 0.3413\n",
      "[Trial 51] Epoch 49/60, Training Loss: 0.1258, Validation Loss: 0.1388\n",
      "[Trial 54] Epoch 59/60, Training Loss: 0.1273, Validation Loss: 0.1298\n",
      "[Trial 57] Epoch 30/60, Training Loss: 0.1283, Validation Loss: 0.1414\n",
      "[Trial 56] Epoch 35/60, Training Loss: 0.1362, Validation Loss: 0.1961\n",
      "[Trial 55] Epoch 47/60, Training Loss: 0.2261, Validation Loss: 0.3381\n",
      "[Trial 52] Epoch 33/60, Training Loss: 0.1463, Validation Loss: 0.2612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:25:36,026] Trial 54 finished with value: 0.12948120286067327 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.0005299425438202311, 'batch_size': 64, 'patience': 7}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 54] Epoch 60/60, Training Loss: 0.1275, Validation Loss: 0.1295\n",
      "[Trial 57] Epoch 31/60, Training Loss: 0.1305, Validation Loss: 0.1395\n",
      "[Trial 55] Epoch 48/60, Training Loss: 0.2226, Validation Loss: 0.3348\n",
      "[Trial 56] Epoch 36/60, Training Loss: 0.1357, Validation Loss: 0.1937\n",
      "[Trial 57] Epoch 32/60, Training Loss: 0.1279, Validation Loss: 0.1388\n",
      "[Trial 55] Epoch 49/60, Training Loss: 0.2195, Validation Loss: 0.3318\n",
      "[Trial 56] Epoch 37/60, Training Loss: 0.1353, Validation Loss: 0.1928\n",
      "[Trial 51] Epoch 50/60, Training Loss: 0.1257, Validation Loss: 0.1370\n",
      "[Trial 58] Epoch 7/60, Training Loss: 0.1260, Validation Loss: 0.1303\n",
      "[Trial 52] Epoch 34/60, Training Loss: 0.1448, Validation Loss: 0.2599\n",
      "[Trial 55] Epoch 50/60, Training Loss: 0.2166, Validation Loss: 0.3288\n",
      "[Trial 57] Epoch 33/60, Training Loss: 0.1282, Validation Loss: 0.1368\n",
      "[Trial 56] Epoch 38/60, Training Loss: 0.1349, Validation Loss: 0.1909\n",
      "[Trial 55] Epoch 51/60, Training Loss: 0.2139, Validation Loss: 0.3259\n",
      "[Trial 57] Epoch 34/60, Training Loss: 0.1277, Validation Loss: 0.1362\n",
      "[Trial 56] Epoch 39/60, Training Loss: 0.1346, Validation Loss: 0.1905\n",
      "[Trial 55] Epoch 52/60, Training Loss: 0.2114, Validation Loss: 0.3232\n",
      "[Trial 59] Epoch 1/60, Training Loss: 0.2169, Validation Loss: 0.1373\n",
      "[Trial 57] Epoch 35/60, Training Loss: 0.1348, Validation Loss: 0.1360\n",
      "[Trial 56] Epoch 40/60, Training Loss: 0.1341, Validation Loss: 0.1883\n",
      "[Trial 51] Epoch 51/60, Training Loss: 0.1257, Validation Loss: 0.1358\n",
      "[Trial 55] Epoch 53/60, Training Loss: 0.2089, Validation Loss: 0.3205\n",
      "[Trial 52] Epoch 35/60, Training Loss: 0.1432, Validation Loss: 0.2588\n",
      "[Trial 57] Epoch 36/60, Training Loss: 0.1275, Validation Loss: 0.1341\n",
      "[Trial 56] Epoch 41/60, Training Loss: 0.1340, Validation Loss: 0.1881\n",
      "[Trial 55] Epoch 54/60, Training Loss: 0.2062, Validation Loss: 0.3179\n",
      "[Trial 57] Epoch 37/60, Training Loss: 0.1277, Validation Loss: 0.1338\n",
      "[Trial 58] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1290\n",
      "[Trial 56] Epoch 42/60, Training Loss: 0.1335, Validation Loss: 0.1865\n",
      "[Trial 55] Epoch 55/60, Training Loss: 0.2037, Validation Loss: 0.3157\n",
      "[Trial 57] Epoch 38/60, Training Loss: 0.1273, Validation Loss: 0.1327\n",
      "[Trial 56] Epoch 43/60, Training Loss: 0.1335, Validation Loss: 0.1853\n",
      "[Trial 51] Epoch 52/60, Training Loss: 0.1256, Validation Loss: 0.1359\n",
      "[Trial 55] Epoch 56/60, Training Loss: 0.2015, Validation Loss: 0.3133\n",
      "[Trial 52] Epoch 36/60, Training Loss: 0.1424, Validation Loss: 0.2575\n",
      "[Trial 57] Epoch 39/60, Training Loss: 0.1275, Validation Loss: 0.1326\n",
      "[Trial 56] Epoch 44/60, Training Loss: 0.1332, Validation Loss: 0.1837\n",
      "[Trial 59] Epoch 2/60, Training Loss: 0.1276, Validation Loss: 0.1277\n",
      "[Trial 55] Epoch 57/60, Training Loss: 0.1995, Validation Loss: 0.3110\n",
      "[Trial 57] Epoch 40/60, Training Loss: 0.1281, Validation Loss: 0.1319\n",
      "[Trial 56] Epoch 45/60, Training Loss: 0.1327, Validation Loss: 0.1833\n",
      "[Trial 55] Epoch 58/60, Training Loss: 0.1973, Validation Loss: 0.3089\n",
      "[Trial 57] Epoch 41/60, Training Loss: 0.1283, Validation Loss: 0.1315\n",
      "[Trial 51] Epoch 53/60, Training Loss: 0.1259, Validation Loss: 0.1346\n",
      "[Trial 56] Epoch 46/60, Training Loss: 0.1326, Validation Loss: 0.1812\n",
      "[Trial 58] Epoch 9/60, Training Loss: 0.1261, Validation Loss: 0.1287\n",
      "[Trial 55] Epoch 59/60, Training Loss: 0.1952, Validation Loss: 0.3068\n",
      "[Trial 52] Epoch 37/60, Training Loss: 0.1413, Validation Loss: 0.2565\n",
      "[Trial 57] Epoch 42/60, Training Loss: 0.1303, Validation Loss: 0.1306\n",
      "[Trial 56] Epoch 47/60, Training Loss: 0.1323, Validation Loss: 0.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:27:10,050] Trial 55 finished with value: 0.3047020177046458 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 1.7412714147429393e-05, 'batch_size': 64, 'patience': 9}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 55] Epoch 60/60, Training Loss: 0.1932, Validation Loss: 0.3047\n",
      "[Trial 57] Epoch 43/60, Training Loss: 0.1270, Validation Loss: 0.1307\n",
      "[Trial 56] Epoch 48/60, Training Loss: 0.1321, Validation Loss: 0.1797\n",
      "[Trial 59] Epoch 3/60, Training Loss: 0.1271, Validation Loss: 0.1250\n",
      "[Trial 51] Epoch 54/60, Training Loss: 0.1259, Validation Loss: 0.1348\n",
      "[Trial 57] Epoch 44/60, Training Loss: 0.1274, Validation Loss: 0.1314\n",
      "[Trial 56] Epoch 49/60, Training Loss: 0.1319, Validation Loss: 0.1779\n",
      "[Trial 52] Epoch 38/60, Training Loss: 0.1405, Validation Loss: 0.2553\n",
      "[Trial 57] Epoch 45/60, Training Loss: 0.1291, Validation Loss: 0.1300\n",
      "[Trial 56] Epoch 50/60, Training Loss: 0.1318, Validation Loss: 0.1775\n",
      "[Trial 58] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1284\n",
      "[Trial 57] Epoch 46/60, Training Loss: 0.1268, Validation Loss: 0.1294\n",
      "[Trial 56] Epoch 51/60, Training Loss: 0.1315, Validation Loss: 0.1764\n",
      "[Trial 60] Epoch 1/60, Training Loss: 0.2486, Validation Loss: 0.1429\n",
      "[Trial 51] Epoch 55/60, Training Loss: 0.1259, Validation Loss: 0.1358\n",
      "[Trial 52] Epoch 39/60, Training Loss: 0.1396, Validation Loss: 0.2543\n",
      "[Trial 57] Epoch 47/60, Training Loss: 0.1296, Validation Loss: 0.1296\n",
      "[Trial 56] Epoch 52/60, Training Loss: 0.1312, Validation Loss: 0.1748\n",
      "[Trial 57] Epoch 48/60, Training Loss: 0.1331, Validation Loss: 0.1287\n",
      "[Trial 59] Epoch 4/60, Training Loss: 0.1267, Validation Loss: 0.1251\n",
      "[Trial 56] Epoch 53/60, Training Loss: 0.1311, Validation Loss: 0.1749\n",
      "[Trial 57] Epoch 49/60, Training Loss: 0.1270, Validation Loss: 0.1291\n",
      "[Trial 56] Epoch 54/60, Training Loss: 0.1309, Validation Loss: 0.1737\n",
      "[Trial 51] Epoch 56/60, Training Loss: 0.1254, Validation Loss: 0.1345\n",
      "[Trial 52] Epoch 40/60, Training Loss: 0.1391, Validation Loss: 0.2534\n",
      "[Trial 57] Epoch 50/60, Training Loss: 0.1270, Validation Loss: 0.1289\n",
      "[Trial 58] Epoch 11/60, Training Loss: 0.1257, Validation Loss: 0.1285\n",
      "[Trial 56] Epoch 55/60, Training Loss: 0.1307, Validation Loss: 0.1723\n",
      "[Trial 60] Epoch 2/60, Training Loss: 0.1267, Validation Loss: 0.1335\n",
      "[Trial 57] Epoch 51/60, Training Loss: 0.1269, Validation Loss: 0.1281\n",
      "[Trial 56] Epoch 56/60, Training Loss: 0.1305, Validation Loss: 0.1712\n",
      "[Trial 57] Epoch 52/60, Training Loss: 0.1272, Validation Loss: 0.1280\n",
      "[Trial 56] Epoch 57/60, Training Loss: 0.1305, Validation Loss: 0.1705\n",
      "[Trial 51] Epoch 57/60, Training Loss: 0.1256, Validation Loss: 0.1344\n",
      "[Trial 59] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1239\n",
      "[Trial 52] Epoch 41/60, Training Loss: 0.1382, Validation Loss: 0.2524\n",
      "[Trial 57] Epoch 53/60, Training Loss: 0.1307, Validation Loss: 0.1277\n",
      "[Trial 56] Epoch 58/60, Training Loss: 0.1303, Validation Loss: 0.1691\n",
      "[Trial 57] Epoch 54/60, Training Loss: 0.1266, Validation Loss: 0.1282\n",
      "[Trial 56] Epoch 59/60, Training Loss: 0.1301, Validation Loss: 0.1686\n",
      "[Trial 60] Epoch 3/60, Training Loss: 0.1262, Validation Loss: 0.1290\n",
      "[Trial 58] Epoch 12/60, Training Loss: 0.1254, Validation Loss: 0.1279\n",
      "[Trial 57] Epoch 55/60, Training Loss: 0.1268, Validation Loss: 0.1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:28:49,367] Trial 56 finished with value: 0.16690493623415628 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.0001491649906996807, 'batch_size': 64, 'patience': 9}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 56] Epoch 60/60, Training Loss: 0.1302, Validation Loss: 0.1669\n",
      "[Trial 51] Epoch 58/60, Training Loss: 0.1257, Validation Loss: 0.1338\n",
      "[Trial 52] Epoch 42/60, Training Loss: 0.1380, Validation Loss: 0.2512\n",
      "[Trial 57] Epoch 56/60, Training Loss: 0.1280, Validation Loss: 0.1279\n",
      "[Trial 57] Epoch 57/60, Training Loss: 0.1290, Validation Loss: 0.1278\n",
      "[Trial 59] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1244\n",
      "[Trial 57] Epoch 58/60, Training Loss: 0.1263, Validation Loss: 0.1273\n",
      "[Trial 51] Epoch 59/60, Training Loss: 0.1257, Validation Loss: 0.1325\n",
      "[Trial 52] Epoch 43/60, Training Loss: 0.1373, Validation Loss: 0.2505\n",
      "[Trial 60] Epoch 4/60, Training Loss: 0.1257, Validation Loss: 0.1284\n",
      "[Trial 57] Epoch 59/60, Training Loss: 0.1262, Validation Loss: 0.1274\n",
      "[Trial 58] Epoch 13/60, Training Loss: 0.1256, Validation Loss: 0.1275\n",
      "[Trial 61] Epoch 1/60, Training Loss: 0.2357, Validation Loss: 0.1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:29:25,329] Trial 57 finished with value: 0.1273322621981303 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.0006954393264931729, 'batch_size': 64, 'patience': 6}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 57] Epoch 60/60, Training Loss: 0.1261, Validation Loss: 0.1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:29:32,368] Trial 51 finished with value: 0.13231307789683341 and parameters: {'hidden_dim': 192, 'latent_dim': 128, 'learning_rate': 0.0003693252385896867, 'batch_size': 16, 'patience': 7}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 51] Epoch 60/60, Training Loss: 0.1253, Validation Loss: 0.1323\n",
      "[Trial 52] Epoch 44/60, Training Loss: 0.1362, Validation Loss: 0.2497\n",
      "[Trial 59] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1240\n",
      "[Trial 60] Epoch 5/60, Training Loss: 0.1253, Validation Loss: 0.1284\n",
      "[Trial 58] Epoch 14/60, Training Loss: 0.1258, Validation Loss: 0.1277\n",
      "[Trial 52] Epoch 45/60, Training Loss: 0.1354, Validation Loss: 0.2489\n",
      "[Trial 61] Epoch 2/60, Training Loss: 0.1274, Validation Loss: 0.1320\n",
      "[Trial 62] Epoch 1/60, Training Loss: 1.0083, Validation Loss: 0.2116\n",
      "[Trial 63] Epoch 1/60, Training Loss: 0.6497, Validation Loss: 0.1932\n",
      "[Trial 59] Epoch 8/60, Training Loss: 0.1258, Validation Loss: 0.1238\n",
      "[Trial 52] Epoch 46/60, Training Loss: 0.1359, Validation Loss: 0.2481\n",
      "[Trial 60] Epoch 6/60, Training Loss: 0.1253, Validation Loss: 0.1279\n",
      "[Trial 58] Epoch 15/60, Training Loss: 0.1256, Validation Loss: 0.1277\n",
      "[Trial 61] Epoch 3/60, Training Loss: 0.1269, Validation Loss: 0.1274\n",
      "[Trial 62] Epoch 2/60, Training Loss: 0.1317, Validation Loss: 0.1857\n",
      "[Trial 52] Epoch 47/60, Training Loss: 0.1350, Validation Loss: 0.2473\n",
      "[Trial 63] Epoch 2/60, Training Loss: 0.1293, Validation Loss: 0.1723\n",
      "[Trial 59] Epoch 9/60, Training Loss: 0.1261, Validation Loss: 0.1237\n",
      "[Trial 60] Epoch 7/60, Training Loss: 0.1252, Validation Loss: 0.1282\n",
      "[Trial 52] Epoch 48/60, Training Loss: 0.1352, Validation Loss: 0.2467\n",
      "[Trial 58] Epoch 16/60, Training Loss: 0.1251, Validation Loss: 0.1273\n",
      "[Trial 61] Epoch 4/60, Training Loss: 0.1261, Validation Loss: 0.1276\n",
      "[Trial 62] Epoch 3/60, Training Loss: 0.1290, Validation Loss: 0.1679\n",
      "[Trial 52] Epoch 49/60, Training Loss: 0.1342, Validation Loss: 0.2460\n",
      "[Trial 63] Epoch 3/60, Training Loss: 0.1281, Validation Loss: 0.1572\n",
      "[Trial 59] Epoch 10/60, Training Loss: 0.1260, Validation Loss: 0.1238\n",
      "[Trial 60] Epoch 8/60, Training Loss: 0.1253, Validation Loss: 0.1280\n",
      "[Trial 58] Epoch 17/60, Training Loss: 0.1253, Validation Loss: 0.1273\n",
      "[Trial 61] Epoch 5/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 52] Epoch 50/60, Training Loss: 0.1337, Validation Loss: 0.2452\n",
      "[Trial 62] Epoch 4/60, Training Loss: 0.1285, Validation Loss: 0.1579\n",
      "[Trial 60] Epoch 9/60, Training Loss: 0.1249, Validation Loss: 0.1276\n",
      "[Trial 59] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1238\n",
      "[Trial 63] Epoch 4/60, Training Loss: 0.1287, Validation Loss: 0.1459\n",
      "[Trial 52] Epoch 51/60, Training Loss: 0.1331, Validation Loss: 0.2446\n",
      "[Trial 58] Epoch 18/60, Training Loss: 0.1253, Validation Loss: 0.1274\n",
      "[Trial 61] Epoch 6/60, Training Loss: 0.1258, Validation Loss: 0.1270\n",
      "[Trial 62] Epoch 5/60, Training Loss: 0.1288, Validation Loss: 0.1509\n",
      "[Trial 52] Epoch 52/60, Training Loss: 0.1331, Validation Loss: 0.2441\n",
      "[Trial 60] Epoch 10/60, Training Loss: 0.1249, Validation Loss: 0.1275\n",
      "[Trial 59] Epoch 12/60, Training Loss: 0.1257, Validation Loss: 0.1237\n",
      "[Trial 63] Epoch 5/60, Training Loss: 0.1270, Validation Loss: 0.1398\n",
      "[Trial 52] Epoch 53/60, Training Loss: 0.1328, Validation Loss: 0.2437\n",
      "[Trial 58] Epoch 19/60, Training Loss: 0.1252, Validation Loss: 0.1272\n",
      "[Trial 61] Epoch 7/60, Training Loss: 0.1257, Validation Loss: 0.1280\n",
      "[Trial 62] Epoch 6/60, Training Loss: 0.1281, Validation Loss: 0.1410\n",
      "[Trial 60] Epoch 11/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 52] Epoch 54/60, Training Loss: 0.1324, Validation Loss: 0.2431\n",
      "[Trial 59] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1238\n",
      "[Trial 63] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1339\n",
      "[Trial 58] Epoch 20/60, Training Loss: 0.1254, Validation Loss: 0.1273\n",
      "[Trial 61] Epoch 8/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 62] Epoch 7/60, Training Loss: 0.1284, Validation Loss: 0.1394\n",
      "[Trial 52] Epoch 55/60, Training Loss: 0.1321, Validation Loss: 0.2423\n",
      "[Trial 60] Epoch 12/60, Training Loss: 0.1252, Validation Loss: 0.1276\n",
      "[Trial 59] Epoch 14/60, Training Loss: 0.1258, Validation Loss: 0.1237\n",
      "[Trial 63] Epoch 7/60, Training Loss: 0.1270, Validation Loss: 0.1305\n",
      "[Trial 52] Epoch 56/60, Training Loss: 0.1320, Validation Loss: 0.2418\n",
      "[Trial 58] Epoch 21/60, Training Loss: 0.1252, Validation Loss: 0.1271\n",
      "[Trial 61] Epoch 9/60, Training Loss: 0.1253, Validation Loss: 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:33:41,949] Trial 60 finished with value: 0.12753611685087282 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.004629197306610482, 'batch_size': 8, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 60] Epoch 13/60, Training Loss: 0.1250, Validation Loss: 0.1275\n",
      "[Trial 60] Early stopping after 13 epochs.\n",
      "[Trial 62] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1362\n",
      "[Trial 52] Epoch 57/60, Training Loss: 0.1320, Validation Loss: 0.2413\n",
      "[Trial 59] Epoch 15/60, Training Loss: 0.1258, Validation Loss: 0.1238\n",
      "[Trial 63] Epoch 8/60, Training Loss: 0.1267, Validation Loss: 0.1281\n",
      "[Trial 58] Epoch 22/60, Training Loss: 0.1255, Validation Loss: 0.1272\n",
      "[Trial 61] Epoch 10/60, Training Loss: 0.1255, Validation Loss: 0.1268\n",
      "[Trial 52] Epoch 58/60, Training Loss: 0.1314, Validation Loss: 0.2408\n",
      "[Trial 62] Epoch 9/60, Training Loss: 0.1280, Validation Loss: 0.1315\n",
      "[Trial 64] Epoch 1/60, Training Loss: 0.1912, Validation Loss: 0.2409\n",
      "[Trial 59] Epoch 16/60, Training Loss: 0.1258, Validation Loss: 0.1237\n",
      "[Trial 52] Epoch 59/60, Training Loss: 0.1313, Validation Loss: 0.2403\n",
      "[Trial 63] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1265\n",
      "[Trial 58] Epoch 23/60, Training Loss: 0.1253, Validation Loss: 0.1273\n",
      "[Trial 61] Epoch 11/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 62] Epoch 10/60, Training Loss: 0.1275, Validation Loss: 0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:34:51,702] Trial 52 finished with value: 0.23984944572051367 and parameters: {'hidden_dim': 192, 'latent_dim': 128, 'learning_rate': 1.0752413793671441e-05, 'batch_size': 16, 'patience': 9}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 52] Epoch 60/60, Training Loss: 0.1313, Validation Loss: 0.2398\n",
      "[Trial 64] Epoch 2/60, Training Loss: 0.1303, Validation Loss: 0.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:35:01,768] Trial 59 finished with value: 0.12371295640865962 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.004554403354737053, 'batch_size': 8, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 59] Epoch 17/60, Training Loss: 0.1258, Validation Loss: 0.1237\n",
      "[Trial 59] Early stopping after 17 epochs.\n",
      "[Trial 63] Epoch 10/60, Training Loss: 0.1266, Validation Loss: 0.1256\n",
      "[Trial 61] Epoch 12/60, Training Loss: 0.1252, Validation Loss: 0.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:35:14,992] Trial 58 finished with value: 0.1270836688578129 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0005175494858307254, 'batch_size': 8, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 58] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1272\n",
      "[Trial 58] Early stopping after 24 epochs.\n",
      "[Trial 62] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1283\n",
      "[Trial 64] Epoch 3/60, Training Loss: 0.1291, Validation Loss: 0.2273\n",
      "[Trial 65] Epoch 1/60, Training Loss: 0.1949, Validation Loss: 0.2506\n",
      "[Trial 66] Epoch 1/60, Training Loss: 0.2007, Validation Loss: 0.2413\n",
      "[Trial 63] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1257\n",
      "[Trial 61] Epoch 13/60, Training Loss: 0.1250, Validation Loss: 0.1265\n",
      "[Trial 67] Epoch 1/60, Training Loss: 0.2768, Validation Loss: 0.1517\n",
      "[Trial 62] Epoch 12/60, Training Loss: 0.1275, Validation Loss: 0.1276\n",
      "[Trial 64] Epoch 4/60, Training Loss: 0.1286, Validation Loss: 0.2146\n",
      "[Trial 65] Epoch 2/60, Training Loss: 0.1300, Validation Loss: 0.2276\n",
      "[Trial 66] Epoch 2/60, Training Loss: 0.1301, Validation Loss: 0.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:36:18,370] Trial 61 finished with value: 0.12646112671742837 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.00442597380338594, 'batch_size': 8, 'patience': 3}. Best is trial 41 with value: 0.1217108176400264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 61] Epoch 14/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 61] Early stopping after 14 epochs.\n",
      "[Trial 63] Epoch 12/60, Training Loss: 0.1268, Validation Loss: 0.1305\n",
      "[Trial 67] Epoch 2/60, Training Loss: 0.1286, Validation Loss: 0.1284\n",
      "[Trial 62] Epoch 13/60, Training Loss: 0.1271, Validation Loss: 0.1268\n",
      "[Trial 64] Epoch 5/60, Training Loss: 0.1287, Validation Loss: 0.2014\n",
      "[Trial 65] Epoch 3/60, Training Loss: 0.1292, Validation Loss: 0.1992\n",
      "[Trial 67] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1258\n",
      "[Trial 66] Epoch 3/60, Training Loss: 0.1297, Validation Loss: 0.1905\n",
      "[Trial 63] Epoch 13/60, Training Loss: 0.1256, Validation Loss: 0.1253\n",
      "[Trial 68] Epoch 1/60, Training Loss: 0.2200, Validation Loss: 0.2339\n",
      "[Trial 62] Epoch 14/60, Training Loss: 0.1274, Validation Loss: 0.1265\n",
      "[Trial 64] Epoch 6/60, Training Loss: 0.1281, Validation Loss: 0.1861\n",
      "[Trial 67] Epoch 4/60, Training Loss: 0.1275, Validation Loss: 0.1234\n",
      "[Trial 65] Epoch 4/60, Training Loss: 0.1290, Validation Loss: 0.1772\n",
      "[Trial 62] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1260\n",
      "[Trial 63] Epoch 14/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 66] Epoch 4/60, Training Loss: 0.1286, Validation Loss: 0.1691\n",
      "[Trial 68] Epoch 2/60, Training Loss: 0.1298, Validation Loss: 0.2078\n",
      "[Trial 64] Epoch 7/60, Training Loss: 0.1279, Validation Loss: 0.1762\n",
      "[Trial 67] Epoch 5/60, Training Loss: 0.1274, Validation Loss: 0.1226\n",
      "[Trial 62] Epoch 16/60, Training Loss: 0.1270, Validation Loss: 0.1250\n",
      "[Trial 63] Epoch 15/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 65] Epoch 5/60, Training Loss: 0.1285, Validation Loss: 0.1594\n",
      "[Trial 66] Epoch 5/60, Training Loss: 0.1280, Validation Loss: 0.1537\n",
      "[Trial 68] Epoch 3/60, Training Loss: 0.1295, Validation Loss: 0.1826\n",
      "[Trial 64] Epoch 8/60, Training Loss: 0.1278, Validation Loss: 0.1652\n",
      "[Trial 67] Epoch 6/60, Training Loss: 0.1275, Validation Loss: 0.1222\n",
      "[Trial 62] Epoch 17/60, Training Loss: 0.1267, Validation Loss: 0.1359\n",
      "[Trial 63] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 65] Epoch 6/60, Training Loss: 0.1284, Validation Loss: 0.1465\n",
      "[Trial 66] Epoch 6/60, Training Loss: 0.1278, Validation Loss: 0.1442\n",
      "[Trial 68] Epoch 4/60, Training Loss: 0.1290, Validation Loss: 0.1633\n",
      "[Trial 67] Epoch 7/60, Training Loss: 0.1268, Validation Loss: 0.1219\n",
      "[Trial 64] Epoch 9/60, Training Loss: 0.1275, Validation Loss: 0.1562\n",
      "[Trial 62] Epoch 18/60, Training Loss: 0.1277, Validation Loss: 0.1240\n",
      "[Trial 63] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 65] Epoch 7/60, Training Loss: 0.1278, Validation Loss: 0.1375\n",
      "[Trial 68] Epoch 5/60, Training Loss: 0.1290, Validation Loss: 0.1458\n",
      "[Trial 66] Epoch 7/60, Training Loss: 0.1279, Validation Loss: 0.1340\n",
      "[Trial 67] Epoch 8/60, Training Loss: 0.1272, Validation Loss: 0.1237\n",
      "[Trial 62] Epoch 19/60, Training Loss: 0.1264, Validation Loss: 0.1238\n",
      "[Trial 64] Epoch 10/60, Training Loss: 0.1277, Validation Loss: 0.1506\n",
      "[Trial 63] Epoch 18/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 65] Epoch 8/60, Training Loss: 0.1277, Validation Loss: 0.1330\n",
      "[Trial 68] Epoch 6/60, Training Loss: 0.1280, Validation Loss: 0.1358\n",
      "[Trial 66] Epoch 8/60, Training Loss: 0.1270, Validation Loss: 0.1292\n",
      "[Trial 67] Epoch 9/60, Training Loss: 0.1272, Validation Loss: 0.1220\n",
      "[Trial 62] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1244\n",
      "[Trial 64] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.1443\n",
      "[Trial 63] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 68] Epoch 7/60, Training Loss: 0.1278, Validation Loss: 0.1300\n",
      "[Trial 65] Epoch 9/60, Training Loss: 0.1270, Validation Loss: 0.1279\n",
      "[Trial 66] Epoch 9/60, Training Loss: 0.1274, Validation Loss: 0.1496\n",
      "[Trial 67] Epoch 10/60, Training Loss: 0.1267, Validation Loss: 0.1218\n",
      "[Trial 62] Epoch 21/60, Training Loss: 0.1268, Validation Loss: 0.1237\n",
      "[Trial 64] Epoch 12/60, Training Loss: 0.1272, Validation Loss: 0.1400\n",
      "[Trial 63] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 67] Epoch 11/60, Training Loss: 0.1266, Validation Loss: 0.1216\n",
      "[Trial 62] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1250\n",
      "[Trial 68] Epoch 8/60, Training Loss: 0.1275, Validation Loss: 0.1271\n",
      "[Trial 66] Epoch 10/60, Training Loss: 0.1269, Validation Loss: 0.1264\n",
      "[Trial 65] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1266\n",
      "[Trial 64] Epoch 13/60, Training Loss: 0.1273, Validation Loss: 0.1351\n",
      "[Trial 63] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 62] Epoch 23/60, Training Loss: 0.1267, Validation Loss: 0.1237\n",
      "[Trial 67] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1224\n",
      "[Trial 68] Epoch 9/60, Training Loss: 0.1762, Validation Loss: 0.1809\n",
      "[Trial 66] Epoch 11/60, Training Loss: 0.1266, Validation Loss: 0.1262\n",
      "[Trial 65] Epoch 11/60, Training Loss: 0.1316, Validation Loss: 0.1261\n",
      "[Trial 63] Epoch 22/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 64] Epoch 14/60, Training Loss: 0.1270, Validation Loss: 0.1318\n",
      "[Trial 62] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1236\n",
      "[Trial 67] Epoch 13/60, Training Loss: 0.1268, Validation Loss: 0.1217\n",
      "[Trial 68] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.1256\n",
      "[Trial 66] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1263\n",
      "[Trial 65] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1260\n",
      "[Trial 63] Epoch 23/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 64] Epoch 15/60, Training Loss: 0.1284, Validation Loss: 0.1292\n",
      "[Trial 62] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:42:46,415] Trial 67 finished with value: 0.12163179100801548 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0020937734068014295, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 67] Epoch 14/60, Training Loss: 0.1263, Validation Loss: 0.1216\n",
      "[Trial 67] Early stopping after 14 epochs.\n",
      "[Trial 68] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1254\n",
      "[Trial 66] Epoch 13/60, Training Loss: 0.1268, Validation Loss: 0.1490\n",
      "[Trial 63] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 65] Epoch 13/60, Training Loss: 0.1264, Validation Loss: 0.1260\n",
      "[Trial 64] Epoch 16/60, Training Loss: 0.1268, Validation Loss: 0.1275\n",
      "[Trial 62] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1234\n",
      "[Trial 69] Epoch 1/60, Training Loss: 0.2021, Validation Loss: 0.2410\n",
      "[Trial 68] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1254\n",
      "[Trial 63] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 66] Epoch 14/60, Training Loss: 0.1314, Validation Loss: 0.1261\n",
      "[Trial 65] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1259\n",
      "[Trial 62] Epoch 27/60, Training Loss: 0.1261, Validation Loss: 0.1234\n",
      "[Trial 64] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1267\n",
      "[Trial 69] Epoch 2/60, Training Loss: 0.1304, Validation Loss: 0.2160\n",
      "[Trial 63] Epoch 26/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 68] Epoch 13/60, Training Loss: 0.1267, Validation Loss: 0.1255\n",
      "[Trial 62] Epoch 28/60, Training Loss: 0.1266, Validation Loss: 0.1242\n",
      "[Trial 66] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1261\n",
      "[Trial 64] Epoch 18/60, Training Loss: 0.1266, Validation Loss: 0.1258\n",
      "[Trial 65] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 3/60, Training Loss: 0.1296, Validation Loss: 0.1873\n",
      "[Trial 63] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:44:53,608] Trial 62 finished with value: 0.12337988683333 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.0012041915382658842, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 62] Epoch 29/60, Training Loss: 0.1260, Validation Loss: 0.1236\n",
      "[Trial 62] Early stopping after 29 epochs.\n",
      "[Trial 68] Epoch 14/60, Training Loss: 0.1269, Validation Loss: 0.1254\n",
      "[Trial 66] Epoch 16/60, Training Loss: 0.1261, Validation Loss: 0.1261\n",
      "[Trial 64] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1257\n",
      "[Trial 65] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 4/60, Training Loss: 0.1286, Validation Loss: 0.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:45:25,115] Trial 63 finished with value: 0.12521121334284543 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.002393174421757893, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 63] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 63] Early stopping after 28 epochs.\n",
      "[Trial 70] Epoch 1/60, Training Loss: 0.1873, Validation Loss: 0.2380\n",
      "[Trial 68] Epoch 15/60, Training Loss: 0.1271, Validation Loss: 0.1254\n",
      "[Trial 66] Epoch 17/60, Training Loss: 0.1266, Validation Loss: 0.1261\n",
      "[Trial 64] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1255\n",
      "[Trial 65] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 5/60, Training Loss: 0.1284, Validation Loss: 0.1478\n",
      "[Trial 71] Epoch 1/60, Training Loss: 0.2062, Validation Loss: 0.2386\n",
      "[Trial 70] Epoch 2/60, Training Loss: 0.1306, Validation Loss: 0.2308\n",
      "[Trial 68] Epoch 16/60, Training Loss: 0.1266, Validation Loss: 0.1254\n",
      "[Trial 66] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1261\n",
      "[Trial 64] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1253\n",
      "[Trial 65] Epoch 18/60, Training Loss: 0.1266, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 6/60, Training Loss: 0.1280, Validation Loss: 0.1372\n",
      "[Trial 71] Epoch 2/60, Training Loss: 0.1304, Validation Loss: 0.2413\n",
      "[Trial 70] Epoch 3/60, Training Loss: 0.1290, Validation Loss: 0.2188\n",
      "[Trial 68] Epoch 17/60, Training Loss: 0.1267, Validation Loss: 0.1254\n",
      "[Trial 64] Epoch 22/60, Training Loss: 0.1263, Validation Loss: 0.1253\n",
      "[Trial 66] Epoch 19/60, Training Loss: 0.1261, Validation Loss: 0.1261\n",
      "[Trial 65] Epoch 19/60, Training Loss: 0.1266, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 7/60, Training Loss: 0.1372, Validation Loss: 0.1305\n",
      "[Trial 71] Epoch 3/60, Training Loss: 0.1292, Validation Loss: 0.2267\n",
      "[Trial 70] Epoch 4/60, Training Loss: 0.1286, Validation Loss: 0.2028\n",
      "[Trial 68] Epoch 18/60, Training Loss: 0.1267, Validation Loss: 0.1254\n",
      "[Trial 64] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1253\n",
      "[Trial 66] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 65] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 8/60, Training Loss: 0.1270, Validation Loss: 0.1271\n",
      "[Trial 71] Epoch 4/60, Training Loss: 0.1284, Validation Loss: 0.2171\n",
      "[Trial 70] Epoch 5/60, Training Loss: 0.1283, Validation Loss: 0.1908\n",
      "[Trial 68] Epoch 19/60, Training Loss: 0.1267, Validation Loss: 0.1254\n",
      "[Trial 64] Epoch 24/60, Training Loss: 0.1265, Validation Loss: 0.1253\n",
      "[Trial 66] Epoch 21/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 65] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1263\n",
      "[Trial 69] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1261\n",
      "[Trial 70] Epoch 6/60, Training Loss: 0.1284, Validation Loss: 0.1783\n",
      "[Trial 71] Epoch 5/60, Training Loss: 0.1283, Validation Loss: 0.2051\n",
      "[Trial 68] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1254\n",
      "[Trial 64] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1253\n",
      "[Trial 66] Epoch 22/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 65] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 69] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1259\n",
      "[Trial 70] Epoch 7/60, Training Loss: 0.1277, Validation Loss: 0.1667\n",
      "[Trial 71] Epoch 6/60, Training Loss: 0.1279, Validation Loss: 0.1914\n",
      "[Trial 68] Epoch 21/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 64] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1252\n",
      "[Trial 66] Epoch 23/60, Training Loss: 0.1255, Validation Loss: 0.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:49:34,918] Trial 65 finished with value: 0.12589201827843985 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.0021476678350223935, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 65] Epoch 23/60, Training Loss: 0.1257, Validation Loss: 0.1259\n",
      "[Trial 65] Early stopping after 23 epochs.\n",
      "[Trial 69] Epoch 11/60, Training Loss: 0.1269, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 8/60, Training Loss: 0.1280, Validation Loss: 0.1586\n",
      "[Trial 71] Epoch 7/60, Training Loss: 0.1276, Validation Loss: 0.1813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:49:54,769] Trial 68 finished with value: 0.12536720695594947 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.0023458780069345003, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 68] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 68] Early stopping after 22 epochs.\n",
      "[Trial 64] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1255\n",
      "[Trial 66] Epoch 24/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 72] Epoch 1/60, Training Loss: 9.5442, Validation Loss: 0.1738\n",
      "[Trial 69] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 9/60, Training Loss: 0.1278, Validation Loss: 0.1502\n",
      "[Trial 71] Epoch 8/60, Training Loss: 0.1278, Validation Loss: 0.1716\n",
      "[Trial 73] Epoch 1/60, Training Loss: 0.1884, Validation Loss: 0.2396\n",
      "[Trial 64] Epoch 28/60, Training Loss: 0.1263, Validation Loss: 0.1253\n",
      "[Trial 66] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 72] Epoch 2/60, Training Loss: 0.1336, Validation Loss: 0.1325\n",
      "[Trial 69] Epoch 13/60, Training Loss: 0.1264, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 10/60, Training Loss: 0.1273, Validation Loss: 0.1436\n",
      "[Trial 71] Epoch 9/60, Training Loss: 0.1275, Validation Loss: 0.1609\n",
      "[Trial 73] Epoch 2/60, Training Loss: 0.1308, Validation Loss: 0.2380\n",
      "[Trial 64] Epoch 29/60, Training Loss: 0.1262, Validation Loss: 0.1252\n",
      "[Trial 66] Epoch 26/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 72] Epoch 3/60, Training Loss: 0.1334, Validation Loss: 0.1248\n",
      "[Trial 69] Epoch 14/60, Training Loss: 0.1267, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 11/60, Training Loss: 0.1273, Validation Loss: 0.1388\n",
      "[Trial 64] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1252\n",
      "[Trial 71] Epoch 10/60, Training Loss: 0.1274, Validation Loss: 0.1554\n",
      "[Trial 73] Epoch 3/60, Training Loss: 0.1292, Validation Loss: 0.2287\n",
      "[Trial 66] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 72] Epoch 4/60, Training Loss: 0.1342, Validation Loss: 0.1240\n",
      "[Trial 69] Epoch 15/60, Training Loss: 0.1265, Validation Loss: 0.1258\n",
      "[Trial 70] Epoch 12/60, Training Loss: 0.1342, Validation Loss: 0.1339\n",
      "[Trial 64] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 71] Epoch 11/60, Training Loss: 0.1273, Validation Loss: 0.1789\n",
      "[Trial 73] Epoch 4/60, Training Loss: 0.1287, Validation Loss: 0.2124\n",
      "[Trial 66] Epoch 28/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 72] Epoch 5/60, Training Loss: 0.1321, Validation Loss: 0.1247\n",
      "[Trial 69] Epoch 16/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 13/60, Training Loss: 0.1267, Validation Loss: 0.1307\n",
      "[Trial 64] Epoch 32/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 71] Epoch 12/60, Training Loss: 0.1280, Validation Loss: 0.1433\n",
      "[Trial 73] Epoch 5/60, Training Loss: 0.1283, Validation Loss: 0.2009\n",
      "[Trial 66] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 72] Epoch 6/60, Training Loss: 0.1365, Validation Loss: 0.1241\n",
      "[Trial 69] Epoch 17/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 14/60, Training Loss: 0.1269, Validation Loss: 0.1287\n",
      "[Trial 64] Epoch 33/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 6/60, Training Loss: 0.1279, Validation Loss: 0.1876\n",
      "[Trial 71] Epoch 13/60, Training Loss: 0.1268, Validation Loss: 0.1399\n",
      "[Trial 66] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 18/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 72] Epoch 7/60, Training Loss: 0.1332, Validation Loss: 0.1247\n",
      "[Trial 70] Epoch 15/60, Training Loss: 0.1266, Validation Loss: 0.1272\n",
      "[Trial 64] Epoch 34/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 7/60, Training Loss: 0.1276, Validation Loss: 0.1811\n",
      "[Trial 71] Epoch 14/60, Training Loss: 0.1268, Validation Loss: 0.1354\n",
      "[Trial 66] Epoch 31/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 19/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 72] Epoch 8/60, Training Loss: 0.1347, Validation Loss: 0.1623\n",
      "[Trial 70] Epoch 16/60, Training Loss: 0.1268, Validation Loss: 0.1263\n",
      "[Trial 64] Epoch 35/60, Training Loss: 0.1258, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1679\n",
      "[Trial 71] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1321\n",
      "[Trial 66] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1260\n",
      "[Trial 72] Epoch 9/60, Training Loss: 0.1276, Validation Loss: 0.1239\n",
      "[Trial 70] Epoch 17/60, Training Loss: 0.1266, Validation Loss: 0.1256\n",
      "[Trial 64] Epoch 36/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 9/60, Training Loss: 0.1277, Validation Loss: 0.1595\n",
      "[Trial 71] Epoch 16/60, Training Loss: 0.1266, Validation Loss: 0.1305\n",
      "[Trial 66] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1257\n",
      "[Trial 72] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1241\n",
      "[Trial 70] Epoch 18/60, Training Loss: 0.1264, Validation Loss: 0.1252\n",
      "[Trial 64] Epoch 37/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 10/60, Training Loss: 0.1275, Validation Loss: 0.1516\n",
      "[Trial 71] Epoch 17/60, Training Loss: 0.1268, Validation Loss: 0.1299\n",
      "[Trial 66] Epoch 34/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 22/60, Training Loss: 0.1256, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 19/60, Training Loss: 0.1268, Validation Loss: 0.1250\n",
      "[Trial 72] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1242\n",
      "[Trial 64] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.1471\n",
      "[Trial 71] Epoch 18/60, Training Loss: 0.1266, Validation Loss: 0.1285\n",
      "[Trial 66] Epoch 35/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 20/60, Training Loss: 0.1266, Validation Loss: 0.1248\n",
      "[Trial 72] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1240\n",
      "[Trial 64] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 12/60, Training Loss: 0.1272, Validation Loss: 0.1412\n",
      "[Trial 71] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1272\n",
      "[Trial 66] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 24/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1247\n",
      "[Trial 72] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.1241\n",
      "[Trial 64] Epoch 40/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 13/60, Training Loss: 0.1316, Validation Loss: 0.1373\n",
      "[Trial 71] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1268\n",
      "[Trial 66] Epoch 37/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 69] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1246\n",
      "[Trial 64] Epoch 41/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 72] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1240\n",
      "[Trial 73] Epoch 14/60, Training Loss: 0.1272, Validation Loss: 0.1345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:58:54,630] Trial 66 finished with value: 0.12607989770670733 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.002162302794006986, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 66] Epoch 38/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 66] Early stopping after 38 epochs.\n",
      "[Trial 71] Epoch 21/60, Training Loss: 0.1264, Validation Loss: 0.1267\n",
      "[Trial 69] Epoch 26/60, Training Loss: 0.1255, Validation Loss: 0.1257\n",
      "[Trial 70] Epoch 23/60, Training Loss: 0.1264, Validation Loss: 0.1246\n",
      "[Trial 64] Epoch 42/60, Training Loss: 0.1258, Validation Loss: 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 12:59:14,880] Trial 72 finished with value: 0.12387513760477305 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.009111127875487516, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 72] Epoch 15/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 72] Early stopping after 15 epochs.\n",
      "[Trial 74] Epoch 1/60, Training Loss: 0.6194, Validation Loss: 0.1982\n",
      "[Trial 73] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1313\n",
      "[Trial 71] Epoch 22/60, Training Loss: 0.1262, Validation Loss: 0.1261\n",
      "[Trial 70] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1246\n",
      "[Trial 69] Epoch 27/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 64] Epoch 43/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 75] Epoch 1/60, Training Loss: 20645880.9679, Validation Loss: 79.5101\n",
      "[Trial 74] Epoch 2/60, Training Loss: 0.1301, Validation Loss: 0.1721\n",
      "[Trial 73] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1303\n",
      "[Trial 71] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1263\n",
      "[Trial 70] Epoch 25/60, Training Loss: 0.1262, Validation Loss: 0.1246\n",
      "[Trial 69] Epoch 28/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 64] Epoch 44/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 74] Epoch 3/60, Training Loss: 0.1285, Validation Loss: 0.1561\n",
      "[Trial 75] Epoch 2/60, Training Loss: 92.8006, Validation Loss: 30.2496\n",
      "[Trial 73] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1284\n",
      "[Trial 71] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1263\n",
      "[Trial 70] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1246\n",
      "[Trial 69] Epoch 29/60, Training Loss: 0.1254, Validation Loss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:01:03,757] Trial 64 finished with value: 0.12515740487724542 and parameters: {'hidden_dim': 384, 'latent_dim': 96, 'learning_rate': 0.0013184337058264979, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 64] Epoch 45/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 64] Early stopping after 45 epochs.\n",
      "[Trial 74] Epoch 4/60, Training Loss: 0.1280, Validation Loss: 0.1487\n",
      "[Trial 75] Epoch 3/60, Training Loss: 41.5875, Validation Loss: 8.6471\n",
      "[Trial 73] Epoch 18/60, Training Loss: 0.1266, Validation Loss: 0.1276\n",
      "[Trial 71] Epoch 25/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 74] Epoch 5/60, Training Loss: 0.1275, Validation Loss: 0.1406\n",
      "[Trial 70] Epoch 27/60, Training Loss: 0.1263, Validation Loss: 0.1248\n",
      "[Trial 69] Epoch 30/60, Training Loss: 0.1251, Validation Loss: 0.1257\n",
      "[Trial 76] Epoch 1/60, Training Loss: 192249.9230, Validation Loss: 0.6098\n",
      "[Trial 75] Epoch 4/60, Training Loss: 16.4680, Validation Loss: 2.2396\n",
      "[Trial 73] Epoch 19/60, Training Loss: 0.1266, Validation Loss: 0.1269\n",
      "[Trial 74] Epoch 6/60, Training Loss: 0.1280, Validation Loss: 0.1456\n",
      "[Trial 71] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1254\n",
      "[Trial 70] Epoch 28/60, Training Loss: 0.1260, Validation Loss: 0.1247\n",
      "[Trial 76] Epoch 2/60, Training Loss: 1.2487, Validation Loss: 0.2598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:02:17,380] Trial 69 finished with value: 0.1256867551555236 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.002387684019688847, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 69] Epoch 31/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 69] Early stopping after 31 epochs.\n",
      "[Trial 75] Epoch 5/60, Training Loss: 6.8048, Validation Loss: 1.9603\n",
      "[Trial 77] Epoch 1/60, Training Loss: 1.2490, Validation Loss: 0.1835\n",
      "[Trial 74] Epoch 7/60, Training Loss: 0.1273, Validation Loss: 0.1338\n",
      "[Trial 73] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1264\n",
      "[Trial 71] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 77] Epoch 2/60, Training Loss: 0.1330, Validation Loss: 0.1557\n",
      "[Trial 70] Epoch 29/60, Training Loss: 0.1264, Validation Loss: 0.1247\n",
      "[Trial 76] Epoch 3/60, Training Loss: 0.4308, Validation Loss: 0.1807\n",
      "[Trial 77] Epoch 3/60, Training Loss: 0.1291, Validation Loss: 0.1411\n",
      "[Trial 75] Epoch 6/60, Training Loss: 3.2065, Validation Loss: 0.6365\n",
      "[Trial 74] Epoch 8/60, Training Loss: 0.1268, Validation Loss: 0.1339\n",
      "[Trial 73] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1259\n",
      "[Trial 77] Epoch 4/60, Training Loss: 0.1277, Validation Loss: 0.1340\n",
      "[Trial 71] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 76] Epoch 4/60, Training Loss: 0.2679, Validation Loss: 0.1477\n",
      "[Trial 70] Epoch 30/60, Training Loss: 0.1260, Validation Loss: 0.1246\n",
      "[Trial 77] Epoch 5/60, Training Loss: 0.1272, Validation Loss: 0.1303\n",
      "[Trial 74] Epoch 9/60, Training Loss: 0.1265, Validation Loss: 0.1295\n",
      "[Trial 75] Epoch 7/60, Training Loss: 1.7751, Validation Loss: 0.5471\n",
      "[Trial 77] Epoch 6/60, Training Loss: 0.1267, Validation Loss: 0.1282\n",
      "[Trial 73] Epoch 22/60, Training Loss: 0.1261, Validation Loss: 0.1258\n",
      "[Trial 76] Epoch 5/60, Training Loss: 0.1871, Validation Loss: 0.1340\n",
      "[Trial 71] Epoch 29/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 70] Epoch 31/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 74] Epoch 10/60, Training Loss: 0.1267, Validation Loss: 0.1303\n",
      "[Trial 77] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1270\n",
      "[Trial 75] Epoch 8/60, Training Loss: 1.2699, Validation Loss: 0.3346\n",
      "[Trial 77] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1268\n",
      "[Trial 73] Epoch 23/60, Training Loss: 0.1264, Validation Loss: 0.1256\n",
      "[Trial 76] Epoch 6/60, Training Loss: 0.1601, Validation Loss: 0.1304\n",
      "[Trial 70] Epoch 32/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 71] Epoch 30/60, Training Loss: 0.1260, Validation Loss: 0.1254\n",
      "[Trial 74] Epoch 11/60, Training Loss: 0.1272, Validation Loss: 0.1278\n",
      "[Trial 77] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1262\n",
      "[Trial 75] Epoch 9/60, Training Loss: 0.8461, Validation Loss: 0.2290\n",
      "[Trial 77] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1264\n",
      "[Trial 76] Epoch 7/60, Training Loss: 0.1488, Validation Loss: 0.1284\n",
      "[Trial 73] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 70] Epoch 33/60, Training Loss: 0.1261, Validation Loss: 0.1246\n",
      "[Trial 74] Epoch 12/60, Training Loss: 0.1263, Validation Loss: 0.1276\n",
      "[Trial 71] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1253\n",
      "[Trial 77] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1260\n",
      "[Trial 77] Epoch 12/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 75] Epoch 10/60, Training Loss: 0.5668, Validation Loss: 0.1925\n",
      "[Trial 76] Epoch 8/60, Training Loss: 0.1417, Validation Loss: 0.1274\n",
      "[Trial 74] Epoch 13/60, Training Loss: 0.1265, Validation Loss: 0.1261\n",
      "[Trial 70] Epoch 34/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 73] Epoch 25/60, Training Loss: 0.1266, Validation Loss: 0.1254\n",
      "[Trial 71] Epoch 32/60, Training Loss: 0.1261, Validation Loss: 0.1255\n",
      "[Trial 77] Epoch 13/60, Training Loss: 0.1262, Validation Loss: 0.1260\n",
      "[Trial 77] Epoch 14/60, Training Loss: 0.1260, Validation Loss: 0.1258\n",
      "[Trial 75] Epoch 11/60, Training Loss: 0.3769, Validation Loss: 0.1708\n",
      "[Trial 74] Epoch 14/60, Training Loss: 0.1266, Validation Loss: 0.1269\n",
      "[Trial 76] Epoch 9/60, Training Loss: 0.1378, Validation Loss: 0.1270\n",
      "[Trial 70] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1246\n",
      "[Trial 73] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1254\n",
      "[Trial 77] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1258\n",
      "[Trial 71] Epoch 33/60, Training Loss: 0.1260, Validation Loss: 0.1253\n",
      "[Trial 77] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 74] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1266\n",
      "[Trial 75] Epoch 12/60, Training Loss: 0.3487, Validation Loss: 0.1684\n",
      "[Trial 76] Epoch 10/60, Training Loss: 0.1327, Validation Loss: 0.1283\n",
      "[Trial 70] Epoch 36/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 77] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 73] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 71] Epoch 34/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 77] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 74] Epoch 16/60, Training Loss: 0.1266, Validation Loss: 0.1261\n",
      "[Trial 76] Epoch 11/60, Training Loss: 0.1344, Validation Loss: 0.1273\n",
      "[Trial 75] Epoch 13/60, Training Loss: 0.2925, Validation Loss: 0.1389\n",
      "[Trial 77] Epoch 19/60, Training Loss: 0.1257, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:07:51,792] Trial 70 finished with value: 0.12460309956222773 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.001414959288302941, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 70] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1246\n",
      "[Trial 70] Early stopping after 37 epochs.\n",
      "[Trial 73] Epoch 28/60, Training Loss: 0.1263, Validation Loss: 0.1254\n",
      "[Trial 71] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 74] Epoch 17/60, Training Loss: 0.1259, Validation Loss: 0.1260\n",
      "[Trial 77] Epoch 20/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 78] Epoch 1/60, Training Loss: 0.8900, Validation Loss: 0.1817\n",
      "[Trial 76] Epoch 12/60, Training Loss: 0.1359, Validation Loss: 0.1269\n",
      "[Trial 77] Epoch 21/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 78] Epoch 2/60, Training Loss: 0.1323, Validation Loss: 0.1572\n",
      "[Trial 75] Epoch 14/60, Training Loss: 0.2079, Validation Loss: 0.1436\n",
      "[Trial 73] Epoch 29/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 74] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 71] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 78] Epoch 3/60, Training Loss: 0.1294, Validation Loss: 0.1434\n",
      "[Trial 77] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1255\n",
      "[Trial 76] Epoch 13/60, Training Loss: 0.1291, Validation Loss: 0.1269\n",
      "[Trial 78] Epoch 4/60, Training Loss: 0.1278, Validation Loss: 0.1344\n",
      "[Trial 77] Epoch 23/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 75] Epoch 15/60, Training Loss: 0.1807, Validation Loss: 0.1303\n",
      "[Trial 74] Epoch 19/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 73] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 78] Epoch 5/60, Training Loss: 0.1271, Validation Loss: 0.1319\n",
      "[Trial 77] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 71] Epoch 37/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 76] Epoch 14/60, Training Loss: 0.1372, Validation Loss: 0.1266\n",
      "[Trial 78] Epoch 6/60, Training Loss: 0.1271, Validation Loss: 0.1280\n",
      "[Trial 77] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 74] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 75] Epoch 16/60, Training Loss: 0.1737, Validation Loss: 0.1314\n",
      "[Trial 73] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1272\n",
      "[Trial 77] Epoch 26/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 71] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 8/60, Training Loss: 0.1263, Validation Loss: 0.1259\n",
      "[Trial 76] Epoch 15/60, Training Loss: 0.1269, Validation Loss: 0.1273\n",
      "[Trial 77] Epoch 27/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 74] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1256\n",
      "[Trial 75] Epoch 17/60, Training Loss: 0.1657, Validation Loss: 0.1304\n",
      "[Trial 78] Epoch 9/60, Training Loss: 0.1265, Validation Loss: 0.1255\n",
      "[Trial 77] Epoch 28/60, Training Loss: 0.1253, Validation Loss: 0.1254\n",
      "[Trial 73] Epoch 32/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 71] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 78] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 76] Epoch 16/60, Training Loss: 0.1277, Validation Loss: 0.1320\n",
      "[Trial 77] Epoch 29/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 74] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 78] Epoch 11/60, Training Loss: 0.1261, Validation Loss: 0.1254\n",
      "[Trial 77] Epoch 30/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 75] Epoch 18/60, Training Loss: 0.1520, Validation Loss: 0.1509\n",
      "[Trial 73] Epoch 33/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 71] Epoch 40/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 12/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 74] Epoch 23/60, Training Loss: 0.1260, Validation Loss: 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:11:24,285] Trial 77 finished with value: 0.12536118117471537 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0010027852312977995, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 77] Epoch 31/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 77] Early stopping after 31 epochs.\n",
      "[Trial 76] Epoch 17/60, Training Loss: 0.1280, Validation Loss: 0.1304\n",
      "[Trial 78] Epoch 13/60, Training Loss: 0.1261, Validation Loss: 0.1254\n",
      "[Trial 79] Epoch 1/60, Training Loss: 0.5368, Validation Loss: 0.1332\n",
      "[Trial 75] Epoch 19/60, Training Loss: 0.1590, Validation Loss: 0.1272\n",
      "[Trial 73] Epoch 34/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 74] Epoch 24/60, Training Loss: 0.1258, Validation Loss: 0.1256\n",
      "[Trial 78] Epoch 14/60, Training Loss: 0.1263, Validation Loss: 0.1252\n",
      "[Trial 71] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 79] Epoch 2/60, Training Loss: 0.1258, Validation Loss: 0.1290\n",
      "[Trial 76] Epoch 18/60, Training Loss: 0.1264, Validation Loss: 0.1265\n",
      "[Trial 78] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1257\n",
      "[Trial 79] Epoch 3/60, Training Loss: 0.1254, Validation Loss: 0.1294\n",
      "[Trial 75] Epoch 20/60, Training Loss: 0.1404, Validation Loss: 0.1268\n",
      "[Trial 73] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 74] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 78] Epoch 16/60, Training Loss: 0.1260, Validation Loss: 0.1254\n",
      "[Trial 79] Epoch 4/60, Training Loss: 0.1253, Validation Loss: 0.1293\n",
      "[Trial 71] Epoch 42/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 76] Epoch 19/60, Training Loss: 0.1258, Validation Loss: 0.1265\n",
      "[Trial 78] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 79] Epoch 5/60, Training Loss: 0.1254, Validation Loss: 0.1294\n",
      "[Trial 74] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 78] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 73] Epoch 36/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 75] Epoch 21/60, Training Loss: 0.1393, Validation Loss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:13:10,658] Trial 79 finished with value: 0.12903032662967842 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.01638819295663316, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 79] Epoch 6/60, Training Loss: 0.1247, Validation Loss: 0.1296\n",
      "[Trial 79] Early stopping after 6 epochs.\n",
      "[Trial 76] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 71] Epoch 43/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 19/60, Training Loss: 0.1261, Validation Loss: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:13:34,190] Trial 74 finished with value: 0.12553933604309955 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.0012834870015437584, 'batch_size': 8, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 74] Epoch 27/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 74] Early stopping after 27 epochs.\n",
      "[Trial 78] Epoch 20/60, Training Loss: 0.1259, Validation Loss: 0.1265\n",
      "[Trial 80] Epoch 1/60, Training Loss: 23.3221, Validation Loss: 0.1306\n",
      "[Trial 73] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 75] Epoch 22/60, Training Loss: 0.1376, Validation Loss: 0.1270\n",
      "[Trial 76] Epoch 21/60, Training Loss: 0.1264, Validation Loss: 0.1275\n",
      "[Trial 81] Epoch 1/60, Training Loss: 0.2954, Validation Loss: 0.1482\n",
      "[Trial 71] Epoch 44/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 21/60, Training Loss: 0.1262, Validation Loss: 0.1254\n",
      "[Trial 81] Epoch 2/60, Training Loss: 0.1285, Validation Loss: 0.1290\n",
      "[Trial 78] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 80] Epoch 2/60, Training Loss: 0.1305, Validation Loss: 0.1246\n",
      "[Trial 73] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 75] Epoch 23/60, Training Loss: 0.1354, Validation Loss: 0.1273\n",
      "[Trial 76] Epoch 22/60, Training Loss: 0.1265, Validation Loss: 0.1269\n",
      "[Trial 81] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1273\n",
      "[Trial 78] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 71] Epoch 45/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 81] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1239\n",
      "[Trial 78] Epoch 24/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 80] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1256\n",
      "[Trial 73] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 76] Epoch 23/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 81] Epoch 5/60, Training Loss: 0.1269, Validation Loss: 0.1234\n",
      "[Trial 75] Epoch 24/60, Training Loss: 0.1326, Validation Loss: 0.1265\n",
      "[Trial 78] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 71] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1249\n",
      "[Trial 81] Epoch 6/60, Training Loss: 0.1267, Validation Loss: 0.1232\n",
      "[Trial 80] Epoch 4/60, Training Loss: 0.1290, Validation Loss: 0.1247\n",
      "[Trial 76] Epoch 24/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 73] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 27/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 81] Epoch 7/60, Training Loss: 0.1262, Validation Loss: 0.1232\n",
      "[Trial 75] Epoch 25/60, Training Loss: 0.1329, Validation Loss: 0.1264\n",
      "[Trial 71] Epoch 47/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 78] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1249\n",
      "[Trial 80] Epoch 5/60, Training Loss: 0.1262, Validation Loss: 0.1245\n",
      "[Trial 81] Epoch 8/60, Training Loss: 0.1268, Validation Loss: 0.1240\n",
      "[Trial 78] Epoch 29/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 76] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1265\n",
      "[Trial 73] Epoch 41/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 81] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1230\n",
      "[Trial 75] Epoch 26/60, Training Loss: 0.1319, Validation Loss: 0.1270\n",
      "[Trial 71] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 80] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1254\n",
      "[Trial 78] Epoch 30/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 81] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:16:51,434] Trial 78 finished with value: 0.12476714042325815 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0010740782088342411, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 78] Epoch 31/60, Training Loss: 0.1254, Validation Loss: 0.1248\n",
      "[Trial 78] Early stopping after 31 epochs.\n",
      "[Trial 76] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 73] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 82] Epoch 1/60, Training Loss: 3.6768, Validation Loss: 0.1996\n",
      "[Trial 81] Epoch 11/60, Training Loss: 0.1266, Validation Loss: 0.1233\n",
      "[Trial 82] Epoch 2/60, Training Loss: 0.1315, Validation Loss: 0.1560\n",
      "[Trial 75] Epoch 27/60, Training Loss: 0.1326, Validation Loss: 0.1276\n",
      "[Trial 80] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "[Trial 71] Epoch 49/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 82] Epoch 3/60, Training Loss: 0.1285, Validation Loss: 0.1368\n",
      "[Trial 81] Epoch 12/60, Training Loss: 0.1260, Validation Loss: 0.1230\n",
      "[Trial 82] Epoch 4/60, Training Loss: 0.1275, Validation Loss: 0.1308\n",
      "[Trial 82] Epoch 5/60, Training Loss: 0.1274, Validation Loss: 0.1281\n",
      "[Trial 76] Epoch 27/60, Training Loss: 0.1254, Validation Loss: 0.1264\n",
      "[Trial 82] Epoch 6/60, Training Loss: 0.1271, Validation Loss: 0.1261\n",
      "[Trial 81] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1230\n",
      "[Trial 73] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 82] Epoch 7/60, Training Loss: 0.1268, Validation Loss: 0.1267\n",
      "[Trial 80] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1245\n",
      "[Trial 82] Epoch 8/60, Training Loss: 0.1266, Validation Loss: 0.1260\n",
      "[Trial 75] Epoch 28/60, Training Loss: 0.1300, Validation Loss: 0.1264\n",
      "[Trial 71] Epoch 50/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 82] Epoch 9/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 81] Epoch 14/60, Training Loss: 0.1260, Validation Loss: 0.1230\n",
      "[Trial 82] Epoch 10/60, Training Loss: 0.1265, Validation Loss: 0.1251\n",
      "[Trial 82] Epoch 11/60, Training Loss: 0.1265, Validation Loss: 0.1260\n",
      "[Trial 76] Epoch 28/60, Training Loss: 0.1252, Validation Loss: 0.1264\n",
      "[Trial 82] Epoch 12/60, Training Loss: 0.1268, Validation Loss: 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:18:16,347] Trial 81 finished with value: 0.1229510615269343 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.006927129776258818, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 81] Epoch 15/60, Training Loss: 0.1258, Validation Loss: 0.1230\n",
      "[Trial 81] Early stopping after 15 epochs.\n",
      "[Trial 80] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1244\n",
      "[Trial 73] Epoch 44/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 82] Epoch 13/60, Training Loss: 0.1267, Validation Loss: 0.1267\n",
      "[Trial 82] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1250\n",
      "[Trial 75] Epoch 29/60, Training Loss: 0.1289, Validation Loss: 0.1262\n",
      "[Trial 71] Epoch 51/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 83] Epoch 1/60, Training Loss: 0.3795, Validation Loss: 0.1466\n",
      "[Trial 82] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 82] Epoch 16/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 76] Epoch 29/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 82] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1244\n",
      "[Trial 80] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 83] Epoch 2/60, Training Loss: 0.1279, Validation Loss: 0.1328\n",
      "[Trial 82] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1244\n",
      "[Trial 73] Epoch 45/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 82] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1247\n",
      "[Trial 82] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 75] Epoch 30/60, Training Loss: 0.1293, Validation Loss: 0.1262\n",
      "[Trial 71] Epoch 52/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 83] Epoch 3/60, Training Loss: 0.1271, Validation Loss: 0.1291\n",
      "[Trial 82] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 82] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 80] Epoch 11/60, Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "[Trial 76] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1266\n",
      "[Trial 82] Epoch 23/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 83] Epoch 4/60, Training Loss: 0.1267, Validation Loss: 0.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:19:37,116] Trial 82 finished with value: 0.12427207032839457 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.006011850468195012, 'batch_size': 64, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 82] Epoch 24/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 82] Early stopping after 24 epochs.\n",
      "[Trial 73] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 75] Epoch 31/60, Training Loss: 0.1275, Validation Loss: 0.1262\n",
      "[Trial 83] Epoch 5/60, Training Loss: 0.1265, Validation Loss: 0.1264\n",
      "[Trial 71] Epoch 53/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 84] Epoch 1/60, Training Loss: 0.5980, Validation Loss: 0.1743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:20:00,571] Trial 80 finished with value: 0.12443151529878378 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.02413172003767094, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 80] Epoch 12/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 80] Early stopping after 12 epochs.\n",
      "[Trial 76] Epoch 31/60, Training Loss: 0.1251, Validation Loss: 0.1265\n",
      "[Trial 83] Epoch 6/60, Training Loss: 0.1260, Validation Loss: 0.1263\n",
      "[Trial 84] Epoch 2/60, Training Loss: 0.1292, Validation Loss: 0.1425\n",
      "[Trial 73] Epoch 47/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 85] Epoch 1/60, Training Loss: 0.6664, Validation Loss: 0.1780\n",
      "[Trial 83] Epoch 7/60, Training Loss: 0.1263, Validation Loss: 0.1256\n",
      "[Trial 75] Epoch 32/60, Training Loss: 0.1278, Validation Loss: 0.1264\n",
      "[Trial 84] Epoch 3/60, Training Loss: 0.1279, Validation Loss: 0.1272\n",
      "[Trial 71] Epoch 54/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 85] Epoch 2/60, Training Loss: 0.1286, Validation Loss: 0.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:20:42,184] Trial 76 finished with value: 0.12643266295393307 and parameters: {'hidden_dim': 384, 'latent_dim': 64, 'learning_rate': 0.01756834170152468, 'batch_size': 8, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 76] Epoch 32/60, Training Loss: 0.1253, Validation Loss: 0.1264\n",
      "[Trial 76] Early stopping after 32 epochs.\n",
      "[Trial 83] Epoch 8/60, Training Loss: 0.1259, Validation Loss: 0.1256\n",
      "[Trial 84] Epoch 4/60, Training Loss: 0.1265, Validation Loss: 0.1252\n",
      "[Trial 73] Epoch 48/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 85] Epoch 3/60, Training Loss: 0.1281, Validation Loss: 0.1273\n",
      "[Trial 86] Epoch 1/60, Training Loss: 0.3698, Validation Loss: 0.1910\n",
      "[Trial 83] Epoch 9/60, Training Loss: 0.1258, Validation Loss: 0.1255\n",
      "[Trial 84] Epoch 5/60, Training Loss: 0.1271, Validation Loss: 0.1251\n",
      "[Trial 75] Epoch 33/60, Training Loss: 0.1284, Validation Loss: 0.1262\n",
      "[Trial 71] Epoch 55/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 85] Epoch 4/60, Training Loss: 0.1273, Validation Loss: 0.1262\n",
      "[Trial 86] Epoch 2/60, Training Loss: 0.1298, Validation Loss: 0.1633\n",
      "[Trial 83] Epoch 10/60, Training Loss: 0.1256, Validation Loss: 0.1259\n",
      "[Trial 84] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1288\n",
      "[Trial 86] Epoch 3/60, Training Loss: 0.1278, Validation Loss: 0.1449\n",
      "[Trial 73] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 85] Epoch 5/60, Training Loss: 0.1273, Validation Loss: 0.1263\n",
      "[Trial 83] Epoch 11/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 84] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1251\n",
      "[Trial 75] Epoch 34/60, Training Loss: 0.1292, Validation Loss: 0.1263\n",
      "[Trial 71] Epoch 56/60, Training Loss: 0.1250, Validation Loss: 0.1252\n",
      "[Trial 86] Epoch 4/60, Training Loss: 0.1274, Validation Loss: 0.1339\n",
      "[Trial 85] Epoch 6/60, Training Loss: 0.1268, Validation Loss: 0.1264\n",
      "[Trial 83] Epoch 12/60, Training Loss: 0.1259, Validation Loss: 0.1256\n",
      "[Trial 84] Epoch 8/60, Training Loss: 0.1264, Validation Loss: 0.1251\n",
      "[Trial 86] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:22:12,365] Trial 85 finished with value: 0.1262221636871497 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'learning_rate': 0.009056298421044787, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 85] Epoch 7/60, Training Loss: 0.1259, Validation Loss: 0.1263\n",
      "[Trial 85] Early stopping after 7 epochs.\n",
      "[Trial 73] Epoch 50/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 83] Epoch 13/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 84] Epoch 9/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 86] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:22:30,208] Trial 71 finished with value: 0.12521197410921256 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.001169118667622305, 'batch_size': 8, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 71] Epoch 57/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 71] Early stopping after 57 epochs.\n",
      "[Trial 87] Epoch 1/60, Training Loss: 0.7669, Validation Loss: 0.1578\n",
      "[Trial 75] Epoch 35/60, Training Loss: 0.1266, Validation Loss: 0.1263\n",
      "[Trial 83] Epoch 14/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 84] Epoch 10/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 86] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1297\n",
      "[Trial 87] Epoch 2/60, Training Loss: 0.1291, Validation Loss: 0.1393\n",
      "[Trial 88] Epoch 1/60, Training Loss: 0.4907, Validation Loss: 0.1530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:22:52,349] Trial 73 finished with value: 0.12519605793058872 and parameters: {'hidden_dim': 384, 'latent_dim': 96, 'learning_rate': 0.0012314348465771979, 'batch_size': 8, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 73] Epoch 51/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 73] Early stopping after 51 epochs.\n",
      "[Trial 83] Epoch 15/60, Training Loss: 0.1258, Validation Loss: 0.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:23:01,230] Trial 84 finished with value: 0.1251205638051033 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'learning_rate': 0.008289172461594486, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 84] Epoch 11/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 84] Early stopping after 11 epochs.\n",
      "[Trial 86] Epoch 8/60, Training Loss: 0.1261, Validation Loss: 0.1277\n",
      "[Trial 87] Epoch 3/60, Training Loss: 0.1278, Validation Loss: 0.1311\n",
      "[Trial 88] Epoch 2/60, Training Loss: 0.1292, Validation Loss: 0.1313\n",
      "[Trial 75] Epoch 36/60, Training Loss: 0.1274, Validation Loss: 0.1262\n",
      "[Trial 89] Epoch 1/60, Training Loss: 0.4639, Validation Loss: 0.1600\n",
      "[Trial 83] Epoch 16/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 86] Epoch 9/60, Training Loss: 0.1262, Validation Loss: 0.1281\n",
      "[Trial 90] Epoch 1/60, Training Loss: 0.5921, Validation Loss: 0.1728\n",
      "[Trial 87] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1282\n",
      "[Trial 88] Epoch 3/60, Training Loss: 0.1276, Validation Loss: 0.1272\n",
      "[Trial 89] Epoch 2/60, Training Loss: 0.1290, Validation Loss: 0.1352\n",
      "[Trial 86] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1276\n",
      "[Trial 90] Epoch 2/60, Training Loss: 0.1295, Validation Loss: 0.1466\n",
      "[Trial 83] Epoch 17/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 87] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1267\n",
      "[Trial 88] Epoch 4/60, Training Loss: 0.1271, Validation Loss: 0.1250\n",
      "[Trial 89] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1298\n",
      "[Trial 75] Epoch 37/60, Training Loss: 0.1282, Validation Loss: 0.1265\n",
      "[Trial 86] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1266\n",
      "[Trial 90] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1356\n",
      "[Trial 83] Epoch 18/60, Training Loss: 0.1253, Validation Loss: 0.1254\n",
      "[Trial 87] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1255\n",
      "[Trial 88] Epoch 5/60, Training Loss: 0.1271, Validation Loss: 0.1244\n",
      "[Trial 89] Epoch 4/60, Training Loss: 0.1264, Validation Loss: 0.1298\n",
      "[Trial 86] Epoch 12/60, Training Loss: 0.1261, Validation Loss: 0.1272\n",
      "[Trial 90] Epoch 4/60, Training Loss: 0.1266, Validation Loss: 0.1312\n",
      "[Trial 83] Epoch 19/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 87] Epoch 7/60, Training Loss: 0.1265, Validation Loss: 0.1252\n",
      "[Trial 89] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1290\n",
      "[Trial 88] Epoch 6/60, Training Loss: 0.1269, Validation Loss: 0.1249\n",
      "[Trial 86] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1295\n",
      "[Trial 75] Epoch 38/60, Training Loss: 0.1270, Validation Loss: 0.1262\n",
      "[Trial 90] Epoch 5/60, Training Loss: 0.1263, Validation Loss: 0.1306\n",
      "[Trial 83] Epoch 20/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 87] Epoch 8/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 89] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1336\n",
      "[Trial 88] Epoch 7/60, Training Loss: 0.1270, Validation Loss: 0.1236\n",
      "[Trial 86] Epoch 14/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 90] Epoch 6/60, Training Loss: 0.1257, Validation Loss: 0.1288\n",
      "[Trial 83] Epoch 21/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 87] Epoch 9/60, Training Loss: 0.1262, Validation Loss: 0.1255\n",
      "[Trial 89] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1272\n",
      "[Trial 88] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1236\n",
      "[Trial 86] Epoch 15/60, Training Loss: 0.1254, Validation Loss: 0.1265\n",
      "[Trial 90] Epoch 7/60, Training Loss: 0.1259, Validation Loss: 0.1283\n",
      "[Trial 75] Epoch 39/60, Training Loss: 0.1267, Validation Loss: 0.1262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:25:13,503] Trial 83 finished with value: 0.12536268221835295 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.00615700491559367, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 83] Epoch 22/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 83] Early stopping after 22 epochs.\n",
      "[Trial 87] Epoch 10/60, Training Loss: 0.1260, Validation Loss: 0.1250\n",
      "[Trial 89] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1271\n",
      "[Trial 88] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1236\n",
      "[Trial 86] Epoch 16/60, Training Loss: 0.1254, Validation Loss: 0.1263\n",
      "[Trial 91] Epoch 1/60, Training Loss: 3.0719, Validation Loss: 0.1973\n",
      "[Trial 90] Epoch 8/60, Training Loss: 0.1257, Validation Loss: 0.1276\n",
      "[Trial 87] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1248\n",
      "[Trial 89] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1277\n",
      "[Trial 91] Epoch 2/60, Training Loss: 0.1433, Validation Loss: 0.1654\n",
      "[Trial 86] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1263\n",
      "[Trial 88] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1252\n",
      "[Trial 90] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1276\n",
      "[Trial 91] Epoch 3/60, Training Loss: 0.1347, Validation Loss: 0.1486\n",
      "[Trial 75] Epoch 40/60, Training Loss: 0.1260, Validation Loss: 0.1262\n",
      "[Trial 87] Epoch 12/60, Training Loss: 0.1263, Validation Loss: 0.1251\n",
      "[Trial 89] Epoch 10/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 86] Epoch 18/60, Training Loss: 0.1254, Validation Loss: 0.1263\n",
      "[Trial 91] Epoch 4/60, Training Loss: 0.1315, Validation Loss: 0.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:25:58,179] Trial 88 finished with value: 0.12355194141467413 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.003098221059274613, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 88] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 88] Early stopping after 11 epochs.\n",
      "[Trial 90] Epoch 10/60, Training Loss: 0.1258, Validation Loss: 0.1279\n",
      "[Trial 91] Epoch 5/60, Training Loss: 0.1300, Validation Loss: 0.1340\n",
      "[Trial 87] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1250\n",
      "[Trial 89] Epoch 11/60, Training Loss: 0.1263, Validation Loss: 0.1271\n",
      "[Trial 86] Epoch 19/60, Training Loss: 0.1258, Validation Loss: 0.1270\n",
      "[Trial 92] Epoch 1/60, Training Loss: 0.7877, Validation Loss: 0.1787\n",
      "[Trial 91] Epoch 6/60, Training Loss: 0.1287, Validation Loss: 0.1303\n",
      "[Trial 90] Epoch 11/60, Training Loss: 0.1256, Validation Loss: 0.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:26:28,647] Trial 87 finished with value: 0.12475217829147975 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0017290821961102078, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 87] Epoch 14/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 87] Early stopping after 14 epochs.\n",
      "[Trial 91] Epoch 7/60, Training Loss: 0.1281, Validation Loss: 0.1293\n",
      "[Trial 89] Epoch 12/60, Training Loss: 0.1259, Validation Loss: 0.1271\n",
      "[Trial 86] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 75] Epoch 41/60, Training Loss: 0.1259, Validation Loss: 0.1261\n",
      "[Trial 92] Epoch 2/60, Training Loss: 0.1308, Validation Loss: 0.1474\n",
      "[Trial 90] Epoch 12/60, Training Loss: 0.1255, Validation Loss: 0.1270\n",
      "[Trial 91] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1274\n",
      "[Trial 93] Epoch 1/60, Training Loss: 0.5561, Validation Loss: 0.1479\n",
      "[Trial 89] Epoch 13/60, Training Loss: 0.1253, Validation Loss: 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:26:47,943] Trial 86 finished with value: 0.12625352243582408 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.0017214996863515054, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 86] Epoch 21/60, Training Loss: 0.1254, Validation Loss: 0.1264\n",
      "[Trial 86] Early stopping after 21 epochs.\n",
      "[Trial 91] Epoch 9/60, Training Loss: 0.1272, Validation Loss: 0.1271\n",
      "[Trial 92] Epoch 3/60, Training Loss: 0.1280, Validation Loss: 0.1350\n",
      "[Trial 90] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1271\n",
      "[Trial 91] Epoch 10/60, Training Loss: 0.1269, Validation Loss: 0.1272\n",
      "[Trial 94] Epoch 1/60, Training Loss: 0.5126, Validation Loss: 0.1538\n",
      "[Trial 93] Epoch 2/60, Training Loss: 0.1285, Validation Loss: 0.1350\n",
      "[Trial 89] Epoch 14/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 75] Epoch 42/60, Training Loss: 0.1259, Validation Loss: 0.1261\n",
      "[Trial 91] Epoch 11/60, Training Loss: 0.1269, Validation Loss: 0.1270\n",
      "[Trial 90] Epoch 14/60, Training Loss: 0.1253, Validation Loss: 0.1274\n",
      "[Trial 92] Epoch 4/60, Training Loss: 0.1276, Validation Loss: 0.1322\n",
      "[Trial 94] Epoch 2/60, Training Loss: 0.1281, Validation Loss: 0.1329\n",
      "[Trial 91] Epoch 12/60, Training Loss: 0.1267, Validation Loss: 0.1261\n",
      "[Trial 93] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1292\n",
      "[Trial 89] Epoch 15/60, Training Loss: 0.1255, Validation Loss: 0.1265\n",
      "[Trial 90] Epoch 15/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 92] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1275\n",
      "[Trial 91] Epoch 13/60, Training Loss: 0.1272, Validation Loss: 0.1262\n",
      "[Trial 94] Epoch 3/60, Training Loss: 0.1273, Validation Loss: 0.1273\n",
      "[Trial 93] Epoch 4/60, Training Loss: 0.1270, Validation Loss: 0.1270\n",
      "[Trial 89] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1264\n",
      "[Trial 91] Epoch 14/60, Training Loss: 0.1263, Validation Loss: 0.1262\n",
      "[Trial 90] Epoch 16/60, Training Loss: 0.1256, Validation Loss: 0.1269\n",
      "[Trial 75] Epoch 43/60, Training Loss: 0.1258, Validation Loss: 0.1261\n",
      "[Trial 92] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1265\n",
      "[Trial 94] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1280\n",
      "[Trial 91] Epoch 15/60, Training Loss: 0.1270, Validation Loss: 0.1260\n",
      "[Trial 89] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1266\n",
      "[Trial 93] Epoch 5/60, Training Loss: 0.1265, Validation Loss: 0.1271\n",
      "[Trial 91] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1267\n",
      "[Trial 90] Epoch 17/60, Training Loss: 0.1253, Validation Loss: 0.1270\n",
      "[Trial 94] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1252\n",
      "[Trial 92] Epoch 7/60, Training Loss: 0.1265, Validation Loss: 0.1267\n",
      "[Trial 91] Epoch 17/60, Training Loss: 0.1271, Validation Loss: 0.1275\n",
      "[Trial 89] Epoch 18/60, Training Loss: 0.1254, Validation Loss: 0.1265\n",
      "[Trial 93] Epoch 6/60, Training Loss: 0.1269, Validation Loss: 0.1268\n",
      "[Trial 90] Epoch 18/60, Training Loss: 0.1250, Validation Loss: 0.1274\n",
      "[Trial 94] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1253\n",
      "[Trial 91] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1258\n",
      "[Trial 92] Epoch 8/60, Training Loss: 0.1266, Validation Loss: 0.1268\n",
      "[Trial 75] Epoch 44/60, Training Loss: 0.1257, Validation Loss: 0.1262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:28:36,839] Trial 89 finished with value: 0.12644275538623334 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0032265606536204753, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 89] Epoch 19/60, Training Loss: 0.1254, Validation Loss: 0.1265\n",
      "[Trial 89] Early stopping after 19 epochs.\n",
      "[Trial 93] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 91] Epoch 19/60, Training Loss: 0.1266, Validation Loss: 0.1258\n",
      "[Trial 94] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:28:44,833] Trial 90 finished with value: 0.12690461886425813 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0016707205241807245, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 90] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1270\n",
      "[Trial 90] Early stopping after 19 epochs.\n",
      "[Trial 91] Epoch 20/60, Training Loss: 0.1267, Validation Loss: 0.1255\n",
      "[Trial 92] Epoch 9/60, Training Loss: 0.1266, Validation Loss: 0.1267\n",
      "[Trial 95] Epoch 1/60, Training Loss: 0.5292, Validation Loss: 0.1544\n",
      "[Trial 93] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1300\n",
      "[Trial 91] Epoch 21/60, Training Loss: 0.1271, Validation Loss: 0.1330\n",
      "[Trial 94] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1252\n",
      "[Trial 96] Epoch 1/60, Training Loss: 664.2919, Validation Loss: 0.1288\n",
      "[Trial 92] Epoch 10/60, Training Loss: 0.1265, Validation Loss: 0.1251\n",
      "[Trial 91] Epoch 22/60, Training Loss: 0.1325, Validation Loss: 0.1256\n",
      "[Trial 75] Epoch 45/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 95] Epoch 2/60, Training Loss: 0.1292, Validation Loss: 0.1319\n",
      "[Trial 93] Epoch 9/60, Training Loss: 0.1266, Validation Loss: 0.1255\n",
      "[Trial 94] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1249\n",
      "[Trial 96] Epoch 2/60, Training Loss: 0.1780, Validation Loss: 0.1244\n",
      "[Trial 91] Epoch 23/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 92] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1253\n",
      "[Trial 95] Epoch 3/60, Training Loss: 0.1276, Validation Loss: 0.1266\n",
      "[Trial 91] Epoch 24/60, Training Loss: 0.1260, Validation Loss: 0.1250\n",
      "[Trial 93] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1259\n",
      "[Trial 94] Epoch 10/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 96] Epoch 3/60, Training Loss: 0.4750, Validation Loss: 2.5069\n",
      "[Trial 91] Epoch 25/60, Training Loss: 0.1259, Validation Loss: 0.1249\n",
      "[Trial 92] Epoch 12/60, Training Loss: 0.1260, Validation Loss: 0.1249\n",
      "[Trial 95] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1260\n",
      "[Trial 93] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1259\n",
      "[Trial 94] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1301\n",
      "[Trial 75] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1262\n",
      "[Trial 91] Epoch 26/60, Training Loss: 0.1260, Validation Loss: 0.1260\n",
      "[Trial 96] Epoch 4/60, Training Loss: 0.1970, Validation Loss: 0.1243\n",
      "[Trial 91] Epoch 27/60, Training Loss: 0.1259, Validation Loss: 0.1252\n",
      "[Trial 92] Epoch 13/60, Training Loss: 0.1265, Validation Loss: 0.1246\n",
      "[Trial 95] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1260\n",
      "[Trial 94] Epoch 12/60, Training Loss: 0.1267, Validation Loss: 0.1251\n",
      "[Trial 93] Epoch 12/60, Training Loss: 0.1261, Validation Loss: 0.1258\n",
      "[Trial 96] Epoch 5/60, Training Loss: 0.1342, Validation Loss: 0.1248\n",
      "[Trial 91] Epoch 28/60, Training Loss: 0.1275, Validation Loss: 0.1250\n",
      "[Trial 92] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1249\n",
      "[Trial 94] Epoch 13/60, Training Loss: 0.1262, Validation Loss: 0.1250\n",
      "[Trial 95] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1242\n",
      "[Trial 91] Epoch 29/60, Training Loss: 0.1259, Validation Loss: 0.1250\n",
      "[Trial 93] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 75] Epoch 47/60, Training Loss: 0.1258, Validation Loss: 0.1261\n",
      "[Trial 96] Epoch 6/60, Training Loss: 0.1326, Validation Loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:30:36,890] Trial 91 finished with value: 0.12492814858754477 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0017497654157414488, 'batch_size': 32, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 91] Epoch 30/60, Training Loss: 0.1257, Validation Loss: 0.1250\n",
      "[Trial 91] Early stopping after 30 epochs.\n",
      "[Trial 94] Epoch 14/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 95] Epoch 7/60, Training Loss: 0.1268, Validation Loss: 0.1239\n",
      "[Trial 92] Epoch 15/60, Training Loss: 0.1262, Validation Loss: 0.1248\n",
      "[Trial 93] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1252\n",
      "[Trial 96] Epoch 7/60, Training Loss: 0.1330, Validation Loss: 0.1250\n",
      "[Trial 97] Epoch 1/60, Training Loss: 0.5151, Validation Loss: 0.1637\n",
      "[Trial 94] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1248\n",
      "[Trial 95] Epoch 8/60, Training Loss: 0.1269, Validation Loss: 0.1248\n",
      "[Trial 92] Epoch 16/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 93] Epoch 15/60, Training Loss: 0.1261, Validation Loss: 0.1254\n",
      "[Trial 96] Epoch 8/60, Training Loss: 0.1288, Validation Loss: 0.1244\n",
      "[Trial 75] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 97] Epoch 2/60, Training Loss: 0.1284, Validation Loss: 0.1375\n",
      "[Trial 94] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1247\n",
      "[Trial 95] Epoch 9/60, Training Loss: 0.1269, Validation Loss: 0.1230\n",
      "[Trial 92] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1246\n",
      "[Trial 93] Epoch 16/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 96] Epoch 9/60, Training Loss: 0.1291, Validation Loss: 0.1243\n",
      "[Trial 94] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1247\n",
      "[Trial 97] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1301\n",
      "[Trial 95] Epoch 10/60, Training Loss: 0.1266, Validation Loss: 0.1243\n",
      "[Trial 92] Epoch 18/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 93] Epoch 17/60, Training Loss: 0.1266, Validation Loss: 0.1253\n",
      "[Trial 96] Epoch 10/60, Training Loss: 0.1293, Validation Loss: 0.1243\n",
      "[Trial 94] Epoch 18/60, Training Loss: 0.1257, Validation Loss: 0.1247\n",
      "[Trial 97] Epoch 4/60, Training Loss: 0.1268, Validation Loss: 0.1270\n",
      "[Trial 75] Epoch 49/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 95] Epoch 11/60, Training Loss: 0.1269, Validation Loss: 0.1270\n",
      "[Trial 92] Epoch 19/60, Training Loss: 0.1263, Validation Loss: 0.1247\n",
      "[Trial 93] Epoch 18/60, Training Loss: 0.1259, Validation Loss: 0.1256\n",
      "[Trial 96] Epoch 11/60, Training Loss: 0.1286, Validation Loss: 0.1244\n",
      "[Trial 94] Epoch 19/60, Training Loss: 0.1256, Validation Loss: 0.1247\n",
      "[Trial 97] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 95] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1233\n",
      "[Trial 93] Epoch 19/60, Training Loss: 0.1263, Validation Loss: 0.1251\n",
      "[Trial 92] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1246\n",
      "[Trial 94] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1247\n",
      "[Trial 96] Epoch 12/60, Training Loss: 0.1292, Validation Loss: 0.1244\n",
      "[Trial 97] Epoch 6/60, Training Loss: 0.1265, Validation Loss: 0.1258\n",
      "[Trial 95] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1246\n",
      "[Trial 75] Epoch 50/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 93] Epoch 20/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 92] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 94] Epoch 21/60, Training Loss: 0.1259, Validation Loss: 0.1247\n",
      "[Trial 96] Epoch 13/60, Training Loss: 0.1275, Validation Loss: 0.1246\n",
      "[Trial 97] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1271\n",
      "[Trial 95] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1235\n",
      "[Trial 94] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 93] Epoch 21/60, Training Loss: 0.1262, Validation Loss: 0.1252\n",
      "[Trial 92] Epoch 22/60, Training Loss: 0.1268, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:32:57,170] Trial 96 finished with value: 0.12427784676353136 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.051041749011062475, 'batch_size': 16, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 96] Epoch 14/60, Training Loss: 0.1268, Validation Loss: 0.1243\n",
      "[Trial 96] Early stopping after 14 epochs.\n",
      "[Trial 97] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1256\n",
      "[Trial 95] Epoch 15/60, Training Loss: 0.1263, Validation Loss: 0.1228\n",
      "[Trial 94] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 93] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 75] Epoch 51/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 92] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1246\n",
      "[Trial 98] Epoch 1/60, Training Loss: 1.1251, Validation Loss: 0.1964\n",
      "[Trial 97] Epoch 9/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 95] Epoch 16/60, Training Loss: 0.1263, Validation Loss: 0.1232\n",
      "[Trial 94] Epoch 24/60, Training Loss: 0.1258, Validation Loss: 0.1248\n",
      "[Trial 93] Epoch 23/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 98] Epoch 2/60, Training Loss: 0.1382, Validation Loss: 0.1716\n",
      "[Trial 92] Epoch 24/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 97] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 94] Epoch 25/60, Training Loss: 0.1262, Validation Loss: 0.1247\n",
      "[Trial 95] Epoch 17/60, Training Loss: 0.1262, Validation Loss: 0.1229\n",
      "[Trial 93] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1291\n",
      "[Trial 98] Epoch 3/60, Training Loss: 0.1322, Validation Loss: 0.1563\n",
      "[Trial 92] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:33:52,753] Trial 75 finished with value: 0.12611370359857876 and parameters: {'hidden_dim': 384, 'latent_dim': 64, 'learning_rate': 0.02095891913291495, 'batch_size': 8, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 75] Epoch 52/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 75] Early stopping after 52 epochs.\n",
      "[Trial 97] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 94] Epoch 26/60, Training Loss: 0.1258, Validation Loss: 0.1248\n",
      "[Trial 95] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1228\n",
      "[Trial 99] Epoch 1/60, Training Loss: 6.8593, Validation Loss: 0.2812\n",
      "[Trial 93] Epoch 25/60, Training Loss: 0.1265, Validation Loss: 0.1253\n",
      "[Trial 98] Epoch 4/60, Training Loss: 0.1296, Validation Loss: 0.1480\n",
      "[Trial 92] Epoch 26/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 99] Epoch 2/60, Training Loss: 0.1859, Validation Loss: 0.2213\n",
      "[Trial 94] Epoch 27/60, Training Loss: 0.1253, Validation Loss: 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:34:17,049] Trial 97 finished with value: 0.12557849002381166 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.002883207866673917, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 97] Epoch 12/60, Training Loss: 0.1261, Validation Loss: 0.1258\n",
      "[Trial 97] Early stopping after 12 epochs.\n",
      "[Trial 95] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1227\n",
      "[Trial 99] Epoch 3/60, Training Loss: 0.1557, Validation Loss: 0.1981\n",
      "[Trial 93] Epoch 26/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 98] Epoch 5/60, Training Loss: 0.1281, Validation Loss: 0.1396\n",
      "[Trial 100] Epoch 1/60, Training Loss: 2.3391, Validation Loss: 0.1414\n",
      "[Trial 92] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1252\n",
      "[Trial 94] Epoch 28/60, Training Loss: 0.1256, Validation Loss: 0.1247\n",
      "[Trial 99] Epoch 4/60, Training Loss: 0.1447, Validation Loss: 0.1824\n",
      "[Trial 95] Epoch 20/60, Training Loss: 0.1261, Validation Loss: 0.1242\n",
      "[Trial 100] Epoch 2/60, Training Loss: 0.1272, Validation Loss: 0.1313\n",
      "[Trial 93] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 98] Epoch 6/60, Training Loss: 0.1276, Validation Loss: 0.1350\n",
      "[Trial 99] Epoch 5/60, Training Loss: 0.1392, Validation Loss: 0.1725\n",
      "[Trial 92] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1246\n",
      "[Trial 100] Epoch 3/60, Training Loss: 0.1264, Validation Loss: 0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:34:50,162] Trial 94 finished with value: 0.12466346882283688 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.003020732985900784, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 94] Epoch 29/60, Training Loss: 0.1259, Validation Loss: 0.1247\n",
      "[Trial 94] Early stopping after 29 epochs.\n",
      "[Trial 95] Epoch 21/60, Training Loss: 0.1262, Validation Loss: 0.1229\n",
      "[Trial 99] Epoch 6/60, Training Loss: 0.1360, Validation Loss: 0.1634\n",
      "[Trial 101] Epoch 1/60, Training Loss: 7.4392, Validation Loss: 0.2723\n",
      "[Trial 100] Epoch 4/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 93] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 98] Epoch 7/60, Training Loss: 0.1268, Validation Loss: 0.1324\n",
      "[Trial 92] Epoch 29/60, Training Loss: 0.1258, Validation Loss: 0.1267\n",
      "[Trial 101] Epoch 2/60, Training Loss: 0.1774, Validation Loss: 0.2205\n",
      "[Trial 99] Epoch 7/60, Training Loss: 0.1335, Validation Loss: 0.1556\n",
      "[Trial 100] Epoch 5/60, Training Loss: 0.1253, Validation Loss: 0.1268\n",
      "[Trial 95] Epoch 22/60, Training Loss: 0.1262, Validation Loss: 0.1230\n",
      "[Trial 101] Epoch 3/60, Training Loss: 0.1509, Validation Loss: 0.1932\n",
      "[Trial 99] Epoch 8/60, Training Loss: 0.1321, Validation Loss: 0.1495\n",
      "[Trial 98] Epoch 8/60, Training Loss: 0.1269, Validation Loss: 0.1294\n",
      "[Trial 93] Epoch 29/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 100] Epoch 6/60, Training Loss: 0.1253, Validation Loss: 0.1267\n",
      "[Trial 92] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1244\n",
      "[Trial 101] Epoch 4/60, Training Loss: 0.1408, Validation Loss: 0.1752\n",
      "[Trial 95] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1229\n",
      "[Trial 99] Epoch 9/60, Training Loss: 0.1310, Validation Loss: 0.1449\n",
      "[Trial 100] Epoch 7/60, Training Loss: 0.1254, Validation Loss: 0.1269\n",
      "[Trial 101] Epoch 5/60, Training Loss: 0.1359, Validation Loss: 0.1633\n",
      "[Trial 98] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1283\n",
      "[Trial 93] Epoch 30/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 99] Epoch 10/60, Training Loss: 0.1300, Validation Loss: 0.1417\n",
      "[Trial 100] Epoch 8/60, Training Loss: 0.1252, Validation Loss: 0.1270\n",
      "[Trial 92] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1244\n",
      "[Trial 101] Epoch 6/60, Training Loss: 0.1333, Validation Loss: 0.1544\n",
      "[Trial 95] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1229\n",
      "[Trial 100] Epoch 9/60, Training Loss: 0.1254, Validation Loss: 0.1273\n",
      "[Trial 99] Epoch 11/60, Training Loss: 0.1291, Validation Loss: 0.1384\n",
      "[Trial 101] Epoch 7/60, Training Loss: 0.1316, Validation Loss: 0.1472\n",
      "[Trial 98] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1282\n",
      "[Trial 93] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 92] Epoch 32/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 100] Epoch 10/60, Training Loss: 0.1253, Validation Loss: 0.1268\n",
      "[Trial 99] Epoch 12/60, Training Loss: 0.1285, Validation Loss: 0.1359\n",
      "[Trial 101] Epoch 8/60, Training Loss: 0.1304, Validation Loss: 0.1411\n",
      "[Trial 95] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1229\n",
      "[Trial 98] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1281\n",
      "[Trial 100] Epoch 11/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 99] Epoch 13/60, Training Loss: 0.1280, Validation Loss: 0.1339\n",
      "[Trial 101] Epoch 9/60, Training Loss: 0.1295, Validation Loss: 0.1370\n",
      "[Trial 93] Epoch 32/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 92] Epoch 33/60, Training Loss: 0.1255, Validation Loss: 0.1244\n",
      "[Trial 95] Epoch 26/60, Training Loss: 0.1260, Validation Loss: 0.1227\n",
      "[Trial 101] Epoch 10/60, Training Loss: 0.1290, Validation Loss: 0.1355\n",
      "[Trial 100] Epoch 12/60, Training Loss: 0.1251, Validation Loss: 0.1270\n",
      "[Trial 99] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1328\n",
      "[Trial 98] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1274\n",
      "[Trial 93] Epoch 33/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 101] Epoch 11/60, Training Loss: 0.1286, Validation Loss: 0.1325\n",
      "[Trial 100] Epoch 13/60, Training Loss: 0.1251, Validation Loss: 0.1267\n",
      "[Trial 99] Epoch 15/60, Training Loss: 0.1272, Validation Loss: 0.1313\n",
      "[Trial 92] Epoch 34/60, Training Loss: 0.1256, Validation Loss: 0.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:36:47,121] Trial 95 finished with value: 0.12271861433982849 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.003012692860035813, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 95] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1228\n",
      "[Trial 95] Early stopping after 27 epochs.\n",
      "[Trial 101] Epoch 12/60, Training Loss: 0.1280, Validation Loss: 0.1314\n",
      "[Trial 100] Epoch 14/60, Training Loss: 0.1251, Validation Loss: 0.1267\n",
      "[Trial 99] Epoch 16/60, Training Loss: 0.1272, Validation Loss: 0.1309\n",
      "[Trial 98] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1266\n",
      "[Trial 93] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 101] Epoch 13/60, Training Loss: 0.1277, Validation Loss: 0.1294\n",
      "[Trial 100] Epoch 15/60, Training Loss: 0.1252, Validation Loss: 0.1267\n",
      "[Trial 99] Epoch 17/60, Training Loss: 0.1269, Validation Loss: 0.1301\n",
      "[Trial 92] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1244\n",
      "[Trial 102] Epoch 1/60, Training Loss: 1.1591, Validation Loss: 0.2212\n",
      "[Trial 101] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1278\n",
      "[Trial 100] Epoch 16/60, Training Loss: 0.1250, Validation Loss: 0.1269\n",
      "[Trial 98] Epoch 14/60, Training Loss: 0.1262, Validation Loss: 0.1261\n",
      "[Trial 99] Epoch 18/60, Training Loss: 0.1267, Validation Loss: 0.1299\n",
      "[Trial 93] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 101] Epoch 15/60, Training Loss: 0.1273, Validation Loss: 0.1279\n",
      "[Trial 102] Epoch 2/60, Training Loss: 0.1384, Validation Loss: 0.1883\n",
      "[Trial 92] Epoch 36/60, Training Loss: 0.1259, Validation Loss: 0.1244\n",
      "[Trial 100] Epoch 17/60, Training Loss: 0.1252, Validation Loss: 0.1267\n",
      "[Trial 99] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1292\n",
      "[Trial 101] Epoch 16/60, Training Loss: 0.1270, Validation Loss: 0.1268\n",
      "[Trial 98] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:37:34,836] Trial 93 finished with value: 0.12505912706255912 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0030850854568223807, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 93] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 93] Early stopping after 36 epochs.\n",
      "[Trial 100] Epoch 18/60, Training Loss: 0.1251, Validation Loss: 0.1267\n",
      "[Trial 99] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1290\n",
      "[Trial 101] Epoch 17/60, Training Loss: 0.1269, Validation Loss: 0.1266\n",
      "[Trial 102] Epoch 3/60, Training Loss: 0.1312, Validation Loss: 0.1668\n",
      "[Trial 92] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:37:47,195] Trial 100 finished with value: 0.1266431892911593 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.014515076275551654, 'batch_size': 32, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 100] Epoch 19/60, Training Loss: 0.1250, Validation Loss: 0.1267\n",
      "[Trial 100] Early stopping after 19 epochs.\n",
      "[Trial 101] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1261\n",
      "[Trial 99] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1284\n",
      "[Trial 98] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1264\n",
      "[Trial 103] Epoch 1/60, Training Loss: 1.6966, Validation Loss: 0.2554\n",
      "[Trial 101] Epoch 19/60, Training Loss: 0.1269, Validation Loss: 0.1252\n",
      "[Trial 99] Epoch 22/60, Training Loss: 0.1261, Validation Loss: 0.1286\n",
      "[Trial 102] Epoch 4/60, Training Loss: 0.1289, Validation Loss: 0.1552\n",
      "[Trial 92] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 104] Epoch 1/60, Training Loss: 1.5400, Validation Loss: 0.2562\n",
      "[Trial 101] Epoch 20/60, Training Loss: 0.1266, Validation Loss: 0.1263\n",
      "[Trial 98] Epoch 17/60, Training Loss: 0.1272, Validation Loss: 0.1260\n",
      "[Trial 99] Epoch 23/60, Training Loss: 0.1260, Validation Loss: 0.1281\n",
      "[Trial 103] Epoch 2/60, Training Loss: 0.1392, Validation Loss: 0.2393\n",
      "[Trial 101] Epoch 21/60, Training Loss: 0.1267, Validation Loss: 0.1257\n",
      "[Trial 102] Epoch 5/60, Training Loss: 0.1277, Validation Loss: 0.1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:38:21,685] Trial 92 finished with value: 0.12440035901963711 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0016291027512334367, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 92] Epoch 39/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 92] Early stopping after 39 epochs.\n",
      "[Trial 99] Epoch 24/60, Training Loss: 0.1260, Validation Loss: 0.1280\n",
      "[Trial 104] Epoch 2/60, Training Loss: 0.1387, Validation Loss: 0.2367\n",
      "[Trial 101] Epoch 22/60, Training Loss: 0.1265, Validation Loss: 0.1249\n",
      "[Trial 98] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1259\n",
      "[Trial 105] Epoch 1/60, Training Loss: 9.6445, Validation Loss: 0.2997\n",
      "[Trial 99] Epoch 25/60, Training Loss: 0.1260, Validation Loss: 0.1279\n",
      "[Trial 103] Epoch 3/60, Training Loss: 0.1330, Validation Loss: 0.2294\n",
      "[Trial 105] Epoch 2/60, Training Loss: 0.1567, Validation Loss: 0.2485\n",
      "[Trial 101] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1249\n",
      "[Trial 102] Epoch 6/60, Training Loss: 0.1271, Validation Loss: 0.1412\n",
      "[Trial 99] Epoch 26/60, Training Loss: 0.1260, Validation Loss: 0.1275\n",
      "[Trial 105] Epoch 3/60, Training Loss: 0.1417, Validation Loss: 0.2286\n",
      "[Trial 104] Epoch 3/60, Training Loss: 0.1321, Validation Loss: 0.2231\n",
      "[Trial 98] Epoch 19/60, Training Loss: 0.1261, Validation Loss: 0.1266\n",
      "[Trial 101] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 105] Epoch 4/60, Training Loss: 0.1364, Validation Loss: 0.2215\n",
      "[Trial 103] Epoch 4/60, Training Loss: 0.1309, Validation Loss: 0.2208\n",
      "[Trial 99] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1276\n",
      "[Trial 102] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1359\n",
      "[Trial 101] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1247\n",
      "[Trial 105] Epoch 5/60, Training Loss: 0.1343, Validation Loss: 0.2099\n",
      "[Trial 98] Epoch 20/60, Training Loss: 0.1262, Validation Loss: 0.1266\n",
      "[Trial 99] Epoch 28/60, Training Loss: 0.1258, Validation Loss: 0.1273\n",
      "[Trial 104] Epoch 4/60, Training Loss: 0.1298, Validation Loss: 0.2130\n",
      "[Trial 105] Epoch 6/60, Training Loss: 0.1326, Validation Loss: 0.2038\n",
      "[Trial 101] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1249\n",
      "[Trial 103] Epoch 5/60, Training Loss: 0.1296, Validation Loss: 0.2103\n",
      "[Trial 105] Epoch 7/60, Training Loss: 0.1314, Validation Loss: 0.1953\n",
      "[Trial 102] Epoch 8/60, Training Loss: 0.1261, Validation Loss: 0.1341\n",
      "[Trial 101] Epoch 27/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 99] Epoch 29/60, Training Loss: 0.1256, Validation Loss: 0.1280\n",
      "[Trial 105] Epoch 8/60, Training Loss: 0.1310, Validation Loss: 0.1847\n",
      "[Trial 98] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 104] Epoch 5/60, Training Loss: 0.1286, Validation Loss: 0.2041\n",
      "[Trial 101] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1248\n",
      "[Trial 99] Epoch 30/60, Training Loss: 0.1261, Validation Loss: 0.1275\n",
      "[Trial 105] Epoch 9/60, Training Loss: 0.1302, Validation Loss: 0.1812\n",
      "[Trial 102] Epoch 9/60, Training Loss: 0.1259, Validation Loss: 0.1327\n",
      "[Trial 103] Epoch 6/60, Training Loss: 0.1293, Validation Loss: 0.2044\n",
      "[Trial 105] Epoch 10/60, Training Loss: 0.1319, Validation Loss: 0.1785\n",
      "[Trial 101] Epoch 29/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 99] Epoch 31/60, Training Loss: 0.1259, Validation Loss: 0.1272\n",
      "[Trial 98] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1265\n",
      "[Trial 105] Epoch 11/60, Training Loss: 0.1296, Validation Loss: 0.1722\n",
      "[Trial 104] Epoch 6/60, Training Loss: 0.1284, Validation Loss: 0.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:39:47,981] Trial 101 finished with value: 0.1245200681189696 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0007930025832783112, 'batch_size': 32, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 101] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1246\n",
      "[Trial 101] Early stopping after 30 epochs.\n",
      "[Trial 99] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1272\n",
      "[Trial 102] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1322\n",
      "[Trial 105] Epoch 12/60, Training Loss: 0.1335, Validation Loss: 0.1693\n",
      "[Trial 106] Epoch 1/60, Training Loss: 9.5364, Validation Loss: 0.2583\n",
      "[Trial 103] Epoch 7/60, Training Loss: 0.1287, Validation Loss: 0.1977\n",
      "[Trial 105] Epoch 13/60, Training Loss: 0.1284, Validation Loss: 0.1638\n",
      "[Trial 98] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1261\n",
      "[Trial 106] Epoch 2/60, Training Loss: 0.1358, Validation Loss: 0.2257\n",
      "[Trial 99] Epoch 33/60, Training Loss: 0.1257, Validation Loss: 0.1272\n",
      "[Trial 104] Epoch 7/60, Training Loss: 0.1274, Validation Loss: 0.1901\n",
      "[Trial 106] Epoch 3/60, Training Loss: 0.1298, Validation Loss: 0.2120\n",
      "[Trial 105] Epoch 14/60, Training Loss: 0.1284, Validation Loss: 0.1599\n",
      "[Trial 102] Epoch 11/60, Training Loss: 0.1255, Validation Loss: 0.1313\n",
      "[Trial 99] Epoch 34/60, Training Loss: 0.1261, Validation Loss: 0.1275\n",
      "[Trial 106] Epoch 4/60, Training Loss: 0.1289, Validation Loss: 0.2004\n",
      "[Trial 103] Epoch 8/60, Training Loss: 0.1283, Validation Loss: 0.1910\n",
      "[Trial 105] Epoch 15/60, Training Loss: 0.1306, Validation Loss: 0.1570\n",
      "[Trial 98] Epoch 24/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 106] Epoch 5/60, Training Loss: 0.1287, Validation Loss: 0.1905\n",
      "[Trial 105] Epoch 16/60, Training Loss: 0.1282, Validation Loss: 0.1538\n",
      "[Trial 99] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1271\n",
      "[Trial 104] Epoch 8/60, Training Loss: 0.1272, Validation Loss: 0.1843\n",
      "[Trial 106] Epoch 6/60, Training Loss: 0.1290, Validation Loss: 0.1840\n",
      "[Trial 102] Epoch 12/60, Training Loss: 0.1254, Validation Loss: 0.1309\n",
      "[Trial 105] Epoch 17/60, Training Loss: 0.1325, Validation Loss: 0.1514\n",
      "[Trial 106] Epoch 7/60, Training Loss: 0.1281, Validation Loss: 0.1742\n",
      "[Trial 103] Epoch 9/60, Training Loss: 0.1280, Validation Loss: 0.1867\n",
      "[Trial 99] Epoch 36/60, Training Loss: 0.1257, Validation Loss: 0.1273\n",
      "[Trial 98] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 105] Epoch 18/60, Training Loss: 0.1277, Validation Loss: 0.1472\n",
      "[Trial 106] Epoch 8/60, Training Loss: 0.1284, Validation Loss: 0.1668\n",
      "[Trial 102] Epoch 13/60, Training Loss: 0.1255, Validation Loss: 0.1300\n",
      "[Trial 105] Epoch 19/60, Training Loss: 0.1286, Validation Loss: 0.1453\n",
      "[Trial 104] Epoch 9/60, Training Loss: 0.1283, Validation Loss: 0.1789\n",
      "[Trial 99] Epoch 37/60, Training Loss: 0.1260, Validation Loss: 0.1273\n",
      "[Trial 106] Epoch 9/60, Training Loss: 0.1279, Validation Loss: 0.1590\n",
      "[Trial 105] Epoch 20/60, Training Loss: 0.1282, Validation Loss: 0.1435\n",
      "[Trial 103] Epoch 10/60, Training Loss: 0.1281, Validation Loss: 0.1805\n",
      "[Trial 106] Epoch 10/60, Training Loss: 0.1278, Validation Loss: 0.1524\n",
      "[Trial 98] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 99] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1271\n",
      "[Trial 105] Epoch 21/60, Training Loss: 0.1326, Validation Loss: 0.1407\n",
      "[Trial 106] Epoch 11/60, Training Loss: 0.1281, Validation Loss: 0.1463\n",
      "[Trial 102] Epoch 14/60, Training Loss: 0.1254, Validation Loss: 0.1305\n",
      "[Trial 104] Epoch 10/60, Training Loss: 0.1269, Validation Loss: 0.1757\n",
      "[Trial 106] Epoch 12/60, Training Loss: 0.1278, Validation Loss: 0.1410\n",
      "[Trial 105] Epoch 22/60, Training Loss: 0.1283, Validation Loss: 0.1383\n",
      "[Trial 99] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1272\n",
      "[Trial 98] Epoch 27/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 103] Epoch 11/60, Training Loss: 0.1276, Validation Loss: 0.1768\n",
      "[Trial 106] Epoch 13/60, Training Loss: 0.1269, Validation Loss: 0.1368\n",
      "[Trial 105] Epoch 23/60, Training Loss: 0.1321, Validation Loss: 0.1376\n",
      "[Trial 99] Epoch 40/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 106] Epoch 14/60, Training Loss: 0.1271, Validation Loss: 0.1339\n",
      "[Trial 102] Epoch 15/60, Training Loss: 0.1252, Validation Loss: 0.1306\n",
      "[Trial 105] Epoch 24/60, Training Loss: 0.1275, Validation Loss: 0.1349\n",
      "[Trial 104] Epoch 11/60, Training Loss: 0.1265, Validation Loss: 0.1736\n",
      "[Trial 106] Epoch 15/60, Training Loss: 0.1272, Validation Loss: 0.1308\n",
      "[Trial 98] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 105] Epoch 25/60, Training Loss: 0.1273, Validation Loss: 0.1331\n",
      "[Trial 99] Epoch 41/60, Training Loss: 0.1256, Validation Loss: 0.1272\n",
      "[Trial 103] Epoch 12/60, Training Loss: 0.1292, Validation Loss: 0.1705\n",
      "[Trial 106] Epoch 16/60, Training Loss: 0.1276, Validation Loss: 0.1288\n",
      "[Trial 105] Epoch 26/60, Training Loss: 0.1270, Validation Loss: 0.1310\n",
      "[Trial 102] Epoch 16/60, Training Loss: 0.1254, Validation Loss: 0.1295\n",
      "[Trial 106] Epoch 17/60, Training Loss: 0.1263, Validation Loss: 0.1274\n",
      "[Trial 99] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1269\n",
      "[Trial 104] Epoch 12/60, Training Loss: 0.1289, Validation Loss: 0.1654\n",
      "[Trial 105] Epoch 27/60, Training Loss: 0.1275, Validation Loss: 0.1295\n",
      "[Trial 106] Epoch 18/60, Training Loss: 0.1262, Validation Loss: 0.1267\n",
      "[Trial 98] Epoch 29/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 99] Epoch 43/60, Training Loss: 0.1259, Validation Loss: 0.1272\n",
      "[Trial 103] Epoch 13/60, Training Loss: 0.1274, Validation Loss: 0.1703\n",
      "[Trial 105] Epoch 28/60, Training Loss: 0.1290, Validation Loss: 0.1322\n",
      "[Trial 106] Epoch 19/60, Training Loss: 0.1262, Validation Loss: 0.1262\n",
      "[Trial 102] Epoch 17/60, Training Loss: 0.1257, Validation Loss: 0.1294\n",
      "[Trial 105] Epoch 29/60, Training Loss: 0.1332, Validation Loss: 0.1277\n",
      "[Trial 106] Epoch 20/60, Training Loss: 0.1287, Validation Loss: 0.1262\n",
      "[Trial 99] Epoch 44/60, Training Loss: 0.1261, Validation Loss: 0.1275\n",
      "[Trial 104] Epoch 13/60, Training Loss: 0.1267, Validation Loss: 0.1616\n",
      "[Trial 98] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1256\n",
      "[Trial 105] Epoch 30/60, Training Loss: 0.1268, Validation Loss: 0.1272\n",
      "[Trial 106] Epoch 21/60, Training Loss: 0.1261, Validation Loss: 0.1259\n",
      "[Trial 103] Epoch 14/60, Training Loss: 0.1272, Validation Loss: 0.1693\n",
      "[Trial 99] Epoch 45/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 106] Epoch 22/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 102] Epoch 18/60, Training Loss: 0.1251, Validation Loss: 0.1300\n",
      "[Trial 105] Epoch 31/60, Training Loss: 0.1270, Validation Loss: 0.1259\n",
      "[Trial 106] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1258\n",
      "[Trial 105] Epoch 32/60, Training Loss: 0.1268, Validation Loss: 0.1254\n",
      "[Trial 98] Epoch 31/60, Training Loss: 0.1257, Validation Loss: 0.1259\n",
      "[Trial 99] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1269\n",
      "[Trial 104] Epoch 14/60, Training Loss: 0.1267, Validation Loss: 0.1595\n",
      "[Trial 106] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 105] Epoch 33/60, Training Loss: 0.1268, Validation Loss: 0.1251\n",
      "[Trial 103] Epoch 15/60, Training Loss: 0.1271, Validation Loss: 0.1634\n",
      "[Trial 102] Epoch 19/60, Training Loss: 0.1254, Validation Loss: 0.1301\n",
      "[Trial 106] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1259\n",
      "[Trial 99] Epoch 47/60, Training Loss: 0.1255, Validation Loss: 0.1271\n",
      "[Trial 105] Epoch 34/60, Training Loss: 0.1268, Validation Loss: 0.1245\n",
      "[Trial 106] Epoch 26/60, Training Loss: 0.1261, Validation Loss: 0.1258\n",
      "[Trial 98] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 104] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1577\n",
      "[Trial 105] Epoch 35/60, Training Loss: 0.1280, Validation Loss: 0.1242\n",
      "[Trial 99] Epoch 48/60, Training Loss: 0.1266, Validation Loss: 0.1269\n",
      "[Trial 106] Epoch 27/60, Training Loss: 0.1281, Validation Loss: 0.1260\n",
      "[Trial 102] Epoch 20/60, Training Loss: 0.1259, Validation Loss: 0.1299\n",
      "[Trial 103] Epoch 16/60, Training Loss: 0.1280, Validation Loss: 0.1600\n",
      "[Trial 105] Epoch 36/60, Training Loss: 0.3910, Validation Loss: 0.1292\n",
      "[Trial 106] Epoch 28/60, Training Loss: 0.1259, Validation Loss: 0.1260\n",
      "[Trial 99] Epoch 49/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 105] Epoch 37/60, Training Loss: 0.1278, Validation Loss: 0.1238\n",
      "[Trial 98] Epoch 33/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 106] Epoch 29/60, Training Loss: 0.1256, Validation Loss: 0.1259\n",
      "[Trial 104] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1554\n",
      "[Trial 105] Epoch 38/60, Training Loss: 0.1267, Validation Loss: 0.1234\n",
      "[Trial 106] Epoch 30/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 99] Epoch 50/60, Training Loss: 0.1259, Validation Loss: 0.1292\n",
      "[Trial 102] Epoch 21/60, Training Loss: 0.1252, Validation Loss: 0.1295\n",
      "[Trial 103] Epoch 17/60, Training Loss: 0.1275, Validation Loss: 0.1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:43:23,634] Trial 106 finished with value: 0.12579966286818187 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.007240491126196595, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 106] Epoch 31/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 106] Early stopping after 31 epochs.\n",
      "[Trial 105] Epoch 39/60, Training Loss: 0.1266, Validation Loss: 0.1232\n",
      "[Trial 98] Epoch 34/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 99] Epoch 51/60, Training Loss: 0.1254, Validation Loss: 0.1268\n",
      "[Trial 105] Epoch 40/60, Training Loss: 0.1266, Validation Loss: 0.1230\n",
      "[Trial 104] Epoch 17/60, Training Loss: 0.1314, Validation Loss: 0.1504\n",
      "[Trial 102] Epoch 22/60, Training Loss: 0.1250, Validation Loss: 0.1316\n",
      "[Trial 99] Epoch 52/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 105] Epoch 41/60, Training Loss: 0.1265, Validation Loss: 0.1228\n",
      "[Trial 103] Epoch 18/60, Training Loss: 0.1287, Validation Loss: 0.1534\n",
      "[Trial 107] Epoch 1/60, Training Loss: 0.4817, Validation Loss: 0.1453\n",
      "[Trial 98] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 105] Epoch 42/60, Training Loss: 0.1265, Validation Loss: 0.1227\n",
      "[Trial 99] Epoch 53/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 104] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1486\n",
      "[Trial 105] Epoch 43/60, Training Loss: 0.1265, Validation Loss: 0.1228\n",
      "[Trial 102] Epoch 23/60, Training Loss: 0.1249, Validation Loss: 0.1288\n",
      "[Trial 107] Epoch 2/60, Training Loss: 0.1271, Validation Loss: 0.1350\n",
      "[Trial 103] Epoch 19/60, Training Loss: 0.1269, Validation Loss: 0.1513\n",
      "[Trial 99] Epoch 54/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 105] Epoch 44/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 98] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 105] Epoch 45/60, Training Loss: 0.1266, Validation Loss: 0.1227\n",
      "[Trial 99] Epoch 55/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 102] Epoch 24/60, Training Loss: 0.1249, Validation Loss: 0.1290\n",
      "[Trial 104] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1466\n",
      "[Trial 107] Epoch 3/60, Training Loss: 0.1263, Validation Loss: 0.1300\n",
      "[Trial 105] Epoch 46/60, Training Loss: 0.1264, Validation Loss: 0.1227\n",
      "[Trial 103] Epoch 20/60, Training Loss: 0.1272, Validation Loss: 0.1500\n",
      "[Trial 98] Epoch 37/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 99] Epoch 56/60, Training Loss: 0.1251, Validation Loss: 0.1269\n",
      "[Trial 105] Epoch 47/60, Training Loss: 0.1264, Validation Loss: 0.1227\n",
      "[Trial 107] Epoch 4/60, Training Loss: 0.1258, Validation Loss: 0.1298\n",
      "[Trial 102] Epoch 25/60, Training Loss: 0.1248, Validation Loss: 0.1291\n",
      "[Trial 105] Epoch 48/60, Training Loss: 0.1266, Validation Loss: 0.1227\n",
      "[Trial 104] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1460\n",
      "[Trial 99] Epoch 57/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 105] Epoch 49/60, Training Loss: 0.1265, Validation Loss: 0.1227\n",
      "[Trial 103] Epoch 21/60, Training Loss: 0.1271, Validation Loss: 0.1557\n",
      "[Trial 98] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 99] Epoch 58/60, Training Loss: 0.1250, Validation Loss: 0.1268\n",
      "[Trial 105] Epoch 50/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 107] Epoch 5/60, Training Loss: 0.1256, Validation Loss: 0.1292\n",
      "[Trial 102] Epoch 26/60, Training Loss: 0.1248, Validation Loss: 0.1289\n",
      "[Trial 104] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1440\n",
      "[Trial 105] Epoch 51/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 99] Epoch 59/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 98] Epoch 39/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 103] Epoch 22/60, Training Loss: 0.1309, Validation Loss: 0.1446\n",
      "[Trial 105] Epoch 52/60, Training Loss: 0.1262, Validation Loss: 0.1226\n",
      "[Trial 107] Epoch 6/60, Training Loss: 0.1255, Validation Loss: 0.1295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:45:09,130] Trial 99 finished with value: 0.12679636354247728 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.000665446486576085, 'batch_size': 32, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 99] Epoch 60/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 102] Epoch 27/60, Training Loss: 0.1247, Validation Loss: 0.1290\n",
      "[Trial 105] Epoch 53/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 104] Epoch 22/60, Training Loss: 0.1260, Validation Loss: 0.1406\n",
      "[Trial 105] Epoch 54/60, Training Loss: 0.1262, Validation Loss: 0.1228\n",
      "[Trial 98] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1255\n",
      "[Trial 103] Epoch 23/60, Training Loss: 0.1267, Validation Loss: 0.1433\n",
      "[Trial 107] Epoch 7/60, Training Loss: 0.1254, Validation Loss: 0.1292\n",
      "[Trial 105] Epoch 55/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 102] Epoch 28/60, Training Loss: 0.1245, Validation Loss: 0.1288\n",
      "[Trial 108] Epoch 1/60, Training Loss: 14.5590, Validation Loss: 0.1785\n",
      "[Trial 105] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 104] Epoch 23/60, Training Loss: 0.1262, Validation Loss: 0.1394\n",
      "[Trial 98] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 107] Epoch 8/60, Training Loss: 0.1251, Validation Loss: 0.1290\n",
      "[Trial 103] Epoch 24/60, Training Loss: 0.1269, Validation Loss: 0.1429\n",
      "[Trial 105] Epoch 57/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 102] Epoch 29/60, Training Loss: 0.1249, Validation Loss: 0.1289\n",
      "[Trial 105] Epoch 58/60, Training Loss: 0.1261, Validation Loss: 0.1227\n",
      "[Trial 104] Epoch 24/60, Training Loss: 0.1267, Validation Loss: 0.1613\n",
      "[Trial 98] Epoch 42/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 108] Epoch 2/60, Training Loss: 0.1317, Validation Loss: 0.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:45:58,260] Trial 105 finished with value: 0.12263844807942709 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0039849118387808, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 105] Epoch 59/60, Training Loss: 0.1261, Validation Loss: 0.1228\n",
      "[Trial 105] Early stopping after 59 epochs.\n",
      "[Trial 107] Epoch 9/60, Training Loss: 0.1250, Validation Loss: 0.1290\n",
      "[Trial 103] Epoch 25/60, Training Loss: 0.1267, Validation Loss: 0.1404\n",
      "[Trial 102] Epoch 30/60, Training Loss: 0.1247, Validation Loss: 0.1287\n",
      "[Trial 109] Epoch 1/60, Training Loss: 10.2092, Validation Loss: 0.2664\n",
      "[Trial 109] Epoch 2/60, Training Loss: 0.1412, Validation Loss: 0.2252\n",
      "[Trial 98] Epoch 43/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 104] Epoch 25/60, Training Loss: 0.1293, Validation Loss: 0.1367\n",
      "[Trial 107] Epoch 10/60, Training Loss: 0.1246, Validation Loss: 0.1289\n",
      "[Trial 108] Epoch 3/60, Training Loss: 0.1293, Validation Loss: 0.1664\n",
      "[Trial 109] Epoch 3/60, Training Loss: 0.1330, Validation Loss: 0.2129\n",
      "[Trial 103] Epoch 26/60, Training Loss: 0.1279, Validation Loss: 0.1404\n",
      "[Trial 102] Epoch 31/60, Training Loss: 0.1248, Validation Loss: 0.1289\n",
      "[Trial 109] Epoch 4/60, Training Loss: 0.1312, Validation Loss: 0.2013\n",
      "[Trial 107] Epoch 11/60, Training Loss: 0.1250, Validation Loss: 0.1287\n",
      "[Trial 98] Epoch 44/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 104] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1356\n",
      "[Trial 109] Epoch 5/60, Training Loss: 0.1305, Validation Loss: 0.1949\n",
      "[Trial 102] Epoch 32/60, Training Loss: 0.1246, Validation Loss: 0.1288\n",
      "[Trial 103] Epoch 27/60, Training Loss: 0.1269, Validation Loss: 0.1378\n",
      "[Trial 109] Epoch 6/60, Training Loss: 0.1295, Validation Loss: 0.1864\n",
      "[Trial 108] Epoch 4/60, Training Loss: 0.1277, Validation Loss: 0.1569\n",
      "[Trial 107] Epoch 12/60, Training Loss: 0.1248, Validation Loss: 0.1290\n",
      "[Trial 109] Epoch 7/60, Training Loss: 0.1298, Validation Loss: 0.1820\n",
      "[Trial 98] Epoch 45/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 104] Epoch 27/60, Training Loss: 0.1255, Validation Loss: 0.1352\n",
      "[Trial 109] Epoch 8/60, Training Loss: 0.1295, Validation Loss: 0.1756\n",
      "[Trial 102] Epoch 33/60, Training Loss: 0.1247, Validation Loss: 0.1289\n",
      "[Trial 103] Epoch 28/60, Training Loss: 0.1269, Validation Loss: 0.1373\n",
      "[Trial 109] Epoch 9/60, Training Loss: 0.1284, Validation Loss: 0.1690\n",
      "[Trial 107] Epoch 13/60, Training Loss: 0.1247, Validation Loss: 0.1289\n",
      "[Trial 108] Epoch 5/60, Training Loss: 0.1269, Validation Loss: 0.1582\n",
      "[Trial 98] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 10/60, Training Loss: 0.1292, Validation Loss: 0.1692\n",
      "[Trial 104] Epoch 28/60, Training Loss: 0.1256, Validation Loss: 0.1346\n",
      "[Trial 102] Epoch 34/60, Training Loss: 0.1246, Validation Loss: 0.1289\n",
      "[Trial 109] Epoch 11/60, Training Loss: 0.1291, Validation Loss: 0.1616\n",
      "[Trial 103] Epoch 29/60, Training Loss: 0.1268, Validation Loss: 0.1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:47:25,492] Trial 107 finished with value: 0.128713483735919 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.004154095662844252, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 107] Epoch 14/60, Training Loss: 0.1247, Validation Loss: 0.1288\n",
      "[Trial 107] Early stopping after 14 epochs.\n",
      "[Trial 98] Epoch 47/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 12/60, Training Loss: 0.1291, Validation Loss: 0.1573\n",
      "[Trial 110] Epoch 1/60, Training Loss: 15.7249, Validation Loss: 0.2468\n",
      "[Trial 108] Epoch 6/60, Training Loss: 0.1285, Validation Loss: 0.1378\n",
      "[Trial 104] Epoch 29/60, Training Loss: 0.1261, Validation Loss: 0.1340\n",
      "[Trial 109] Epoch 13/60, Training Loss: 0.1284, Validation Loss: 0.1531\n",
      "[Trial 102] Epoch 35/60, Training Loss: 0.1246, Validation Loss: 0.1290\n",
      "[Trial 110] Epoch 2/60, Training Loss: 0.1335, Validation Loss: 0.2093\n",
      "[Trial 103] Epoch 30/60, Training Loss: 0.1265, Validation Loss: 0.1396\n",
      "[Trial 110] Epoch 3/60, Training Loss: 0.1290, Validation Loss: 0.1932\n",
      "[Trial 109] Epoch 14/60, Training Loss: 0.1272, Validation Loss: 0.1504\n",
      "[Trial 98] Epoch 48/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 110] Epoch 4/60, Training Loss: 0.1281, Validation Loss: 0.1804\n",
      "[Trial 109] Epoch 15/60, Training Loss: 0.1289, Validation Loss: 0.1470\n",
      "[Trial 102] Epoch 36/60, Training Loss: 0.1245, Validation Loss: 0.1288\n",
      "[Trial 104] Epoch 30/60, Training Loss: 0.1254, Validation Loss: 0.1332\n",
      "[Trial 110] Epoch 5/60, Training Loss: 0.1286, Validation Loss: 0.1674\n",
      "[Trial 108] Epoch 7/60, Training Loss: 0.1260, Validation Loss: 0.1338\n",
      "[Trial 109] Epoch 16/60, Training Loss: 0.1273, Validation Loss: 0.1445\n",
      "[Trial 103] Epoch 31/60, Training Loss: 0.1274, Validation Loss: 0.1332\n",
      "[Trial 110] Epoch 6/60, Training Loss: 0.1283, Validation Loss: 0.1558\n",
      "[Trial 98] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 17/60, Training Loss: 0.1272, Validation Loss: 0.1413\n",
      "[Trial 110] Epoch 7/60, Training Loss: 0.1277, Validation Loss: 0.1470\n",
      "[Trial 102] Epoch 37/60, Training Loss: 0.1246, Validation Loss: 0.1288\n",
      "[Trial 109] Epoch 18/60, Training Loss: 0.1273, Validation Loss: 0.1391\n",
      "[Trial 104] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1320\n",
      "[Trial 110] Epoch 8/60, Training Loss: 0.1275, Validation Loss: 0.1405\n",
      "[Trial 108] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1312\n",
      "[Trial 109] Epoch 19/60, Training Loss: 0.1298, Validation Loss: 0.1372\n",
      "[Trial 103] Epoch 32/60, Training Loss: 0.1270, Validation Loss: 0.1331\n",
      "[Trial 98] Epoch 50/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 110] Epoch 9/60, Training Loss: 0.1280, Validation Loss: 0.1352\n",
      "[Trial 109] Epoch 20/60, Training Loss: 0.1281, Validation Loss: 0.1342\n",
      "[Trial 110] Epoch 10/60, Training Loss: 0.1272, Validation Loss: 0.1317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:48:36,192] Trial 102 finished with value: 0.12874057330191135 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0007478626405491125, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 102] Epoch 38/60, Training Loss: 0.1246, Validation Loss: 0.1288\n",
      "[Trial 102] Early stopping after 38 epochs.\n",
      "[Trial 104] Epoch 32/60, Training Loss: 0.1261, Validation Loss: 0.1314\n",
      "[Trial 109] Epoch 21/60, Training Loss: 0.1270, Validation Loss: 0.1322\n",
      "[Trial 110] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.1287\n",
      "[Trial 111] Epoch 1/60, Training Loss: 10.8726, Validation Loss: 0.2875\n",
      "[Trial 98] Epoch 51/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 103] Epoch 33/60, Training Loss: 0.1266, Validation Loss: 0.1362\n",
      "[Trial 110] Epoch 12/60, Training Loss: 0.1268, Validation Loss: 0.1271\n",
      "[Trial 109] Epoch 22/60, Training Loss: 0.1265, Validation Loss: 0.1309\n",
      "[Trial 111] Epoch 2/60, Training Loss: 0.1412, Validation Loss: 0.2508\n",
      "[Trial 108] Epoch 9/60, Training Loss: 0.1258, Validation Loss: 0.1307\n",
      "[Trial 110] Epoch 13/60, Training Loss: 0.1270, Validation Loss: 0.1263\n",
      "[Trial 109] Epoch 23/60, Training Loss: 0.1266, Validation Loss: 0.1297\n",
      "[Trial 111] Epoch 3/60, Training Loss: 0.1328, Validation Loss: 0.2380\n",
      "[Trial 104] Epoch 33/60, Training Loss: 0.1257, Validation Loss: 0.1313\n",
      "[Trial 110] Epoch 14/60, Training Loss: 0.1270, Validation Loss: 0.1259\n",
      "[Trial 98] Epoch 52/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 24/60, Training Loss: 0.1264, Validation Loss: 0.1287\n",
      "[Trial 111] Epoch 4/60, Training Loss: 0.1311, Validation Loss: 0.2251\n",
      "[Trial 103] Epoch 34/60, Training Loss: 0.1284, Validation Loss: 0.1312\n",
      "[Trial 110] Epoch 15/60, Training Loss: 0.1266, Validation Loss: 0.1258\n",
      "[Trial 111] Epoch 5/60, Training Loss: 0.1298, Validation Loss: 0.2170\n",
      "[Trial 109] Epoch 25/60, Training Loss: 0.1279, Validation Loss: 0.1277\n",
      "[Trial 108] Epoch 10/60, Training Loss: 0.1269, Validation Loss: 0.8272\n",
      "[Trial 110] Epoch 16/60, Training Loss: 0.1275, Validation Loss: 0.1256\n",
      "[Trial 104] Epoch 34/60, Training Loss: 0.1261, Validation Loss: 0.1300\n",
      "[Trial 111] Epoch 6/60, Training Loss: 0.1287, Validation Loss: 0.2060\n",
      "[Trial 109] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1269\n",
      "[Trial 98] Epoch 53/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 110] Epoch 17/60, Training Loss: 0.1271, Validation Loss: 0.1256\n",
      "[Trial 111] Epoch 7/60, Training Loss: 0.1288, Validation Loss: 0.2010\n",
      "[Trial 103] Epoch 35/60, Training Loss: 0.1266, Validation Loss: 0.1305\n",
      "[Trial 109] Epoch 27/60, Training Loss: 0.1276, Validation Loss: 0.1263\n",
      "[Trial 110] Epoch 18/60, Training Loss: 0.1262, Validation Loss: 0.1256\n",
      "[Trial 111] Epoch 8/60, Training Loss: 0.1283, Validation Loss: 0.1927\n",
      "[Trial 109] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 110] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1256\n",
      "[Trial 104] Epoch 35/60, Training Loss: 0.1253, Validation Loss: 0.1325\n",
      "[Trial 108] Epoch 11/60, Training Loss: 0.1590, Validation Loss: 0.1282\n",
      "[Trial 98] Epoch 54/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 111] Epoch 9/60, Training Loss: 0.1288, Validation Loss: 0.1894\n",
      "[Trial 109] Epoch 29/60, Training Loss: 0.1281, Validation Loss: 0.1268\n",
      "[Trial 110] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1256\n",
      "[Trial 103] Epoch 36/60, Training Loss: 0.1264, Validation Loss: 0.1295\n",
      "[Trial 111] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.1816\n",
      "[Trial 109] Epoch 30/60, Training Loss: 0.1272, Validation Loss: 0.1251\n",
      "[Trial 110] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1257\n",
      "[Trial 111] Epoch 11/60, Training Loss: 0.1275, Validation Loss: 0.1751\n",
      "[Trial 98] Epoch 55/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 31/60, Training Loss: 0.1261, Validation Loss: 0.1249\n",
      "[Trial 110] Epoch 22/60, Training Loss: 0.1266, Validation Loss: 0.1256\n",
      "[Trial 104] Epoch 36/60, Training Loss: 0.1261, Validation Loss: 0.1298\n",
      "[Trial 111] Epoch 12/60, Training Loss: 0.1273, Validation Loss: 0.1707\n",
      "[Trial 108] Epoch 12/60, Training Loss: 0.1253, Validation Loss: 0.1277\n",
      "[Trial 110] Epoch 23/60, Training Loss: 0.1259, Validation Loss: 0.1256\n",
      "[Trial 109] Epoch 32/60, Training Loss: 0.1262, Validation Loss: 0.1248\n",
      "[Trial 103] Epoch 37/60, Training Loss: 0.1266, Validation Loss: 0.1295\n",
      "[Trial 111] Epoch 13/60, Training Loss: 0.1275, Validation Loss: 0.1650\n",
      "[Trial 110] Epoch 24/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 109] Epoch 33/60, Training Loss: 0.1295, Validation Loss: 0.1247\n",
      "[Trial 98] Epoch 56/60, Training Loss: 0.1252, Validation Loss: 0.1255\n",
      "[Trial 111] Epoch 14/60, Training Loss: 0.1276, Validation Loss: 0.1603\n",
      "[Trial 110] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1256\n",
      "[Trial 104] Epoch 37/60, Training Loss: 0.1258, Validation Loss: 0.1302\n",
      "[Trial 109] Epoch 34/60, Training Loss: 0.1263, Validation Loss: 0.1246\n",
      "[Trial 111] Epoch 15/60, Training Loss: 0.1271, Validation Loss: 0.1551\n",
      "[Trial 110] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 103] Epoch 38/60, Training Loss: 0.1264, Validation Loss: 0.1285\n",
      "[Trial 109] Epoch 35/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 108] Epoch 13/60, Training Loss: 0.1251, Validation Loss: 0.1275\n",
      "[Trial 111] Epoch 16/60, Training Loss: 0.1269, Validation Loss: 0.1503\n",
      "[Trial 110] Epoch 27/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 98] Epoch 57/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 36/60, Training Loss: 0.1263, Validation Loss: 0.1245\n",
      "[Trial 110] Epoch 28/60, Training Loss: 0.1256, Validation Loss: 0.1256\n",
      "[Trial 104] Epoch 38/60, Training Loss: 0.1265, Validation Loss: 0.1307\n",
      "[Trial 111] Epoch 17/60, Training Loss: 0.1268, Validation Loss: 0.1457\n",
      "[Trial 109] Epoch 37/60, Training Loss: 0.1260, Validation Loss: 0.1245\n",
      "[Trial 110] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 111] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1423\n",
      "[Trial 103] Epoch 39/60, Training Loss: 0.1263, Validation Loss: 0.1318\n",
      "[Trial 98] Epoch 58/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 38/60, Training Loss: 0.1303, Validation Loss: 0.1245\n",
      "[Trial 110] Epoch 30/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 108] Epoch 14/60, Training Loss: 0.1253, Validation Loss: 0.1274\n",
      "[Trial 111] Epoch 19/60, Training Loss: 0.1267, Validation Loss: 0.1392\n",
      "[Trial 109] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 104] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1305\n",
      "[Trial 110] Epoch 31/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 111] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1366\n",
      "[Trial 109] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 110] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 103] Epoch 40/60, Training Loss: 0.1268, Validation Loss: 0.1289\n",
      "[Trial 111] Epoch 21/60, Training Loss: 0.1272, Validation Loss: 0.1344\n",
      "[Trial 98] Epoch 59/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 110] Epoch 33/60, Training Loss: 0.1253, Validation Loss: 0.1256\n",
      "[Trial 109] Epoch 41/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 111] Epoch 22/60, Training Loss: 0.1282, Validation Loss: 0.1321\n",
      "[Trial 108] Epoch 15/60, Training Loss: 0.1254, Validation Loss: 0.1273\n",
      "[Trial 104] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1287\n",
      "[Trial 110] Epoch 34/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 109] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 111] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:51:39,515] Trial 98 finished with value: 0.12546008576949438 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0007281721838327557, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 98] Epoch 60/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 110] Epoch 35/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 103] Epoch 41/60, Training Loss: 0.1265, Validation Loss: 0.1279\n",
      "[Trial 109] Epoch 43/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 111] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1289\n",
      "[Trial 110] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 112] Epoch 1/60, Training Loss: 324.9398, Validation Loss: 0.2767\n",
      "[Trial 109] Epoch 44/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 111] Epoch 25/60, Training Loss: 0.1258, Validation Loss: 0.1280\n",
      "[Trial 104] Epoch 41/60, Training Loss: 0.1251, Validation Loss: 0.1284\n",
      "[Trial 110] Epoch 37/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 108] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1273\n",
      "[Trial 112] Epoch 2/60, Training Loss: 0.1439, Validation Loss: 0.2159\n",
      "[Trial 109] Epoch 45/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 111] Epoch 26/60, Training Loss: 0.1266, Validation Loss: 0.1273\n",
      "[Trial 103] Epoch 42/60, Training Loss: 0.1267, Validation Loss: 0.1266\n",
      "[Trial 110] Epoch 38/60, Training Loss: 0.1253, Validation Loss: 0.1256\n",
      "[Trial 112] Epoch 3/60, Training Loss: 0.1303, Validation Loss: 0.2038\n",
      "[Trial 109] Epoch 46/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 111] Epoch 27/60, Training Loss: 0.1260, Validation Loss: 0.1269\n",
      "[Trial 110] Epoch 39/60, Training Loss: 0.1253, Validation Loss: 0.1256\n",
      "[Trial 104] Epoch 42/60, Training Loss: 0.1252, Validation Loss: 0.1303\n",
      "[Trial 112] Epoch 4/60, Training Loss: 0.1296, Validation Loss: 0.1933\n",
      "[Trial 111] Epoch 28/60, Training Loss: 0.1276, Validation Loss: 0.1275\n",
      "[Trial 109] Epoch 47/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 110] Epoch 40/60, Training Loss: 0.1252, Validation Loss: 0.1256\n",
      "[Trial 103] Epoch 43/60, Training Loss: 0.1266, Validation Loss: 0.1266\n",
      "[Trial 108] Epoch 17/60, Training Loss: 0.1258, Validation Loss: 0.1274\n",
      "[Trial 112] Epoch 5/60, Training Loss: 0.1285, Validation Loss: 0.1841\n",
      "[Trial 111] Epoch 29/60, Training Loss: 0.1292, Validation Loss: 0.1265\n",
      "[Trial 110] Epoch 41/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 109] Epoch 48/60, Training Loss: 0.1256, Validation Loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:52:34,457] Trial 110 finished with value: 0.12557626912991207 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.010114024552601741, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 110] Epoch 42/60, Training Loss: 0.1254, Validation Loss: 0.1256\n",
      "[Trial 110] Early stopping after 42 epochs.\n",
      "[Trial 111] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 112] Epoch 6/60, Training Loss: 0.1286, Validation Loss: 0.1766\n",
      "[Trial 109] Epoch 49/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 104] Epoch 43/60, Training Loss: 0.1251, Validation Loss: 0.1286\n",
      "[Trial 111] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1262\n",
      "[Trial 113] Epoch 1/60, Training Loss: 11.9355, Validation Loss: 0.2631\n",
      "[Trial 112] Epoch 7/60, Training Loss: 0.1279, Validation Loss: 0.1709\n",
      "[Trial 109] Epoch 50/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 103] Epoch 44/60, Training Loss: 0.1265, Validation Loss: 0.1291\n",
      "[Trial 111] Epoch 32/60, Training Loss: 0.1258, Validation Loss: 0.1262\n",
      "[Trial 113] Epoch 2/60, Training Loss: 0.1368, Validation Loss: 0.2358\n",
      "[Trial 112] Epoch 8/60, Training Loss: 0.1283, Validation Loss: 0.1643\n",
      "[Trial 108] Epoch 18/60, Training Loss: 0.1255, Validation Loss: 0.1273\n",
      "[Trial 109] Epoch 51/60, Training Loss: 0.1255, Validation Loss: 0.1244\n",
      "[Trial 111] Epoch 33/60, Training Loss: 0.1321, Validation Loss: 0.1264\n",
      "[Trial 104] Epoch 44/60, Training Loss: 0.1253, Validation Loss: 0.1288\n",
      "[Trial 113] Epoch 3/60, Training Loss: 0.1310, Validation Loss: 0.2246\n",
      "[Trial 112] Epoch 9/60, Training Loss: 0.1277, Validation Loss: 0.1578\n",
      "[Trial 109] Epoch 52/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 111] Epoch 34/60, Training Loss: 0.1258, Validation Loss: 0.1262\n",
      "[Trial 103] Epoch 45/60, Training Loss: 0.1285, Validation Loss: 0.1256\n",
      "[Trial 113] Epoch 4/60, Training Loss: 0.1297, Validation Loss: 0.2184\n",
      "[Trial 109] Epoch 53/60, Training Loss: 0.1255, Validation Loss: 0.1244\n",
      "[Trial 112] Epoch 10/60, Training Loss: 0.1272, Validation Loss: 0.1536\n",
      "[Trial 111] Epoch 35/60, Training Loss: 0.1259, Validation Loss: 0.1263\n",
      "[Trial 113] Epoch 5/60, Training Loss: 0.1293, Validation Loss: 0.2103\n",
      "[Trial 109] Epoch 54/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 112] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:53:22,694] Trial 108 finished with value: 0.1272503886371851 and parameters: {'hidden_dim': 512, 'latent_dim': 32, 'learning_rate': 0.004469284529183204, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 108] Epoch 19/60, Training Loss: 0.1252, Validation Loss: 0.1273\n",
      "[Trial 108] Early stopping after 19 epochs.\n",
      "[Trial 104] Epoch 45/60, Training Loss: 0.1251, Validation Loss: 0.1281\n",
      "[Trial 111] Epoch 36/60, Training Loss: 0.1269, Validation Loss: 0.1263\n",
      "[Trial 109] Epoch 55/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 113] Epoch 6/60, Training Loss: 0.1287, Validation Loss: 0.2067\n",
      "[Trial 112] Epoch 12/60, Training Loss: 0.1267, Validation Loss: 0.1448\n",
      "[Trial 103] Epoch 46/60, Training Loss: 0.1263, Validation Loss: 0.1258\n",
      "[Trial 114] Epoch 1/60, Training Loss: 9.0935, Validation Loss: 0.2592\n",
      "[Trial 111] Epoch 37/60, Training Loss: 0.1259, Validation Loss: 0.1261\n",
      "[Trial 109] Epoch 56/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 113] Epoch 7/60, Training Loss: 0.1285, Validation Loss: 0.2029\n",
      "[Trial 112] Epoch 13/60, Training Loss: 0.1273, Validation Loss: 0.1416\n",
      "[Trial 114] Epoch 2/60, Training Loss: 0.1382, Validation Loss: 0.2390\n",
      "[Trial 111] Epoch 38/60, Training Loss: 0.1261, Validation Loss: 0.1261\n",
      "[Trial 104] Epoch 46/60, Training Loss: 0.1253, Validation Loss: 0.1282\n",
      "[Trial 109] Epoch 57/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 113] Epoch 8/60, Training Loss: 0.1281, Validation Loss: 0.1986\n",
      "[Trial 112] Epoch 14/60, Training Loss: 0.1267, Validation Loss: 0.1382\n",
      "[Trial 111] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1262\n",
      "[Trial 114] Epoch 3/60, Training Loss: 0.1319, Validation Loss: 0.2292\n",
      "[Trial 103] Epoch 47/60, Training Loss: 0.1263, Validation Loss: 0.1255\n",
      "[Trial 109] Epoch 58/60, Training Loss: 0.1254, Validation Loss: 0.1245\n",
      "[Trial 113] Epoch 9/60, Training Loss: 0.1283, Validation Loss: 0.1924\n",
      "[Trial 111] Epoch 40/60, Training Loss: 0.1258, Validation Loss: 0.1261\n",
      "[Trial 112] Epoch 15/60, Training Loss: 0.1265, Validation Loss: 0.1356\n",
      "[Trial 114] Epoch 4/60, Training Loss: 0.1300, Validation Loss: 0.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:54:06,168] Trial 109 finished with value: 0.12444628874460856 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.00446302382921184, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 109] Epoch 59/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 109] Early stopping after 59 epochs.\n",
      "[Trial 113] Epoch 10/60, Training Loss: 0.1287, Validation Loss: 0.1902\n",
      "[Trial 104] Epoch 47/60, Training Loss: 0.1268, Validation Loss: 0.1282\n",
      "[Trial 111] Epoch 41/60, Training Loss: 0.1664, Validation Loss: 0.4257\n",
      "[Trial 112] Epoch 16/60, Training Loss: 0.1268, Validation Loss: 0.1332\n",
      "[Trial 114] Epoch 5/60, Training Loss: 0.1300, Validation Loss: 0.2169\n",
      "[Trial 115] Epoch 1/60, Training Loss: 4.7036, Validation Loss: 0.2591\n",
      "[Trial 103] Epoch 48/60, Training Loss: 0.1263, Validation Loss: 0.1254\n",
      "[Trial 113] Epoch 11/60, Training Loss: 0.1284, Validation Loss: 0.1837\n",
      "[Trial 111] Epoch 42/60, Training Loss: 0.1869, Validation Loss: 0.1276\n",
      "[Trial 112] Epoch 17/60, Training Loss: 0.1267, Validation Loss: 0.1310\n",
      "[Trial 114] Epoch 6/60, Training Loss: 0.1296, Validation Loss: 0.2127\n",
      "[Trial 115] Epoch 2/60, Training Loss: 0.1386, Validation Loss: 0.2405\n",
      "[Trial 113] Epoch 12/60, Training Loss: 0.1281, Validation Loss: 0.1782\n",
      "[Trial 111] Epoch 43/60, Training Loss: 0.1256, Validation Loss: 0.1262\n",
      "[Trial 112] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1297\n",
      "[Trial 104] Epoch 48/60, Training Loss: 0.1251, Validation Loss: 0.1281\n",
      "[Trial 114] Epoch 7/60, Training Loss: 0.1284, Validation Loss: 0.2068\n",
      "[Trial 113] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.1737\n",
      "[Trial 115] Epoch 3/60, Training Loss: 0.1323, Validation Loss: 0.2328\n",
      "[Trial 111] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 103] Epoch 49/60, Training Loss: 0.1264, Validation Loss: 0.1246\n",
      "[Trial 112] Epoch 19/60, Training Loss: 0.1264, Validation Loss: 0.1284\n",
      "[Trial 114] Epoch 8/60, Training Loss: 0.1286, Validation Loss: 0.2028\n",
      "[Trial 111] Epoch 45/60, Training Loss: 0.1254, Validation Loss: 0.1262\n",
      "[Trial 113] Epoch 14/60, Training Loss: 0.1284, Validation Loss: 0.1687\n",
      "[Trial 115] Epoch 4/60, Training Loss: 0.1307, Validation Loss: 0.2284\n",
      "[Trial 112] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1274\n",
      "[Trial 114] Epoch 9/60, Training Loss: 0.1283, Validation Loss: 0.2003\n",
      "[Trial 111] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 104] Epoch 49/60, Training Loss: 0.1250, Validation Loss: 0.1282\n",
      "[Trial 113] Epoch 15/60, Training Loss: 0.1282, Validation Loss: 0.1633\n",
      "[Trial 115] Epoch 5/60, Training Loss: 0.1296, Validation Loss: 0.2221\n",
      "[Trial 103] Epoch 50/60, Training Loss: 0.1260, Validation Loss: 0.1245\n",
      "[Trial 112] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 111] Epoch 47/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 114] Epoch 10/60, Training Loss: 0.1287, Validation Loss: 0.1971\n",
      "[Trial 113] Epoch 16/60, Training Loss: 0.1276, Validation Loss: 0.1581\n",
      "[Trial 115] Epoch 6/60, Training Loss: 0.1288, Validation Loss: 0.2195\n",
      "[Trial 112] Epoch 22/60, Training Loss: 0.1499, Validation Loss: 0.1339\n",
      "[Trial 111] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 113] Epoch 17/60, Training Loss: 0.1278, Validation Loss: 0.1537\n",
      "[Trial 114] Epoch 11/60, Training Loss: 0.1282, Validation Loss: 0.1935\n",
      "[Trial 104] Epoch 50/60, Training Loss: 0.1252, Validation Loss: 0.1280\n",
      "[Trial 115] Epoch 7/60, Training Loss: 0.1282, Validation Loss: 0.2138\n",
      "[Trial 112] Epoch 23/60, Training Loss: 0.1976, Validation Loss: 0.1269\n",
      "[Trial 111] Epoch 49/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 103] Epoch 51/60, Training Loss: 0.1270, Validation Loss: 0.1248\n",
      "[Trial 113] Epoch 18/60, Training Loss: 0.1275, Validation Loss: 0.1498\n",
      "[Trial 114] Epoch 12/60, Training Loss: 0.1277, Validation Loss: 0.1876\n",
      "[Trial 115] Epoch 8/60, Training Loss: 0.1284, Validation Loss: 0.2100\n",
      "[Trial 111] Epoch 50/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 112] Epoch 24/60, Training Loss: 0.1265, Validation Loss: 0.1266\n",
      "[Trial 113] Epoch 19/60, Training Loss: 0.1276, Validation Loss: 0.1463\n",
      "[Trial 114] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.1846\n",
      "[Trial 111] Epoch 51/60, Training Loss: 0.1251, Validation Loss: 0.1261\n",
      "[Trial 115] Epoch 9/60, Training Loss: 0.1280, Validation Loss: 0.2065\n",
      "[Trial 112] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1265\n",
      "[Trial 104] Epoch 51/60, Training Loss: 0.1252, Validation Loss: 0.1282\n",
      "[Trial 103] Epoch 52/60, Training Loss: 0.1262, Validation Loss: 0.1243\n",
      "[Trial 113] Epoch 20/60, Training Loss: 0.1278, Validation Loss: 0.1430\n",
      "[Trial 111] Epoch 52/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 114] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1814\n",
      "[Trial 115] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.2044\n",
      "[Trial 112] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1266\n",
      "[Trial 113] Epoch 21/60, Training Loss: 0.1280, Validation Loss: 0.1397\n",
      "[Trial 111] Epoch 53/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 114] Epoch 15/60, Training Loss: 0.1280, Validation Loss: 0.1779\n",
      "[Trial 115] Epoch 11/60, Training Loss: 0.1282, Validation Loss: 0.2010\n",
      "[Trial 112] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1265\n",
      "[Trial 104] Epoch 52/60, Training Loss: 0.1251, Validation Loss: 0.1280\n",
      "[Trial 111] Epoch 54/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 113] Epoch 22/60, Training Loss: 0.1274, Validation Loss: 0.1368\n",
      "[Trial 103] Epoch 53/60, Training Loss: 0.1265, Validation Loss: 0.1243\n",
      "[Trial 114] Epoch 16/60, Training Loss: 0.1279, Validation Loss: 0.1739\n",
      "[Trial 112] Epoch 28/60, Training Loss: 0.1263, Validation Loss: 0.1265\n",
      "[Trial 115] Epoch 12/60, Training Loss: 0.1280, Validation Loss: 0.1993\n",
      "[Trial 111] Epoch 55/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 113] Epoch 23/60, Training Loss: 0.1346, Validation Loss: 0.1357\n",
      "[Trial 112] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1264\n",
      "[Trial 114] Epoch 17/60, Training Loss: 0.1287, Validation Loss: 0.1697\n",
      "[Trial 115] Epoch 13/60, Training Loss: 0.1274, Validation Loss: 0.1953\n",
      "[Trial 111] Epoch 56/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 113] Epoch 24/60, Training Loss: 0.1281, Validation Loss: 0.1318\n",
      "[Trial 104] Epoch 53/60, Training Loss: 0.1252, Validation Loss: 0.1287\n",
      "[Trial 112] Epoch 30/60, Training Loss: 0.1265, Validation Loss: 0.1264\n",
      "[Trial 103] Epoch 54/60, Training Loss: 0.1261, Validation Loss: 0.1242\n",
      "[Trial 114] Epoch 18/60, Training Loss: 0.1275, Validation Loss: 0.1669\n",
      "[Trial 115] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1926\n",
      "[Trial 111] Epoch 57/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 113] Epoch 25/60, Training Loss: 0.1273, Validation Loss: 0.1299\n",
      "[Trial 112] Epoch 31/60, Training Loss: 0.1273, Validation Loss: 0.1265\n",
      "[Trial 111] Epoch 58/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 114] Epoch 19/60, Training Loss: 0.1277, Validation Loss: 0.1636\n",
      "[Trial 115] Epoch 15/60, Training Loss: 0.1277, Validation Loss: 0.1892\n",
      "[Trial 113] Epoch 26/60, Training Loss: 0.1270, Validation Loss: 0.1284\n",
      "[Trial 112] Epoch 32/60, Training Loss: 0.1690, Validation Loss: 0.2886\n",
      "[Trial 111] Epoch 59/60, Training Loss: 0.1251, Validation Loss: 0.1261\n",
      "[Trial 104] Epoch 54/60, Training Loss: 0.1253, Validation Loss: 0.1285\n",
      "[Trial 114] Epoch 20/60, Training Loss: 0.1283, Validation Loss: 0.1608\n",
      "[Trial 115] Epoch 16/60, Training Loss: 0.1272, Validation Loss: 0.1864\n",
      "[Trial 103] Epoch 55/60, Training Loss: 0.1263, Validation Loss: 0.1246\n",
      "[Trial 113] Epoch 27/60, Training Loss: 0.1270, Validation Loss: 0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:57:02,922] Trial 111 finished with value: 0.1260945424437523 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.005258187177894826, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 112] Epoch 33/60, Training Loss: 0.2167, Validation Loss: 0.1574\n",
      "[Trial 111] Epoch 60/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 114] Epoch 21/60, Training Loss: 0.1275, Validation Loss: 0.1567\n",
      "[Trial 115] Epoch 17/60, Training Loss: 0.1274, Validation Loss: 0.1829\n",
      "[Trial 113] Epoch 28/60, Training Loss: 0.1275, Validation Loss: 0.1264\n",
      "[Trial 116] Epoch 1/60, Training Loss: 34.4966, Validation Loss: 0.2667\n",
      "[Trial 112] Epoch 34/60, Training Loss: 0.1290, Validation Loss: 0.1262\n",
      "[Trial 104] Epoch 55/60, Training Loss: 0.1252, Validation Loss: 0.1281\n",
      "[Trial 114] Epoch 22/60, Training Loss: 0.1271, Validation Loss: 0.1534\n",
      "[Trial 115] Epoch 18/60, Training Loss: 0.1275, Validation Loss: 0.1812\n",
      "[Trial 113] Epoch 29/60, Training Loss: 0.1269, Validation Loss: 0.1254\n",
      "[Trial 103] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1260\n",
      "[Trial 116] Epoch 2/60, Training Loss: 0.1367, Validation Loss: 0.2011\n",
      "[Trial 112] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1262\n",
      "[Trial 114] Epoch 23/60, Training Loss: 0.1273, Validation Loss: 0.1506\n",
      "[Trial 115] Epoch 19/60, Training Loss: 0.1273, Validation Loss: 0.1771\n",
      "[Trial 116] Epoch 3/60, Training Loss: 0.1285, Validation Loss: 0.1847\n",
      "[Trial 113] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1250\n",
      "[Trial 112] Epoch 36/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 116] Epoch 4/60, Training Loss: 0.1289, Validation Loss: 0.1709\n",
      "[Trial 113] Epoch 31/60, Training Loss: 0.1279, Validation Loss: 0.1249\n",
      "[Trial 114] Epoch 24/60, Training Loss: 0.1270, Validation Loss: 0.1477\n",
      "[Trial 115] Epoch 20/60, Training Loss: 0.1271, Validation Loss: 0.1748\n",
      "[Trial 104] Epoch 56/60, Training Loss: 0.1250, Validation Loss: 0.1279\n",
      "[Trial 112] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1262\n",
      "[Trial 103] Epoch 57/60, Training Loss: 0.1268, Validation Loss: 0.1240\n",
      "[Trial 116] Epoch 5/60, Training Loss: 0.1289, Validation Loss: 0.1583\n",
      "[Trial 113] Epoch 32/60, Training Loss: 0.1270, Validation Loss: 0.1246\n",
      "[Trial 114] Epoch 25/60, Training Loss: 0.1273, Validation Loss: 0.1450\n",
      "[Trial 115] Epoch 21/60, Training Loss: 0.1274, Validation Loss: 0.1712\n",
      "[Trial 112] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1263\n",
      "[Trial 116] Epoch 6/60, Training Loss: 0.1291, Validation Loss: 0.1485\n",
      "[Trial 113] Epoch 33/60, Training Loss: 0.1265, Validation Loss: 0.1244\n",
      "[Trial 114] Epoch 26/60, Training Loss: 0.1274, Validation Loss: 0.1426\n",
      "[Trial 115] Epoch 22/60, Training Loss: 0.1269, Validation Loss: 0.1694\n",
      "[Trial 112] Epoch 39/60, Training Loss: 0.1258, Validation Loss: 0.1262\n",
      "[Trial 104] Epoch 57/60, Training Loss: 0.1248, Validation Loss: 0.1279\n",
      "[Trial 116] Epoch 7/60, Training Loss: 0.1283, Validation Loss: 0.1399\n",
      "[Trial 103] Epoch 58/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 113] Epoch 34/60, Training Loss: 0.1285, Validation Loss: 0.1244\n",
      "[Trial 114] Epoch 27/60, Training Loss: 0.1276, Validation Loss: 0.1403\n",
      "[Trial 115] Epoch 23/60, Training Loss: 0.1268, Validation Loss: 0.1670\n",
      "[Trial 112] Epoch 40/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 116] Epoch 8/60, Training Loss: 0.1281, Validation Loss: 0.1340\n",
      "[Trial 113] Epoch 35/60, Training Loss: 0.1273, Validation Loss: 0.1242\n",
      "[Trial 114] Epoch 28/60, Training Loss: 0.1274, Validation Loss: 0.1384\n",
      "[Trial 116] Epoch 9/60, Training Loss: 0.1278, Validation Loss: 0.1299\n",
      "[Trial 115] Epoch 24/60, Training Loss: 0.1270, Validation Loss: 0.1630\n",
      "[Trial 112] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1262\n",
      "[Trial 104] Epoch 58/60, Training Loss: 0.1248, Validation Loss: 0.1281\n",
      "[Trial 113] Epoch 36/60, Training Loss: 0.1271, Validation Loss: 0.1242\n",
      "[Trial 103] Epoch 59/60, Training Loss: 0.1263, Validation Loss: 0.1243\n",
      "[Trial 116] Epoch 10/60, Training Loss: 0.1286, Validation Loss: 0.1274\n",
      "[Trial 114] Epoch 29/60, Training Loss: 0.1275, Validation Loss: 0.1361\n",
      "[Trial 112] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 115] Epoch 25/60, Training Loss: 0.1270, Validation Loss: 0.1611\n",
      "[Trial 113] Epoch 37/60, Training Loss: 0.1288, Validation Loss: 0.1244\n",
      "[Trial 116] Epoch 11/60, Training Loss: 0.1275, Validation Loss: 0.1258\n",
      "[Trial 114] Epoch 30/60, Training Loss: 0.1273, Validation Loss: 0.1339\n",
      "[Trial 112] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 115] Epoch 26/60, Training Loss: 0.1272, Validation Loss: 0.1592\n",
      "[Trial 113] Epoch 38/60, Training Loss: 0.1297, Validation Loss: 0.1242\n",
      "[Trial 116] Epoch 12/60, Training Loss: 0.1273, Validation Loss: 0.1251\n",
      "[Trial 104] Epoch 59/60, Training Loss: 0.1250, Validation Loss: 0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:58:52,053] Trial 103 finished with value: 0.12401259280741214 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0007067363154048988, 'batch_size': 16, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 103] Epoch 60/60, Training Loss: 0.1262, Validation Loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:58:53,213] Trial 112 finished with value: 0.1261646792292595 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.010632234199415638, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 112] Epoch 44/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 112] Early stopping after 44 epochs.\n",
      "[Trial 114] Epoch 31/60, Training Loss: 0.1267, Validation Loss: 0.1322\n",
      "[Trial 115] Epoch 27/60, Training Loss: 0.1268, Validation Loss: 0.1563\n",
      "[Trial 113] Epoch 39/60, Training Loss: 0.1269, Validation Loss: 0.1242\n",
      "[Trial 116] Epoch 13/60, Training Loss: 0.1499, Validation Loss: 0.1249\n",
      "[Trial 118] Epoch 1/60, Training Loss: 36.7333, Validation Loss: 0.4604\n",
      "[Trial 117] Epoch 1/60, Training Loss: 7.6705, Validation Loss: 0.2708\n",
      "[Trial 114] Epoch 32/60, Training Loss: 0.1269, Validation Loss: 0.1310\n",
      "[Trial 115] Epoch 28/60, Training Loss: 0.1274, Validation Loss: 0.1535\n",
      "[Trial 116] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1248\n",
      "[Trial 113] Epoch 40/60, Training Loss: 0.1266, Validation Loss: 0.1242\n",
      "[Trial 118] Epoch 2/60, Training Loss: 0.2417, Validation Loss: 0.2627\n",
      "[Trial 117] Epoch 2/60, Training Loss: 0.1476, Validation Loss: 0.2399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 13:59:14,593] Trial 104 finished with value: 0.12791324878732363 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.000884033380270574, 'batch_size': 16, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 104] Epoch 60/60, Training Loss: 0.1248, Validation Loss: 0.1280\n",
      "[Trial 114] Epoch 33/60, Training Loss: 0.1268, Validation Loss: 0.1296\n",
      "[Trial 116] Epoch 15/60, Training Loss: 0.1273, Validation Loss: 0.1247\n",
      "[Trial 115] Epoch 29/60, Training Loss: 0.1269, Validation Loss: 0.1518\n",
      "[Trial 113] Epoch 41/60, Training Loss: 0.1262, Validation Loss: 0.1242\n",
      "[Trial 118] Epoch 3/60, Training Loss: 0.1738, Validation Loss: 0.2171\n",
      "[Trial 117] Epoch 3/60, Training Loss: 0.1364, Validation Loss: 0.2285\n",
      "[Trial 119] Epoch 1/60, Training Loss: 39.5888, Validation Loss: 0.4810\n",
      "[Trial 116] Epoch 16/60, Training Loss: 0.1292, Validation Loss: 0.1247\n",
      "[Trial 118] Epoch 4/60, Training Loss: 0.1549, Validation Loss: 0.1966\n",
      "[Trial 114] Epoch 34/60, Training Loss: 0.1275, Validation Loss: 0.1287\n",
      "[Trial 113] Epoch 42/60, Training Loss: 0.1277, Validation Loss: 0.1242\n",
      "[Trial 115] Epoch 30/60, Training Loss: 0.1269, Validation Loss: 0.1502\n",
      "[Trial 119] Epoch 2/60, Training Loss: 0.2499, Validation Loss: 0.2657\n",
      "[Trial 117] Epoch 4/60, Training Loss: 0.1334, Validation Loss: 0.2222\n",
      "[Trial 118] Epoch 5/60, Training Loss: 0.1456, Validation Loss: 0.1821\n",
      "[Trial 116] Epoch 17/60, Training Loss: 0.1549, Validation Loss: 0.1298\n",
      "[Trial 113] Epoch 43/60, Training Loss: 0.1268, Validation Loss: 0.1241\n",
      "[Trial 114] Epoch 35/60, Training Loss: 0.1269, Validation Loss: 0.1277\n",
      "[Trial 115] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1485\n",
      "[Trial 119] Epoch 3/60, Training Loss: 0.1808, Validation Loss: 0.2246\n",
      "[Trial 118] Epoch 6/60, Training Loss: 0.1408, Validation Loss: 0.1727\n",
      "[Trial 117] Epoch 5/60, Training Loss: 0.1315, Validation Loss: 0.2164\n",
      "[Trial 116] Epoch 18/60, Training Loss: 0.1302, Validation Loss: 0.1248\n",
      "[Trial 113] Epoch 44/60, Training Loss: 0.1311, Validation Loss: 0.1241\n",
      "[Trial 119] Epoch 4/60, Training Loss: 0.1617, Validation Loss: 0.2010\n",
      "[Trial 114] Epoch 36/60, Training Loss: 0.1269, Validation Loss: 0.1267\n",
      "[Trial 115] Epoch 32/60, Training Loss: 0.1266, Validation Loss: 0.1461\n",
      "[Trial 118] Epoch 7/60, Training Loss: 0.1376, Validation Loss: 0.1660\n",
      "[Trial 116] Epoch 19/60, Training Loss: 0.1275, Validation Loss: 0.1247\n",
      "[Trial 117] Epoch 6/60, Training Loss: 0.1305, Validation Loss: 0.2137\n",
      "[Trial 119] Epoch 5/60, Training Loss: 0.1514, Validation Loss: 0.1856\n",
      "[Trial 113] Epoch 45/60, Training Loss: 0.1269, Validation Loss: 0.1242\n",
      "[Trial 118] Epoch 8/60, Training Loss: 0.1356, Validation Loss: 0.1623\n",
      "[Trial 114] Epoch 37/60, Training Loss: 0.1269, Validation Loss: 0.1261\n",
      "[Trial 115] Epoch 33/60, Training Loss: 0.1269, Validation Loss: 0.1439\n",
      "[Trial 116] Epoch 20/60, Training Loss: 0.1264, Validation Loss: 0.1247\n",
      "[Trial 117] Epoch 7/60, Training Loss: 0.1298, Validation Loss: 0.2095\n",
      "[Trial 119] Epoch 6/60, Training Loss: 0.1457, Validation Loss: 0.1753\n",
      "[Trial 118] Epoch 9/60, Training Loss: 0.1342, Validation Loss: 0.1539\n",
      "[Trial 113] Epoch 46/60, Training Loss: 0.1270, Validation Loss: 0.1242\n",
      "[Trial 115] Epoch 34/60, Training Loss: 0.1275, Validation Loss: 0.1428\n",
      "[Trial 114] Epoch 38/60, Training Loss: 0.1267, Validation Loss: 0.1255\n",
      "[Trial 116] Epoch 21/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 119] Epoch 7/60, Training Loss: 0.1419, Validation Loss: 0.1661\n",
      "[Trial 118] Epoch 10/60, Training Loss: 0.1323, Validation Loss: 0.1511\n",
      "[Trial 117] Epoch 8/60, Training Loss: 0.1305, Validation Loss: 0.2073\n",
      "[Trial 113] Epoch 47/60, Training Loss: 0.1266, Validation Loss: 0.1241\n",
      "[Trial 119] Epoch 8/60, Training Loss: 0.1393, Validation Loss: 0.1625\n",
      "[Trial 116] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1248\n",
      "[Trial 115] Epoch 35/60, Training Loss: 0.1264, Validation Loss: 0.1412\n",
      "[Trial 114] Epoch 39/60, Training Loss: 0.1266, Validation Loss: 0.1251\n",
      "[Trial 118] Epoch 11/60, Training Loss: 0.1313, Validation Loss: 0.1475\n",
      "[Trial 117] Epoch 9/60, Training Loss: 0.1291, Validation Loss: 0.2033\n",
      "[Trial 113] Epoch 48/60, Training Loss: 0.1645, Validation Loss: 0.1260\n",
      "[Trial 119] Epoch 9/60, Training Loss: 0.1373, Validation Loss: 0.1552\n",
      "[Trial 118] Epoch 12/60, Training Loss: 0.1303, Validation Loss: 0.1458\n",
      "[Trial 116] Epoch 23/60, Training Loss: 0.1259, Validation Loss: 0.1247\n",
      "[Trial 115] Epoch 36/60, Training Loss: 0.1263, Validation Loss: 0.1396\n",
      "[Trial 114] Epoch 40/60, Training Loss: 0.1270, Validation Loss: 0.1247\n",
      "[Trial 117] Epoch 10/60, Training Loss: 0.1289, Validation Loss: 0.1981\n",
      "[Trial 113] Epoch 49/60, Training Loss: 0.1269, Validation Loss: 0.1241\n",
      "[Trial 119] Epoch 10/60, Training Loss: 0.1356, Validation Loss: 0.1538\n",
      "[Trial 118] Epoch 13/60, Training Loss: 0.1299, Validation Loss: 0.1440\n",
      "[Trial 116] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 115] Epoch 37/60, Training Loss: 0.1267, Validation Loss: 0.1378\n",
      "[Trial 114] Epoch 41/60, Training Loss: 0.1267, Validation Loss: 0.1244\n",
      "[Trial 113] Epoch 50/60, Training Loss: 0.1262, Validation Loss: 0.1242\n",
      "[Trial 117] Epoch 11/60, Training Loss: 0.1286, Validation Loss: 0.1962\n",
      "[Trial 119] Epoch 11/60, Training Loss: 0.1340, Validation Loss: 0.1485\n",
      "[Trial 118] Epoch 14/60, Training Loss: 0.1294, Validation Loss: 0.1421\n",
      "[Trial 116] Epoch 25/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 119] Epoch 12/60, Training Loss: 0.1331, Validation Loss: 0.1450\n",
      "[Trial 115] Epoch 38/60, Training Loss: 0.1262, Validation Loss: 0.1373\n",
      "[Trial 118] Epoch 15/60, Training Loss: 0.1290, Validation Loss: 0.1408\n",
      "[Trial 114] Epoch 42/60, Training Loss: 0.1271, Validation Loss: 0.1242\n",
      "[Trial 113] Epoch 51/60, Training Loss: 0.1260, Validation Loss: 0.1241\n",
      "[Trial 117] Epoch 12/60, Training Loss: 0.1287, Validation Loss: 0.1931\n",
      "[Trial 116] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 118] Epoch 16/60, Training Loss: 0.1285, Validation Loss: 0.1389\n",
      "[Trial 119] Epoch 13/60, Training Loss: 0.1322, Validation Loss: 0.1439\n",
      "[Trial 113] Epoch 52/60, Training Loss: 0.1261, Validation Loss: 0.1242\n",
      "[Trial 115] Epoch 39/60, Training Loss: 0.1277, Validation Loss: 0.1380\n",
      "[Trial 114] Epoch 43/60, Training Loss: 0.1264, Validation Loss: 0.1241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:01:11,338] Trial 116 finished with value: 0.12466700772444407 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.013416491084443117, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 116] Epoch 27/60, Training Loss: 0.1257, Validation Loss: 0.1247\n",
      "[Trial 116] Early stopping after 27 epochs.\n",
      "[Trial 117] Epoch 13/60, Training Loss: 0.1288, Validation Loss: 0.1910\n",
      "[Trial 118] Epoch 17/60, Training Loss: 0.1280, Validation Loss: 0.1379\n",
      "[Trial 119] Epoch 14/60, Training Loss: 0.1316, Validation Loss: 0.1417\n",
      "[Trial 113] Epoch 53/60, Training Loss: 0.1259, Validation Loss: 0.1242\n",
      "[Trial 115] Epoch 40/60, Training Loss: 0.1294, Validation Loss: 0.1350\n",
      "[Trial 114] Epoch 44/60, Training Loss: 0.1268, Validation Loss: 0.1240\n",
      "[Trial 120] Epoch 1/60, Training Loss: 3.4433, Validation Loss: 0.2294\n",
      "[Trial 117] Epoch 14/60, Training Loss: 0.1284, Validation Loss: 0.1893\n",
      "[Trial 118] Epoch 18/60, Training Loss: 0.1281, Validation Loss: 0.1376\n",
      "[Trial 119] Epoch 15/60, Training Loss: 0.1312, Validation Loss: 0.1393\n",
      "[Trial 113] Epoch 54/60, Training Loss: 0.1259, Validation Loss: 0.1241\n",
      "[Trial 115] Epoch 41/60, Training Loss: 0.1261, Validation Loss: 0.1340\n",
      "[Trial 114] Epoch 45/60, Training Loss: 0.1348, Validation Loss: 0.1495\n",
      "[Trial 118] Epoch 19/60, Training Loss: 0.1277, Validation Loss: 0.1369\n",
      "[Trial 120] Epoch 2/60, Training Loss: 0.1385, Validation Loss: 0.2115\n",
      "[Trial 119] Epoch 16/60, Training Loss: 0.1307, Validation Loss: 0.1387\n",
      "[Trial 117] Epoch 15/60, Training Loss: 0.1282, Validation Loss: 0.1866\n",
      "[Trial 113] Epoch 55/60, Training Loss: 0.1260, Validation Loss: 0.1241\n",
      "[Trial 118] Epoch 20/60, Training Loss: 0.1274, Validation Loss: 0.1362\n",
      "[Trial 119] Epoch 17/60, Training Loss: 0.1300, Validation Loss: 0.1382\n",
      "[Trial 115] Epoch 42/60, Training Loss: 0.1262, Validation Loss: 0.1328\n",
      "[Trial 120] Epoch 3/60, Training Loss: 0.1326, Validation Loss: 0.1982\n",
      "[Trial 114] Epoch 46/60, Training Loss: 0.1299, Validation Loss: 0.1239\n",
      "[Trial 117] Epoch 16/60, Training Loss: 0.1291, Validation Loss: 0.1828\n",
      "[Trial 118] Epoch 21/60, Training Loss: 0.1276, Validation Loss: 0.1357\n",
      "[Trial 113] Epoch 56/60, Training Loss: 0.1260, Validation Loss: 0.1242\n",
      "[Trial 119] Epoch 18/60, Training Loss: 0.1297, Validation Loss: 0.1369\n",
      "[Trial 120] Epoch 4/60, Training Loss: 0.1307, Validation Loss: 0.1886\n",
      "[Trial 115] Epoch 43/60, Training Loss: 0.1265, Validation Loss: 0.1318\n",
      "[Trial 117] Epoch 17/60, Training Loss: 0.1282, Validation Loss: 0.1803\n",
      "[Trial 114] Epoch 47/60, Training Loss: 0.1265, Validation Loss: 0.1238\n",
      "[Trial 118] Epoch 22/60, Training Loss: 0.1274, Validation Loss: 0.1349\n",
      "[Trial 113] Epoch 57/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 119] Epoch 19/60, Training Loss: 0.1294, Validation Loss: 0.1359\n",
      "[Trial 120] Epoch 5/60, Training Loss: 0.1298, Validation Loss: 0.1799\n",
      "[Trial 115] Epoch 44/60, Training Loss: 0.1260, Validation Loss: 0.1311\n",
      "[Trial 117] Epoch 18/60, Training Loss: 0.1281, Validation Loss: 0.1788\n",
      "[Trial 118] Epoch 23/60, Training Loss: 0.1275, Validation Loss: 0.1346\n",
      "[Trial 114] Epoch 48/60, Training Loss: 0.1264, Validation Loss: 0.1238\n",
      "[Trial 119] Epoch 20/60, Training Loss: 0.1292, Validation Loss: 0.1360\n",
      "[Trial 113] Epoch 58/60, Training Loss: 0.1259, Validation Loss: 0.1241\n",
      "[Trial 118] Epoch 24/60, Training Loss: 0.1270, Validation Loss: 0.1338\n",
      "[Trial 120] Epoch 6/60, Training Loss: 0.1292, Validation Loss: 0.1687\n",
      "[Trial 117] Epoch 19/60, Training Loss: 0.1281, Validation Loss: 0.1755\n",
      "[Trial 115] Epoch 45/60, Training Loss: 0.1260, Validation Loss: 0.1303\n",
      "[Trial 119] Epoch 21/60, Training Loss: 0.1292, Validation Loss: 0.1352\n",
      "[Trial 114] Epoch 49/60, Training Loss: 0.1264, Validation Loss: 0.1238\n",
      "[Trial 113] Epoch 59/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 118] Epoch 25/60, Training Loss: 0.1266, Validation Loss: 0.1325\n",
      "[Trial 120] Epoch 7/60, Training Loss: 0.1286, Validation Loss: 0.1611\n",
      "[Trial 119] Epoch 22/60, Training Loss: 0.1288, Validation Loss: 0.1349\n",
      "[Trial 117] Epoch 20/60, Training Loss: 0.1283, Validation Loss: 0.1727\n",
      "[Trial 115] Epoch 46/60, Training Loss: 0.1261, Validation Loss: 0.1298\n",
      "[Trial 114] Epoch 50/60, Training Loss: 0.1264, Validation Loss: 0.1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:02:31,642] Trial 113 finished with value: 0.12413602620363236 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.004873811620693466, 'batch_size': 64, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 113] Epoch 60/60, Training Loss: 0.1258, Validation Loss: 0.1241\n",
      "[Trial 118] Epoch 26/60, Training Loss: 0.1265, Validation Loss: 0.1327\n",
      "[Trial 119] Epoch 23/60, Training Loss: 0.1283, Validation Loss: 0.1335\n",
      "[Trial 120] Epoch 8/60, Training Loss: 0.1283, Validation Loss: 0.1565\n",
      "[Trial 117] Epoch 21/60, Training Loss: 0.1302, Validation Loss: 0.1690\n",
      "[Trial 115] Epoch 47/60, Training Loss: 0.1264, Validation Loss: 0.1292\n",
      "[Trial 118] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1323\n",
      "[Trial 114] Epoch 51/60, Training Loss: 0.1265, Validation Loss: 0.1237\n",
      "[Trial 119] Epoch 24/60, Training Loss: 0.1282, Validation Loss: 0.1322\n",
      "[Trial 120] Epoch 9/60, Training Loss: 0.1281, Validation Loss: 0.1495\n",
      "[Trial 117] Epoch 22/60, Training Loss: 0.1280, Validation Loss: 0.1674\n",
      "[Trial 118] Epoch 28/60, Training Loss: 0.1265, Validation Loss: 0.1330\n",
      "[Trial 115] Epoch 48/60, Training Loss: 0.1264, Validation Loss: 0.1288\n",
      "[Trial 119] Epoch 25/60, Training Loss: 0.1278, Validation Loss: 0.1315\n",
      "[Trial 114] Epoch 52/60, Training Loss: 0.1264, Validation Loss: 0.1237\n",
      "[Trial 120] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.1499\n",
      "[Trial 118] Epoch 29/60, Training Loss: 0.1281, Validation Loss: 0.1323\n",
      "[Trial 117] Epoch 23/60, Training Loss: 0.1281, Validation Loss: 0.1646\n",
      "[Trial 119] Epoch 26/60, Training Loss: 0.1277, Validation Loss: 0.1313\n",
      "[Trial 115] Epoch 49/60, Training Loss: 0.1260, Validation Loss: 0.1282\n",
      "[Trial 114] Epoch 53/60, Training Loss: 0.1266, Validation Loss: 0.1238\n",
      "[Trial 118] Epoch 30/60, Training Loss: 0.1352, Validation Loss: 0.1318\n",
      "[Trial 121] Epoch 1/60, Training Loss: 0.3864, Validation Loss: 0.1920\n",
      "[Trial 120] Epoch 11/60, Training Loss: 0.1284, Validation Loss: 0.1465\n",
      "[Trial 119] Epoch 27/60, Training Loss: 0.1274, Validation Loss: 0.1319\n",
      "[Trial 117] Epoch 24/60, Training Loss: 0.1271, Validation Loss: 0.1615\n",
      "[Trial 115] Epoch 50/60, Training Loss: 1.0792, Validation Loss: 2.5873\n",
      "[Trial 118] Epoch 31/60, Training Loss: 0.1262, Validation Loss: 0.1312\n",
      "[Trial 114] Epoch 54/60, Training Loss: 0.1265, Validation Loss: 0.1238\n",
      "[Trial 119] Epoch 28/60, Training Loss: 0.1275, Validation Loss: 0.1305\n",
      "[Trial 120] Epoch 12/60, Training Loss: 0.1276, Validation Loss: 0.1395\n",
      "[Trial 117] Epoch 25/60, Training Loss: 0.1279, Validation Loss: 0.1588\n",
      "[Trial 118] Epoch 32/60, Training Loss: 0.1260, Validation Loss: 0.1312\n",
      "[Trial 115] Epoch 51/60, Training Loss: 0.3922, Validation Loss: 0.1298\n",
      "[Trial 119] Epoch 29/60, Training Loss: 0.1274, Validation Loss: 0.1304\n",
      "[Trial 114] Epoch 55/60, Training Loss: 0.1262, Validation Loss: 0.1238\n",
      "[Trial 120] Epoch 13/60, Training Loss: 0.1278, Validation Loss: 0.1388\n",
      "[Trial 118] Epoch 33/60, Training Loss: 0.1271, Validation Loss: 0.1312\n",
      "[Trial 117] Epoch 26/60, Training Loss: 0.1280, Validation Loss: 0.1566\n",
      "[Trial 119] Epoch 30/60, Training Loss: 0.1273, Validation Loss: 0.1305\n",
      "[Trial 115] Epoch 52/60, Training Loss: 0.1262, Validation Loss: 0.1273\n",
      "[Trial 114] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1238\n",
      "[Trial 120] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1368\n",
      "[Trial 118] Epoch 34/60, Training Loss: 0.1281, Validation Loss: 0.1313\n",
      "[Trial 117] Epoch 27/60, Training Loss: 0.1298, Validation Loss: 0.1536\n",
      "[Trial 119] Epoch 31/60, Training Loss: 0.1274, Validation Loss: 0.1293\n",
      "[Trial 121] Epoch 2/60, Training Loss: 0.1289, Validation Loss: 0.1598\n",
      "[Trial 115] Epoch 53/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 118] Epoch 35/60, Training Loss: 0.1258, Validation Loss: 0.1308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:03:47,335] Trial 114 finished with value: 0.12371973792711893 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.003819869840934904, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 114] Epoch 57/60, Training Loss: 0.1260, Validation Loss: 0.1238\n",
      "[Trial 114] Early stopping after 57 epochs.\n",
      "[Trial 120] Epoch 15/60, Training Loss: 0.1276, Validation Loss: 0.1360\n",
      "[Trial 119] Epoch 32/60, Training Loss: 0.1272, Validation Loss: 0.1304\n",
      "[Trial 117] Epoch 28/60, Training Loss: 0.1271, Validation Loss: 0.1517\n",
      "[Trial 118] Epoch 36/60, Training Loss: 0.1257, Validation Loss: 0.1304\n",
      "[Trial 115] Epoch 54/60, Training Loss: 0.1259, Validation Loss: 0.1266\n",
      "[Trial 120] Epoch 16/60, Training Loss: 0.1281, Validation Loss: 0.1352\n",
      "[Trial 119] Epoch 33/60, Training Loss: 0.1270, Validation Loss: 0.1296\n",
      "[Trial 117] Epoch 29/60, Training Loss: 0.1270, Validation Loss: 0.1493\n",
      "[Trial 118] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1304\n",
      "[Trial 115] Epoch 55/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 119] Epoch 34/60, Training Loss: 0.1269, Validation Loss: 0.1292\n",
      "[Trial 120] Epoch 17/60, Training Loss: 0.1276, Validation Loss: 0.1327\n",
      "[Trial 118] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1305\n",
      "[Trial 117] Epoch 30/60, Training Loss: 0.1268, Validation Loss: 0.1471\n",
      "[Trial 119] Epoch 35/60, Training Loss: 0.1277, Validation Loss: 0.1290\n",
      "[Trial 115] Epoch 56/60, Training Loss: 0.1258, Validation Loss: 0.1264\n",
      "[Trial 120] Epoch 18/60, Training Loss: 0.1277, Validation Loss: 0.1328\n",
      "[Trial 118] Epoch 39/60, Training Loss: 0.1256, Validation Loss: 0.1304\n",
      "[Trial 121] Epoch 3/60, Training Loss: 0.1279, Validation Loss: 0.1451\n",
      "[Trial 117] Epoch 31/60, Training Loss: 0.1285, Validation Loss: 0.1451\n",
      "[Trial 119] Epoch 36/60, Training Loss: 0.1271, Validation Loss: 0.1283\n",
      "[Trial 115] Epoch 57/60, Training Loss: 0.1259, Validation Loss: 0.1262\n",
      "[Trial 118] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1303\n",
      "[Trial 120] Epoch 19/60, Training Loss: 0.1275, Validation Loss: 0.1303\n",
      "[Trial 122] Epoch 1/60, Training Loss: 0.3763, Validation Loss: 0.1758\n",
      "[Trial 117] Epoch 32/60, Training Loss: 0.1269, Validation Loss: 0.1438\n",
      "[Trial 119] Epoch 37/60, Training Loss: 0.1278, Validation Loss: 0.1289\n",
      "[Trial 118] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1303\n",
      "[Trial 115] Epoch 58/60, Training Loss: 0.1258, Validation Loss: 0.1260\n",
      "[Trial 120] Epoch 20/60, Training Loss: 0.1271, Validation Loss: 0.1289\n",
      "[Trial 119] Epoch 38/60, Training Loss: 0.1267, Validation Loss: 0.1286\n",
      "[Trial 117] Epoch 33/60, Training Loss: 0.1268, Validation Loss: 0.1418\n",
      "[Trial 118] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1303\n",
      "[Trial 120] Epoch 21/60, Training Loss: 0.1272, Validation Loss: 0.1279\n",
      "[Trial 115] Epoch 59/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 119] Epoch 39/60, Training Loss: 0.1270, Validation Loss: 0.1275\n",
      "[Trial 118] Epoch 43/60, Training Loss: 0.1256, Validation Loss: 0.1301\n",
      "[Trial 117] Epoch 34/60, Training Loss: 0.1279, Validation Loss: 0.1403\n",
      "[Trial 119] Epoch 40/60, Training Loss: 0.1270, Validation Loss: 0.1280\n",
      "[Trial 120] Epoch 22/60, Training Loss: 0.1277, Validation Loss: 0.1270\n",
      "[Trial 121] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:04:54,497] Trial 115 finished with value: 0.1258116657535235 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.0025886616441371762, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 115] Epoch 60/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 118] Epoch 44/60, Training Loss: 0.1256, Validation Loss: 0.1301\n",
      "[Trial 117] Epoch 35/60, Training Loss: 0.1267, Validation Loss: 0.1392\n",
      "[Trial 119] Epoch 41/60, Training Loss: 0.1271, Validation Loss: 0.1273\n",
      "[Trial 120] Epoch 23/60, Training Loss: 0.1284, Validation Loss: 0.1265\n",
      "[Trial 118] Epoch 45/60, Training Loss: 0.1255, Validation Loss: 0.1300\n",
      "[Trial 122] Epoch 2/60, Training Loss: 0.1287, Validation Loss: 0.1501\n",
      "[Trial 117] Epoch 36/60, Training Loss: 0.1281, Validation Loss: 0.1382\n",
      "[Trial 119] Epoch 42/60, Training Loss: 0.1287, Validation Loss: 0.1274\n",
      "[Trial 118] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1300\n",
      "[Trial 120] Epoch 24/60, Training Loss: 0.1269, Validation Loss: 0.1259\n",
      "[Trial 119] Epoch 43/60, Training Loss: 0.1262, Validation Loss: 0.1277\n",
      "[Trial 117] Epoch 37/60, Training Loss: 0.1280, Validation Loss: 0.1365\n",
      "[Trial 118] Epoch 47/60, Training Loss: 0.1256, Validation Loss: 0.1299\n",
      "[Trial 120] Epoch 25/60, Training Loss: 0.1270, Validation Loss: 0.1259\n",
      "[Trial 119] Epoch 44/60, Training Loss: 0.1268, Validation Loss: 0.1272\n",
      "[Trial 117] Epoch 38/60, Training Loss: 0.1265, Validation Loss: 0.1352\n",
      "[Trial 118] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1298\n",
      "[Trial 121] Epoch 5/60, Training Loss: 0.1269, Validation Loss: 0.1304\n",
      "[Trial 120] Epoch 26/60, Training Loss: 0.1271, Validation Loss: 0.1259\n",
      "[Trial 119] Epoch 45/60, Training Loss: 0.1274, Validation Loss: 0.1278\n",
      "[Trial 123] Epoch 1/60, Training Loss: 0.3310, Validation Loss: 0.1556\n",
      "[Trial 118] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1300\n",
      "[Trial 117] Epoch 39/60, Training Loss: 0.1267, Validation Loss: 0.1341\n",
      "[Trial 120] Epoch 27/60, Training Loss: 0.1274, Validation Loss: 0.1246\n",
      "[Trial 119] Epoch 46/60, Training Loss: 0.1268, Validation Loss: 0.1271\n",
      "[Trial 118] Epoch 50/60, Training Loss: 0.1255, Validation Loss: 0.1298\n",
      "[Trial 122] Epoch 3/60, Training Loss: 0.1277, Validation Loss: 0.1393\n",
      "[Trial 117] Epoch 40/60, Training Loss: 0.1282, Validation Loss: 0.1337\n",
      "[Trial 120] Epoch 28/60, Training Loss: 0.1269, Validation Loss: 0.1244\n",
      "[Trial 119] Epoch 47/60, Training Loss: 0.1262, Validation Loss: 0.1271\n",
      "[Trial 118] Epoch 51/60, Training Loss: 0.1255, Validation Loss: 0.1298\n",
      "[Trial 117] Epoch 41/60, Training Loss: 0.1263, Validation Loss: 0.1326\n",
      "[Trial 119] Epoch 48/60, Training Loss: 0.1347, Validation Loss: 0.1296\n",
      "[Trial 120] Epoch 29/60, Training Loss: 0.1269, Validation Loss: 0.1242\n",
      "[Trial 118] Epoch 52/60, Training Loss: 0.1254, Validation Loss: 0.1298\n",
      "[Trial 117] Epoch 42/60, Training Loss: 0.1263, Validation Loss: 0.1317\n",
      "[Trial 119] Epoch 49/60, Training Loss: 0.1288, Validation Loss: 0.1266\n",
      "[Trial 121] Epoch 6/60, Training Loss: 0.1267, Validation Loss: 0.1289\n",
      "[Trial 118] Epoch 53/60, Training Loss: 0.1255, Validation Loss: 0.1298\n",
      "[Trial 120] Epoch 30/60, Training Loss: 0.1272, Validation Loss: 0.1235\n",
      "[Trial 119] Epoch 50/60, Training Loss: 0.1261, Validation Loss: 0.1270\n",
      "[Trial 123] Epoch 2/60, Training Loss: 0.1271, Validation Loss: 0.1287\n",
      "[Trial 117] Epoch 43/60, Training Loss: 0.1269, Validation Loss: 0.1309\n",
      "[Trial 118] Epoch 54/60, Training Loss: 0.1254, Validation Loss: 0.1298\n",
      "[Trial 120] Epoch 31/60, Training Loss: 0.1268, Validation Loss: 0.1234\n",
      "[Trial 119] Epoch 51/60, Training Loss: 0.1261, Validation Loss: 0.1268\n",
      "[Trial 118] Epoch 55/60, Training Loss: 0.1253, Validation Loss: 0.1297\n",
      "[Trial 122] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1313\n",
      "[Trial 117] Epoch 44/60, Training Loss: 0.1268, Validation Loss: 0.1303\n",
      "[Trial 120] Epoch 32/60, Training Loss: 0.1275, Validation Loss: 0.1237\n",
      "[Trial 119] Epoch 52/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 118] Epoch 56/60, Training Loss: 0.1254, Validation Loss: 0.1297\n",
      "[Trial 117] Epoch 45/60, Training Loss: 0.1263, Validation Loss: 0.1299\n",
      "[Trial 120] Epoch 33/60, Training Loss: 0.1268, Validation Loss: 0.1238\n",
      "[Trial 119] Epoch 53/60, Training Loss: 0.1260, Validation Loss: 0.1264\n",
      "[Trial 118] Epoch 57/60, Training Loss: 0.1254, Validation Loss: 0.1296\n",
      "[Trial 121] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1268\n",
      "[Trial 117] Epoch 46/60, Training Loss: 0.1271, Validation Loss: 0.1295\n",
      "[Trial 120] Epoch 34/60, Training Loss: 0.1266, Validation Loss: 0.1234\n",
      "[Trial 119] Epoch 54/60, Training Loss: 0.1259, Validation Loss: 0.1265\n",
      "[Trial 118] Epoch 58/60, Training Loss: 0.1253, Validation Loss: 0.1297\n",
      "[Trial 117] Epoch 47/60, Training Loss: 0.1266, Validation Loss: 0.1287\n",
      "[Trial 120] Epoch 35/60, Training Loss: 0.1268, Validation Loss: 0.1230\n",
      "[Trial 123] Epoch 3/60, Training Loss: 0.1258, Validation Loss: 0.1285\n",
      "[Trial 119] Epoch 55/60, Training Loss: 0.1260, Validation Loss: 0.1264\n",
      "[Trial 118] Epoch 59/60, Training Loss: 0.1253, Validation Loss: 0.1296\n",
      "[Trial 117] Epoch 48/60, Training Loss: 0.1265, Validation Loss: 0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:06:56,021] Trial 118 finished with value: 0.12957590768734614 and parameters: {'hidden_dim': 64, 'latent_dim': 128, 'learning_rate': 0.0027971574726659257, 'batch_size': 64, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 119] Epoch 56/60, Training Loss: 0.1258, Validation Loss: 0.1264\n",
      "[Trial 118] Epoch 60/60, Training Loss: 0.1254, Validation Loss: 0.1296\n",
      "[Trial 120] Epoch 36/60, Training Loss: 0.1279, Validation Loss: 0.1230\n",
      "[Trial 122] Epoch 5/60, Training Loss: 0.1270, Validation Loss: 0.1289\n",
      "[Trial 119] Epoch 57/60, Training Loss: 0.1258, Validation Loss: 0.1265\n",
      "[Trial 117] Epoch 49/60, Training Loss: 0.1264, Validation Loss: 0.1278\n",
      "[Trial 120] Epoch 37/60, Training Loss: 0.1267, Validation Loss: 0.1234\n",
      "[Trial 121] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1263\n",
      "[Trial 119] Epoch 58/60, Training Loss: 0.1259, Validation Loss: 0.1263\n",
      "[Trial 117] Epoch 50/60, Training Loss: 0.1267, Validation Loss: 0.1275\n",
      "[Trial 120] Epoch 38/60, Training Loss: 0.1265, Validation Loss: 0.1234\n",
      "[Trial 119] Epoch 59/60, Training Loss: 0.1260, Validation Loss: 0.1262\n",
      "[Trial 117] Epoch 51/60, Training Loss: 0.1279, Validation Loss: 0.1269\n",
      "[Trial 120] Epoch 39/60, Training Loss: 0.1267, Validation Loss: 0.1230\n",
      "[Trial 123] Epoch 4/60, Training Loss: 0.1258, Validation Loss: 0.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:07:25,961] Trial 119 finished with value: 0.12617656340201697 and parameters: {'hidden_dim': 64, 'latent_dim': 128, 'learning_rate': 0.0026354472476095197, 'batch_size': 64, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 119] Epoch 60/60, Training Loss: 0.1257, Validation Loss: 0.1262\n",
      "[Trial 117] Epoch 52/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 120] Epoch 40/60, Training Loss: 0.1265, Validation Loss: 0.1231\n",
      "[Trial 124] Epoch 1/60, Training Loss: 1.1677, Validation Loss: 0.2176\n",
      "[Trial 122] Epoch 6/60, Training Loss: 0.1265, Validation Loss: 0.1273\n",
      "[Trial 120] Epoch 41/60, Training Loss: 0.1268, Validation Loss: 0.1229\n",
      "[Trial 117] Epoch 53/60, Training Loss: 0.1275, Validation Loss: 0.1265\n",
      "[Trial 121] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1257\n",
      "[Trial 120] Epoch 42/60, Training Loss: 0.1272, Validation Loss: 0.1227\n",
      "[Trial 117] Epoch 54/60, Training Loss: 0.1261, Validation Loss: 0.1263\n",
      "[Trial 120] Epoch 43/60, Training Loss: 0.1265, Validation Loss: 0.1227\n",
      "[Trial 117] Epoch 55/60, Training Loss: 0.1260, Validation Loss: 0.1261\n",
      "[Trial 125] Epoch 1/60, Training Loss: 0.3085, Validation Loss: 0.1451\n",
      "[Trial 123] Epoch 5/60, Training Loss: 0.1258, Validation Loss: 0.1285\n",
      "[Trial 120] Epoch 44/60, Training Loss: 0.1269, Validation Loss: 0.1229\n",
      "[Trial 117] Epoch 56/60, Training Loss: 0.1259, Validation Loss: 0.1260\n",
      "[Trial 124] Epoch 2/60, Training Loss: 0.1388, Validation Loss: 0.1938\n",
      "[Trial 122] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1297\n",
      "[Trial 120] Epoch 45/60, Training Loss: 0.1281, Validation Loss: 0.1225\n",
      "[Trial 117] Epoch 57/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 121] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1257\n",
      "[Trial 120] Epoch 46/60, Training Loss: 0.1264, Validation Loss: 0.1231\n",
      "[Trial 117] Epoch 58/60, Training Loss: 0.1295, Validation Loss: 0.1300\n",
      "[Trial 120] Epoch 47/60, Training Loss: 0.1264, Validation Loss: 0.1225\n",
      "[Trial 117] Epoch 59/60, Training Loss: 0.1266, Validation Loss: 0.1257\n",
      "[Trial 125] Epoch 2/60, Training Loss: 0.1280, Validation Loss: 0.1264\n",
      "[Trial 120] Epoch 48/60, Training Loss: 0.1266, Validation Loss: 0.1227\n",
      "[Trial 123] Epoch 6/60, Training Loss: 0.1256, Validation Loss: 0.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:08:38,387] Trial 117 finished with value: 0.12562121202548346 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.002643840254565049, 'batch_size': 64, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 117] Epoch 60/60, Training Loss: 0.1261, Validation Loss: 0.1256\n",
      "[Trial 124] Epoch 3/60, Training Loss: 0.1324, Validation Loss: 0.1772\n",
      "[Trial 120] Epoch 49/60, Training Loss: 0.1266, Validation Loss: 0.1227\n",
      "[Trial 122] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1264\n",
      "[Trial 121] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1256\n",
      "[Trial 120] Epoch 50/60, Training Loss: 0.1267, Validation Loss: 0.1224\n",
      "[Trial 120] Epoch 51/60, Training Loss: 0.1267, Validation Loss: 0.1225\n",
      "[Trial 120] Epoch 52/60, Training Loss: 0.1265, Validation Loss: 0.1224\n",
      "[Trial 125] Epoch 3/60, Training Loss: 0.1269, Validation Loss: 0.1262\n",
      "[Trial 124] Epoch 4/60, Training Loss: 0.1299, Validation Loss: 0.1680\n",
      "[Trial 123] Epoch 7/60, Training Loss: 0.1277, Validation Loss: 0.1282\n",
      "[Trial 126] Epoch 1/60, Training Loss: 0.2857, Validation Loss: 0.1469\n",
      "[Trial 120] Epoch 53/60, Training Loss: 0.1264, Validation Loss: 0.1226\n",
      "[Trial 121] Epoch 12/60, Training Loss: 0.1263, Validation Loss: 0.1255\n",
      "[Trial 122] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1266\n",
      "[Trial 120] Epoch 54/60, Training Loss: 0.1288, Validation Loss: 0.1242\n",
      "[Trial 120] Epoch 55/60, Training Loss: 0.1267, Validation Loss: 0.1225\n",
      "[Trial 120] Epoch 56/60, Training Loss: 0.1265, Validation Loss: 0.1225\n",
      "[Trial 125] Epoch 4/60, Training Loss: 0.1264, Validation Loss: 0.1257\n",
      "[Trial 124] Epoch 5/60, Training Loss: 0.1286, Validation Loss: 0.1570\n",
      "[Trial 126] Epoch 2/60, Training Loss: 0.1264, Validation Loss: 0.1293\n",
      "[Trial 123] Epoch 8/60, Training Loss: 0.1255, Validation Loss: 0.1288\n",
      "[Trial 120] Epoch 57/60, Training Loss: 0.1262, Validation Loss: 0.1226\n",
      "[Trial 121] Epoch 13/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 120] Epoch 58/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 122] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1259\n",
      "[Trial 120] Epoch 59/60, Training Loss: 0.1261, Validation Loss: 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:10:10,135] Trial 120 finished with value: 0.12236815889676413 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.0026064740272410797, 'batch_size': 64, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 120] Epoch 60/60, Training Loss: 0.1261, Validation Loss: 0.1224\n",
      "[Trial 125] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1256\n",
      "[Trial 124] Epoch 6/60, Training Loss: 0.1275, Validation Loss: 0.1490\n",
      "[Trial 126] Epoch 3/60, Training Loss: 0.1253, Validation Loss: 0.1288\n",
      "[Trial 123] Epoch 9/60, Training Loss: 0.1255, Validation Loss: 0.1282\n",
      "[Trial 121] Epoch 14/60, Training Loss: 0.1262, Validation Loss: 0.1254\n",
      "[Trial 122] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1259\n",
      "[Trial 127] Epoch 1/60, Training Loss: 0.3232, Validation Loss: 0.1575\n",
      "[Trial 125] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1256\n",
      "[Trial 124] Epoch 7/60, Training Loss: 0.1271, Validation Loss: 0.1429\n",
      "[Trial 126] Epoch 4/60, Training Loss: 0.1256, Validation Loss: 0.1297\n",
      "[Trial 121] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1281\n",
      "[Trial 123] Epoch 10/60, Training Loss: 0.1252, Validation Loss: 0.1286\n",
      "[Trial 122] Epoch 12/60, Training Loss: 0.1256, Validation Loss: 0.1256\n",
      "[Trial 127] Epoch 2/60, Training Loss: 0.1279, Validation Loss: 0.1283\n",
      "[Trial 124] Epoch 8/60, Training Loss: 0.1271, Validation Loss: 0.1419\n",
      "[Trial 125] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1256\n",
      "[Trial 121] Epoch 16/60, Training Loss: 0.1259, Validation Loss: 0.1254\n",
      "[Trial 126] Epoch 5/60, Training Loss: 0.1255, Validation Loss: 0.1288\n",
      "[Trial 123] Epoch 11/60, Training Loss: 0.1253, Validation Loss: 0.1282\n",
      "[Trial 122] Epoch 13/60, Training Loss: 0.1259, Validation Loss: 0.1260\n",
      "[Trial 127] Epoch 3/60, Training Loss: 0.1266, Validation Loss: 0.1266\n",
      "[Trial 121] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1254\n",
      "[Trial 124] Epoch 9/60, Training Loss: 0.1266, Validation Loss: 0.1362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:11:55,530] Trial 125 finished with value: 0.1255573496222496 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.007667486963166551, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 125] Epoch 8/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 125] Early stopping after 8 epochs.\n",
      "[Trial 126] Epoch 6/60, Training Loss: 0.1278, Validation Loss: 0.1290\n",
      "[Trial 123] Epoch 12/60, Training Loss: 0.1255, Validation Loss: 0.1282\n",
      "[Trial 122] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1260\n",
      "[Trial 128] Epoch 1/60, Training Loss: 0.6557, Validation Loss: 0.2303\n",
      "[Trial 127] Epoch 4/60, Training Loss: 0.1260, Validation Loss: 0.1265\n",
      "[Trial 121] Epoch 18/60, Training Loss: 0.1259, Validation Loss: 0.1255\n",
      "[Trial 124] Epoch 10/60, Training Loss: 0.1265, Validation Loss: 0.1346\n",
      "[Trial 126] Epoch 7/60, Training Loss: 0.1254, Validation Loss: 0.1290\n",
      "[Trial 128] Epoch 2/60, Training Loss: 0.1399, Validation Loss: 0.2156\n",
      "[Trial 123] Epoch 13/60, Training Loss: 0.1247, Validation Loss: 0.1281\n",
      "[Trial 122] Epoch 15/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 127] Epoch 5/60, Training Loss: 0.1263, Validation Loss: 0.1284\n",
      "[Trial 128] Epoch 3/60, Training Loss: 0.1336, Validation Loss: 0.2052\n",
      "[Trial 121] Epoch 19/60, Training Loss: 0.1259, Validation Loss: 0.1294\n",
      "[Trial 124] Epoch 11/60, Training Loss: 0.1270, Validation Loss: 0.1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:13:03,202] Trial 126 finished with value: 0.1287866100668907 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.007347411529326584, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 126] Epoch 8/60, Training Loss: 0.1248, Validation Loss: 0.1293\n",
      "[Trial 126] Early stopping after 8 epochs.\n",
      "[Trial 123] Epoch 14/60, Training Loss: 0.1252, Validation Loss: 0.1281\n",
      "[Trial 128] Epoch 4/60, Training Loss: 0.1310, Validation Loss: 0.1975\n",
      "[Trial 122] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 129] Epoch 1/60, Training Loss: 0.7229, Validation Loss: 0.2306\n",
      "[Trial 127] Epoch 6/60, Training Loss: 0.1260, Validation Loss: 0.1265\n",
      "[Trial 121] Epoch 20/60, Training Loss: 0.1262, Validation Loss: 0.1254\n",
      "[Trial 124] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1308\n",
      "[Trial 128] Epoch 5/60, Training Loss: 0.1299, Validation Loss: 0.1920\n",
      "[Trial 129] Epoch 2/60, Training Loss: 0.1409, Validation Loss: 0.2155\n",
      "[Trial 123] Epoch 15/60, Training Loss: 0.1248, Validation Loss: 0.1282\n",
      "[Trial 128] Epoch 6/60, Training Loss: 0.1287, Validation Loss: 0.1878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:13:53,071] Trial 122 finished with value: 0.12558420604715745 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.002734060493440931, 'batch_size': 8, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 122] Epoch 17/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 122] Early stopping after 17 epochs.\n",
      "[Trial 121] Epoch 21/60, Training Loss: 0.1253, Validation Loss: 0.1254\n",
      "[Trial 127] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 129] Epoch 3/60, Training Loss: 0.1346, Validation Loss: 0.2064\n",
      "[Trial 124] Epoch 13/60, Training Loss: 0.1261, Validation Loss: 0.1298\n",
      "[Trial 128] Epoch 7/60, Training Loss: 0.1281, Validation Loss: 0.1818\n",
      "[Trial 130] Epoch 1/60, Training Loss: 0.3561, Validation Loss: 0.2194\n",
      "[Trial 123] Epoch 16/60, Training Loss: 0.1254, Validation Loss: 0.1283\n",
      "[Trial 129] Epoch 4/60, Training Loss: 0.1313, Validation Loss: 0.1972\n",
      "[Trial 121] Epoch 22/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 127] Epoch 8/60, Training Loss: 0.1286, Validation Loss: 0.1268\n",
      "[Trial 128] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1753\n",
      "[Trial 130] Epoch 2/60, Training Loss: 0.1298, Validation Loss: 0.2003\n",
      "[Trial 124] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1292\n",
      "[Trial 129] Epoch 5/60, Training Loss: 0.1307, Validation Loss: 0.1915\n",
      "[Trial 128] Epoch 9/60, Training Loss: 0.1274, Validation Loss: 0.1721\n",
      "[Trial 123] Epoch 17/60, Training Loss: 0.1249, Validation Loss: 0.1282\n",
      "[Trial 130] Epoch 3/60, Training Loss: 0.1283, Validation Loss: 0.1774\n",
      "[Trial 121] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 129] Epoch 6/60, Training Loss: 0.1295, Validation Loss: 0.1874\n",
      "[Trial 127] Epoch 9/60, Training Loss: 0.1258, Validation Loss: 0.1268\n",
      "[Trial 124] Epoch 15/60, Training Loss: 0.1262, Validation Loss: 0.1313\n",
      "[Trial 128] Epoch 10/60, Training Loss: 0.1272, Validation Loss: 0.1676\n",
      "[Trial 130] Epoch 4/60, Training Loss: 0.1280, Validation Loss: 0.1613\n",
      "[Trial 129] Epoch 7/60, Training Loss: 0.1293, Validation Loss: 0.1799\n",
      "[Trial 123] Epoch 18/60, Training Loss: 0.1247, Validation Loss: 0.1282\n",
      "[Trial 121] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 128] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1638\n",
      "[Trial 130] Epoch 5/60, Training Loss: 0.1279, Validation Loss: 0.1526\n",
      "[Trial 127] Epoch 10/60, Training Loss: 0.1258, Validation Loss: 0.1265\n",
      "[Trial 129] Epoch 8/60, Training Loss: 0.1285, Validation Loss: 0.1757\n",
      "[Trial 124] Epoch 16/60, Training Loss: 0.1263, Validation Loss: 0.1273\n",
      "[Trial 128] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1594\n",
      "[Trial 130] Epoch 6/60, Training Loss: 0.1275, Validation Loss: 0.1473\n",
      "[Trial 129] Epoch 9/60, Training Loss: 0.1284, Validation Loss: 0.1699\n",
      "[Trial 121] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:16:00,852] Trial 123 finished with value: 0.1280738785242041 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.007269603644860026, 'batch_size': 8, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 123] Epoch 19/60, Training Loss: 0.1247, Validation Loss: 0.1282\n",
      "[Trial 123] Early stopping after 19 epochs.\n",
      "[Trial 127] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1267\n",
      "[Trial 128] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1564\n",
      "[Trial 130] Epoch 7/60, Training Loss: 0.1280, Validation Loss: 0.1401\n",
      "[Trial 124] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1282\n",
      "[Trial 129] Epoch 10/60, Training Loss: 0.1280, Validation Loss: 0.1647\n",
      "[Trial 131] Epoch 1/60, Training Loss: 0.3779, Validation Loss: 0.2157\n",
      "[Trial 128] Epoch 14/60, Training Loss: 0.1271, Validation Loss: 0.1551\n",
      "[Trial 121] Epoch 26/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 130] Epoch 8/60, Training Loss: 0.1270, Validation Loss: 0.1364\n",
      "[Trial 129] Epoch 11/60, Training Loss: 0.1278, Validation Loss: 0.1604\n",
      "[Trial 131] Epoch 2/60, Training Loss: 0.1300, Validation Loss: 0.1990\n",
      "[Trial 127] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1268\n",
      "[Trial 124] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1278\n",
      "[Trial 128] Epoch 15/60, Training Loss: 0.1271, Validation Loss: 0.1506\n",
      "[Trial 130] Epoch 9/60, Training Loss: 0.1270, Validation Loss: 0.1361\n",
      "[Trial 129] Epoch 12/60, Training Loss: 0.1285, Validation Loss: 0.1572\n",
      "[Trial 131] Epoch 3/60, Training Loss: 0.1281, Validation Loss: 0.1817\n",
      "[Trial 121] Epoch 27/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 128] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1484\n",
      "[Trial 127] Epoch 13/60, Training Loss: 0.1255, Validation Loss: 0.1267\n",
      "[Trial 130] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1319\n",
      "[Trial 129] Epoch 13/60, Training Loss: 0.1278, Validation Loss: 0.1520\n",
      "[Trial 124] Epoch 19/60, Training Loss: 0.1258, Validation Loss: 0.1266\n",
      "[Trial 131] Epoch 4/60, Training Loss: 0.1285, Validation Loss: 0.1708\n",
      "[Trial 128] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1465\n",
      "[Trial 121] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 130] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1282\n",
      "[Trial 129] Epoch 14/60, Training Loss: 0.1275, Validation Loss: 0.1498\n",
      "[Trial 131] Epoch 5/60, Training Loss: 0.1279, Validation Loss: 0.1588\n",
      "[Trial 127] Epoch 14/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 128] Epoch 18/60, Training Loss: 0.1262, Validation Loss: 0.1439\n",
      "[Trial 124] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1263\n",
      "[Trial 130] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1277\n",
      "[Trial 129] Epoch 15/60, Training Loss: 0.1276, Validation Loss: 0.1461\n",
      "[Trial 131] Epoch 6/60, Training Loss: 0.1276, Validation Loss: 0.1517\n",
      "[Trial 121] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 128] Epoch 19/60, Training Loss: 0.1268, Validation Loss: 0.1424\n",
      "[Trial 130] Epoch 13/60, Training Loss: 0.1268, Validation Loss: 0.1699\n",
      "[Trial 129] Epoch 16/60, Training Loss: 0.1278, Validation Loss: 0.1433\n",
      "[Trial 127] Epoch 15/60, Training Loss: 0.1253, Validation Loss: 0.1264\n",
      "[Trial 131] Epoch 7/60, Training Loss: 0.1273, Validation Loss: 0.1473\n",
      "[Trial 124] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1262\n",
      "[Trial 128] Epoch 20/60, Training Loss: 0.1264, Validation Loss: 0.1412\n",
      "[Trial 121] Epoch 30/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 130] Epoch 14/60, Training Loss: 0.1288, Validation Loss: 0.1267\n",
      "[Trial 129] Epoch 17/60, Training Loss: 0.1279, Validation Loss: 0.1392\n",
      "[Trial 131] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1429\n",
      "[Trial 128] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1397\n",
      "[Trial 127] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1266\n",
      "[Trial 129] Epoch 18/60, Training Loss: 0.1273, Validation Loss: 0.1386\n",
      "[Trial 130] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1260\n",
      "[Trial 124] Epoch 22/60, Training Loss: 0.1260, Validation Loss: 0.1261\n",
      "[Trial 121] Epoch 31/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 131] Epoch 9/60, Training Loss: 0.1273, Validation Loss: 0.1380\n",
      "[Trial 128] Epoch 22/60, Training Loss: 0.1268, Validation Loss: 0.1377\n",
      "[Trial 129] Epoch 19/60, Training Loss: 0.1275, Validation Loss: 0.1381\n",
      "[Trial 130] Epoch 16/60, Training Loss: 0.1264, Validation Loss: 0.1256\n",
      "[Trial 131] Epoch 10/60, Training Loss: 0.1271, Validation Loss: 0.1351\n",
      "[Trial 127] Epoch 17/60, Training Loss: 0.1255, Validation Loss: 0.1267\n",
      "[Trial 128] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1355\n",
      "[Trial 124] Epoch 23/60, Training Loss: 0.1258, Validation Loss: 0.1259\n",
      "[Trial 121] Epoch 32/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 129] Epoch 20/60, Training Loss: 0.1274, Validation Loss: 0.1399\n",
      "[Trial 130] Epoch 17/60, Training Loss: 0.1261, Validation Loss: 0.1260\n",
      "[Trial 131] Epoch 11/60, Training Loss: 0.1270, Validation Loss: 0.1307\n",
      "[Trial 128] Epoch 24/60, Training Loss: 0.1259, Validation Loss: 0.1353\n",
      "[Trial 129] Epoch 21/60, Training Loss: 0.1281, Validation Loss: 0.1326\n",
      "[Trial 130] Epoch 18/60, Training Loss: 0.1270, Validation Loss: 0.1250\n",
      "[Trial 127] Epoch 18/60, Training Loss: 0.1255, Validation Loss: 0.1266\n",
      "[Trial 124] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 121] Epoch 33/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 131] Epoch 12/60, Training Loss: 0.1272, Validation Loss: 0.1305\n",
      "[Trial 128] Epoch 25/60, Training Loss: 0.1265, Validation Loss: 0.1333\n",
      "[Trial 129] Epoch 22/60, Training Loss: 0.1270, Validation Loss: 0.1315\n",
      "[Trial 130] Epoch 19/60, Training Loss: 0.1263, Validation Loss: 0.1251\n",
      "[Trial 131] Epoch 13/60, Training Loss: 0.1263, Validation Loss: 0.1288\n",
      "[Trial 128] Epoch 26/60, Training Loss: 0.1260, Validation Loss: 0.1333\n",
      "[Trial 127] Epoch 19/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 121] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 124] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 129] Epoch 23/60, Training Loss: 0.1269, Validation Loss: 0.1303\n",
      "[Trial 130] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1270\n",
      "[Trial 128] Epoch 27/60, Training Loss: 0.1260, Validation Loss: 0.1330\n",
      "[Trial 131] Epoch 14/60, Training Loss: 0.1268, Validation Loss: 0.1283\n",
      "[Trial 129] Epoch 24/60, Training Loss: 0.1276, Validation Loss: 0.1297\n",
      "[Trial 130] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 121] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 127] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 128] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1317\n",
      "[Trial 131] Epoch 15/60, Training Loss: 0.1269, Validation Loss: 0.1273\n",
      "[Trial 124] Epoch 26/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 129] Epoch 25/60, Training Loss: 0.1270, Validation Loss: 0.1299\n",
      "[Trial 130] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1288\n",
      "[Trial 128] Epoch 29/60, Training Loss: 0.1258, Validation Loss: 0.1314\n",
      "[Trial 131] Epoch 16/60, Training Loss: 0.1262, Validation Loss: 0.1261\n",
      "[Trial 121] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 129] Epoch 26/60, Training Loss: 0.1276, Validation Loss: 0.1306\n",
      "[Trial 127] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 130] Epoch 23/60, Training Loss: 0.1266, Validation Loss: 0.1250\n",
      "[Trial 124] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1259\n",
      "[Trial 128] Epoch 30/60, Training Loss: 0.1259, Validation Loss: 0.1312\n",
      "[Trial 131] Epoch 17/60, Training Loss: 0.1268, Validation Loss: 0.1254\n",
      "[Trial 129] Epoch 27/60, Training Loss: 0.1271, Validation Loss: 0.1299\n",
      "[Trial 130] Epoch 24/60, Training Loss: 0.1265, Validation Loss: 0.1252\n",
      "[Trial 121] Epoch 37/60, Training Loss: 0.1252, Validation Loss: 0.1254\n",
      "[Trial 128] Epoch 31/60, Training Loss: 0.1262, Validation Loss: 0.1316\n",
      "[Trial 131] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1248\n",
      "[Trial 127] Epoch 22/60, Training Loss: 0.1252, Validation Loss: 0.1264\n",
      "[Trial 124] Epoch 28/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 129] Epoch 28/60, Training Loss: 0.1273, Validation Loss: 0.1275\n",
      "[Trial 130] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1248\n",
      "[Trial 128] Epoch 32/60, Training Loss: 0.1262, Validation Loss: 0.1289\n",
      "[Trial 131] Epoch 19/60, Training Loss: 0.1266, Validation Loss: 0.1247\n",
      "[Trial 121] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 129] Epoch 29/60, Training Loss: 0.1270, Validation Loss: 0.1273\n",
      "[Trial 130] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1249\n",
      "[Trial 127] Epoch 23/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 128] Epoch 33/60, Training Loss: 0.1259, Validation Loss: 0.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:22:45,479] Trial 124 finished with value: 0.12581136214236419 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.0004478536277728934, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 124] Epoch 29/60, Training Loss: 0.1257, Validation Loss: 0.1259\n",
      "[Trial 124] Early stopping after 29 epochs.\n",
      "[Trial 131] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1246\n",
      "[Trial 129] Epoch 30/60, Training Loss: 0.1268, Validation Loss: 0.1613\n",
      "[Trial 130] Epoch 27/60, Training Loss: 0.1270, Validation Loss: 0.1248\n",
      "[Trial 128] Epoch 34/60, Training Loss: 0.1257, Validation Loss: 0.1305\n",
      "[Trial 132] Epoch 1/60, Training Loss: 0.2663, Validation Loss: 0.1571\n",
      "[Trial 121] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 131] Epoch 21/60, Training Loss: 0.1265, Validation Loss: 0.1247\n",
      "[Trial 129] Epoch 31/60, Training Loss: 0.1293, Validation Loss: 0.1262\n",
      "[Trial 127] Epoch 24/60, Training Loss: 0.1254, Validation Loss: 0.1265\n",
      "[Trial 130] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1249\n",
      "[Trial 132] Epoch 2/60, Training Loss: 0.1278, Validation Loss: 0.1398\n",
      "[Trial 128] Epoch 35/60, Training Loss: 0.1259, Validation Loss: 0.1320\n",
      "[Trial 131] Epoch 22/60, Training Loss: 0.1268, Validation Loss: 0.1272\n",
      "[Trial 129] Epoch 32/60, Training Loss: 0.1268, Validation Loss: 0.1254\n",
      "[Trial 121] Epoch 40/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 130] Epoch 29/60, Training Loss: 0.1260, Validation Loss: 0.1250\n",
      "[Trial 132] Epoch 3/60, Training Loss: 0.1271, Validation Loss: 0.1364\n",
      "[Trial 128] Epoch 36/60, Training Loss: 0.1258, Validation Loss: 0.1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:23:49,761] Trial 127 finished with value: 0.12644067046542962 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.007572486961417125, 'batch_size': 8, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 127] Epoch 25/60, Training Loss: 0.1250, Validation Loss: 0.1265\n",
      "[Trial 127] Early stopping after 25 epochs.\n",
      "[Trial 131] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1244\n",
      "[Trial 129] Epoch 33/60, Training Loss: 0.1266, Validation Loss: 0.1261\n",
      "[Trial 132] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1328\n",
      "[Trial 130] Epoch 30/60, Training Loss: 0.1265, Validation Loss: 0.1249\n",
      "[Trial 128] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1287\n",
      "[Trial 121] Epoch 41/60, Training Loss: 0.1253, Validation Loss: 0.1254\n",
      "[Trial 133] Epoch 1/60, Training Loss: 0.2388, Validation Loss: 0.1752\n",
      "[Trial 131] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1243\n",
      "[Trial 129] Epoch 34/60, Training Loss: 0.1267, Validation Loss: 0.1252\n",
      "[Trial 132] Epoch 5/60, Training Loss: 0.1261, Validation Loss: 0.1307\n",
      "[Trial 130] Epoch 31/60, Training Loss: 0.1266, Validation Loss: 0.1249\n",
      "[Trial 128] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1290\n",
      "[Trial 133] Epoch 2/60, Training Loss: 0.1293, Validation Loss: 0.1489\n",
      "[Trial 131] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1260\n",
      "[Trial 129] Epoch 35/60, Training Loss: 0.1265, Validation Loss: 0.1251\n",
      "[Trial 121] Epoch 42/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 132] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1298\n",
      "[Trial 128] Epoch 39/60, Training Loss: 0.1259, Validation Loss: 0.1286\n",
      "[Trial 130] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1250\n",
      "[Trial 133] Epoch 3/60, Training Loss: 0.1285, Validation Loss: 0.1428\n",
      "[Trial 131] Epoch 26/60, Training Loss: 0.1268, Validation Loss: 0.1243\n",
      "[Trial 129] Epoch 36/60, Training Loss: 0.1264, Validation Loss: 0.1249\n",
      "[Trial 132] Epoch 7/60, Training Loss: 0.1258, Validation Loss: 0.1286\n",
      "[Trial 128] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1287\n",
      "[Trial 130] Epoch 33/60, Training Loss: 0.1269, Validation Loss: 0.1257\n",
      "[Trial 133] Epoch 4/60, Training Loss: 0.1273, Validation Loss: 0.1311\n",
      "[Trial 121] Epoch 43/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 131] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 129] Epoch 37/60, Training Loss: 0.1266, Validation Loss: 0.1291\n",
      "[Trial 132] Epoch 8/60, Training Loss: 0.1257, Validation Loss: 0.1310\n",
      "[Trial 128] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1277\n",
      "[Trial 130] Epoch 34/60, Training Loss: 0.1259, Validation Loss: 0.1248\n",
      "[Trial 133] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1290\n",
      "[Trial 131] Epoch 28/60, Training Loss: 0.1263, Validation Loss: 0.1243\n",
      "[Trial 129] Epoch 38/60, Training Loss: 0.1274, Validation Loss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:25:40,275] Trial 121 finished with value: 0.1253767877196272 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.002510016750439143, 'batch_size': 8, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 121] Epoch 44/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 121] Early stopping after 44 epochs.\n",
      "[Trial 132] Epoch 9/60, Training Loss: 0.1259, Validation Loss: 0.1276\n",
      "[Trial 128] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1278\n",
      "[Trial 133] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1275\n",
      "[Trial 130] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 131] Epoch 29/60, Training Loss: 0.1266, Validation Loss: 0.1243\n",
      "[Trial 129] Epoch 39/60, Training Loss: 0.1267, Validation Loss: 0.1242\n",
      "[Trial 134] Epoch 1/60, Training Loss: 0.2276, Validation Loss: 0.1857\n",
      "[Trial 132] Epoch 10/60, Training Loss: 0.1254, Validation Loss: 0.1276\n",
      "[Trial 128] Epoch 43/60, Training Loss: 0.1258, Validation Loss: 0.1276\n",
      "[Trial 133] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1276\n",
      "[Trial 130] Epoch 36/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 131] Epoch 30/60, Training Loss: 0.1261, Validation Loss: 0.1243\n",
      "[Trial 134] Epoch 2/60, Training Loss: 0.1287, Validation Loss: 0.1579\n",
      "[Trial 129] Epoch 40/60, Training Loss: 0.1264, Validation Loss: 0.1241\n",
      "[Trial 132] Epoch 11/60, Training Loss: 0.1257, Validation Loss: 0.1274\n",
      "[Trial 128] Epoch 44/60, Training Loss: 0.1256, Validation Loss: 0.1274\n",
      "[Trial 133] Epoch 8/60, Training Loss: 0.1267, Validation Loss: 0.1278\n",
      "[Trial 130] Epoch 37/60, Training Loss: 0.1259, Validation Loss: 0.1248\n",
      "[Trial 134] Epoch 3/60, Training Loss: 0.1273, Validation Loss: 0.1398\n",
      "[Trial 131] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1243\n",
      "[Trial 129] Epoch 41/60, Training Loss: 0.1269, Validation Loss: 0.1286\n",
      "[Trial 132] Epoch 12/60, Training Loss: 0.1254, Validation Loss: 0.1272\n",
      "[Trial 133] Epoch 9/60, Training Loss: 0.1262, Validation Loss: 0.1270\n",
      "[Trial 128] Epoch 45/60, Training Loss: 0.1257, Validation Loss: 0.1273\n",
      "[Trial 130] Epoch 38/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 134] Epoch 4/60, Training Loss: 0.1275, Validation Loss: 0.1337\n",
      "[Trial 131] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1243\n",
      "[Trial 129] Epoch 42/60, Training Loss: 0.1273, Validation Loss: 0.1245\n",
      "[Trial 132] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1273\n",
      "[Trial 133] Epoch 10/60, Training Loss: 0.1266, Validation Loss: 0.1263\n",
      "[Trial 128] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1272\n",
      "[Trial 130] Epoch 39/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 134] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1322\n",
      "[Trial 131] Epoch 33/60, Training Loss: 0.1260, Validation Loss: 0.1244\n",
      "[Trial 129] Epoch 43/60, Training Loss: 0.1268, Validation Loss: 0.1245\n",
      "[Trial 132] Epoch 14/60, Training Loss: 0.1256, Validation Loss: 0.1272\n",
      "[Trial 133] Epoch 11/60, Training Loss: 0.1259, Validation Loss: 0.1266\n",
      "[Trial 128] Epoch 47/60, Training Loss: 0.1260, Validation Loss: 0.1268\n",
      "[Trial 134] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1291\n",
      "[Trial 130] Epoch 40/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 131] Epoch 34/60, Training Loss: 0.1257, Validation Loss: 0.1242\n",
      "[Trial 129] Epoch 44/60, Training Loss: 0.1272, Validation Loss: 0.1466\n",
      "[Trial 132] Epoch 15/60, Training Loss: 0.1255, Validation Loss: 0.1287\n",
      "[Trial 133] Epoch 12/60, Training Loss: 0.1265, Validation Loss: 0.1282\n",
      "[Trial 128] Epoch 48/60, Training Loss: 0.1256, Validation Loss: 0.1277\n",
      "[Trial 134] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1286\n",
      "[Trial 130] Epoch 41/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 129] Epoch 45/60, Training Loss: 0.1274, Validation Loss: 0.1236\n",
      "[Trial 131] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1243\n",
      "[Trial 132] Epoch 16/60, Training Loss: 0.1295, Validation Loss: 0.1272\n",
      "[Trial 133] Epoch 13/60, Training Loss: 0.1271, Validation Loss: 0.1255\n",
      "[Trial 134] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1292\n",
      "[Trial 128] Epoch 49/60, Training Loss: 0.1257, Validation Loss: 0.1272\n",
      "[Trial 130] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 129] Epoch 46/60, Training Loss: 0.1265, Validation Loss: 0.1230\n",
      "[Trial 131] Epoch 36/60, Training Loss: 0.1256, Validation Loss: 0.1242\n",
      "[Trial 132] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1273\n",
      "[Trial 133] Epoch 14/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 134] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1280\n",
      "[Trial 128] Epoch 50/60, Training Loss: 0.1255, Validation Loss: 0.1267\n",
      "[Trial 130] Epoch 43/60, Training Loss: 0.1258, Validation Loss: 0.1248\n",
      "[Trial 129] Epoch 47/60, Training Loss: 0.1262, Validation Loss: 0.1231\n",
      "[Trial 132] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:28:40,289] Trial 131 finished with value: 0.12421973372499148 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.001952798319605836, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 131] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1242\n",
      "[Trial 131] Early stopping after 37 epochs.\n",
      "[Trial 134] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1320\n",
      "[Trial 133] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 128] Epoch 51/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 130] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 132] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1271\n",
      "[Trial 129] Epoch 48/60, Training Loss: 0.1265, Validation Loss: 0.1236\n",
      "[Trial 135] Epoch 1/60, Training Loss: 0.2157, Validation Loss: 0.1774\n",
      "[Trial 134] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1275\n",
      "[Trial 133] Epoch 16/60, Training Loss: 0.1260, Validation Loss: 0.1256\n",
      "[Trial 128] Epoch 52/60, Training Loss: 0.1258, Validation Loss: 0.1262\n",
      "[Trial 130] Epoch 45/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 132] Epoch 20/60, Training Loss: 0.1253, Validation Loss: 0.1272\n",
      "[Trial 129] Epoch 49/60, Training Loss: 0.1268, Validation Loss: 0.1235\n",
      "[Trial 134] Epoch 12/60, Training Loss: 0.1259, Validation Loss: 0.1279\n",
      "[Trial 135] Epoch 2/60, Training Loss: 0.1303, Validation Loss: 0.1525\n",
      "[Trial 133] Epoch 17/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 128] Epoch 53/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 130] Epoch 46/60, Training Loss: 0.1253, Validation Loss: 0.1247\n",
      "[Trial 132] Epoch 21/60, Training Loss: 0.1252, Validation Loss: 0.1271\n",
      "[Trial 129] Epoch 50/60, Training Loss: 0.1262, Validation Loss: 0.1228\n",
      "[Trial 134] Epoch 13/60, Training Loss: 0.1259, Validation Loss: 0.1274\n",
      "[Trial 133] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 135] Epoch 3/60, Training Loss: 0.1293, Validation Loss: 0.1496\n",
      "[Trial 128] Epoch 54/60, Training Loss: 0.1257, Validation Loss: 0.1273\n",
      "[Trial 130] Epoch 47/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 132] Epoch 22/60, Training Loss: 0.1253, Validation Loss: 0.1272\n",
      "[Trial 134] Epoch 14/60, Training Loss: 0.1254, Validation Loss: 0.1303\n",
      "[Trial 129] Epoch 51/60, Training Loss: 0.1262, Validation Loss: 0.1232\n",
      "[Trial 133] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1253\n",
      "[Trial 135] Epoch 4/60, Training Loss: 0.1288, Validation Loss: 0.1396\n",
      "[Trial 128] Epoch 55/60, Training Loss: 0.1254, Validation Loss: 0.1263\n",
      "[Trial 130] Epoch 48/60, Training Loss: 0.1259, Validation Loss: 0.1248\n",
      "[Trial 134] Epoch 15/60, Training Loss: 0.1261, Validation Loss: 0.1270\n",
      "[Trial 132] Epoch 23/60, Training Loss: 0.1249, Validation Loss: 0.1271\n",
      "[Trial 129] Epoch 52/60, Training Loss: 0.1267, Validation Loss: 0.1231\n",
      "[Trial 133] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1276\n",
      "[Trial 128] Epoch 56/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 135] Epoch 5/60, Training Loss: 0.1276, Validation Loss: 0.1374\n",
      "[Trial 134] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1270\n",
      "[Trial 130] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 132] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1272\n",
      "[Trial 129] Epoch 53/60, Training Loss: 0.1265, Validation Loss: 0.1227\n",
      "[Trial 133] Epoch 21/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 128] Epoch 57/60, Training Loss: 0.1255, Validation Loss: 0.1263\n",
      "[Trial 135] Epoch 6/60, Training Loss: 0.1268, Validation Loss: 0.1336\n",
      "[Trial 134] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1270\n",
      "[Trial 132] Epoch 25/60, Training Loss: 0.1254, Validation Loss: 0.1271\n",
      "[Trial 130] Epoch 50/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 129] Epoch 54/60, Training Loss: 0.1264, Validation Loss: 0.1230\n",
      "[Trial 133] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 128] Epoch 58/60, Training Loss: 0.1264, Validation Loss: 0.1262\n",
      "[Trial 135] Epoch 7/60, Training Loss: 0.1270, Validation Loss: 0.1338\n",
      "[Trial 134] Epoch 18/60, Training Loss: 0.1253, Validation Loss: 0.1270\n",
      "[Trial 132] Epoch 26/60, Training Loss: 0.1249, Validation Loss: 0.1271\n",
      "[Trial 130] Epoch 51/60, Training Loss: 0.1259, Validation Loss: 0.1248\n",
      "[Trial 129] Epoch 55/60, Training Loss: 0.1265, Validation Loss: 0.1233\n",
      "[Trial 133] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 128] Epoch 59/60, Training Loss: 0.1253, Validation Loss: 0.1263\n",
      "[Trial 134] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1275\n",
      "[Trial 135] Epoch 8/60, Training Loss: 0.1269, Validation Loss: 0.1302\n",
      "[Trial 132] Epoch 27/60, Training Loss: 0.1248, Validation Loss: 0.1271\n",
      "[Trial 129] Epoch 56/60, Training Loss: 0.1267, Validation Loss: 0.1232\n",
      "[Trial 130] Epoch 52/60, Training Loss: 0.1259, Validation Loss: 0.1248\n",
      "[Trial 133] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:31:50,457] Trial 128 finished with value: 0.1261104464530945 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.0004491336913871609, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 128] Epoch 60/60, Training Loss: 0.1253, Validation Loss: 0.1261\n",
      "[Trial 134] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1267\n",
      "[Trial 132] Epoch 28/60, Training Loss: 0.1250, Validation Loss: 0.1271\n",
      "[Trial 135] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1280\n",
      "[Trial 129] Epoch 57/60, Training Loss: 0.1267, Validation Loss: 0.1232\n",
      "[Trial 130] Epoch 53/60, Training Loss: 0.1254, Validation Loss: 0.1248\n",
      "[Trial 136] Epoch 1/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 133] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 134] Epoch 21/60, Training Loss: 0.1255, Validation Loss: 0.1267\n",
      "[Trial 136] Epoch 2/60, Training Loss: nan, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:32:19,033] Trial 132 finished with value: 0.12707609807451567 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0037876368758864598, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 132] Epoch 29/60, Training Loss: 0.1250, Validation Loss: 0.1271\n",
      "[Trial 132] Early stopping after 29 epochs.\n",
      "[Trial 135] Epoch 10/60, Training Loss: 0.1264, Validation Loss: 0.1280\n",
      "[Trial 129] Epoch 58/60, Training Loss: 0.1265, Validation Loss: 0.1229\n",
      "[Trial 130] Epoch 54/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 133] Epoch 26/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 136] Epoch 3/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 134] Epoch 22/60, Training Loss: 0.1253, Validation Loss: 0.1267\n",
      "[Trial 137] Epoch 1/60, Training Loss: 2.4055, Validation Loss: 0.1909\n",
      "[Trial 136] Epoch 4/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 129] Epoch 59/60, Training Loss: 0.1262, Validation Loss: 0.1227\n",
      "[Trial 135] Epoch 11/60, Training Loss: 0.1263, Validation Loss: 0.1286\n",
      "[Trial 137] Epoch 2/60, Training Loss: 0.1294, Validation Loss: 0.1612\n",
      "[Trial 130] Epoch 55/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 133] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 134] Epoch 23/60, Training Loss: 0.1251, Validation Loss: 0.1267\n",
      "[Trial 136] Epoch 5/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 137] Epoch 3/60, Training Loss: 0.1281, Validation Loss: 0.1440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:33:05,057] Trial 129 finished with value: 0.12268291028837362 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.0004397555337687042, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 129] Epoch 60/60, Training Loss: 0.1264, Validation Loss: 0.1230\n",
      "[Trial 133] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:33:08,106] Trial 130 finished with value: 0.12474973884721598 and parameters: {'hidden_dim': 192, 'latent_dim': 64, 'learning_rate': 0.002006192874091039, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 130] Epoch 56/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 130] Early stopping after 56 epochs.\n",
      "[Trial 135] Epoch 12/60, Training Loss: 0.1265, Validation Loss: 0.1267\n",
      "[Trial 136] Epoch 6/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 137] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1368\n",
      "[Trial 134] Epoch 24/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 136] Epoch 7/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 137] Epoch 5/60, Training Loss: 0.1265, Validation Loss: 0.1332\n",
      "[Trial 138] Epoch 1/60, Training Loss: 0.3159, Validation Loss: 0.2111\n",
      "[Trial 133] Epoch 29/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 139] Epoch 1/60, Training Loss: 0.2260, Validation Loss: 0.1907\n",
      "[Trial 135] Epoch 13/60, Training Loss: 0.1264, Validation Loss: 0.1271\n",
      "[Trial 134] Epoch 25/60, Training Loss: 0.1249, Validation Loss: 0.1266\n",
      "[Trial 136] Epoch 8/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 137] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:33:49,282] Trial 136 finished with value: inf and parameters: {'hidden_dim': 320, 'latent_dim': 32, 'learning_rate': 0.03001501752158105, 'batch_size': 32, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 136] Epoch 9/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 136] Early stopping after 9 epochs.\n",
      "[Trial 138] Epoch 2/60, Training Loss: 0.1308, Validation Loss: 0.1999\n",
      "[Trial 133] Epoch 30/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 137] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1300\n",
      "[Trial 134] Epoch 26/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 139] Epoch 2/60, Training Loss: 0.1306, Validation Loss: 0.1641\n",
      "[Trial 135] Epoch 14/60, Training Loss: 0.1263, Validation Loss: 0.1262\n",
      "[Trial 137] Epoch 8/60, Training Loss: 0.1261, Validation Loss: 0.1300\n",
      "[Trial 133] Epoch 31/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 140] Epoch 1/60, Training Loss: 1.3544, Validation Loss: 0.1580\n",
      "[Trial 138] Epoch 3/60, Training Loss: 0.1289, Validation Loss: 0.1910\n",
      "[Trial 134] Epoch 27/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 139] Epoch 3/60, Training Loss: 0.1288, Validation Loss: 0.1483\n",
      "[Trial 137] Epoch 9/60, Training Loss: 0.1262, Validation Loss: 0.1295\n",
      "[Trial 135] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1255\n",
      "[Trial 137] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1295\n",
      "[Trial 133] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 134] Epoch 28/60, Training Loss: 0.1252, Validation Loss: 0.1267\n",
      "[Trial 140] Epoch 2/60, Training Loss: 0.1286, Validation Loss: 0.1386\n",
      "[Trial 138] Epoch 4/60, Training Loss: 0.1284, Validation Loss: 0.1790\n",
      "[Trial 139] Epoch 4/60, Training Loss: 0.1282, Validation Loss: 0.1412\n",
      "[Trial 135] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1262\n",
      "[Trial 137] Epoch 11/60, Training Loss: 0.1257, Validation Loss: 0.1299\n",
      "[Trial 134] Epoch 29/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 133] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 140] Epoch 3/60, Training Loss: 0.1277, Validation Loss: 0.1327\n",
      "[Trial 137] Epoch 12/60, Training Loss: 0.1257, Validation Loss: 0.1290\n",
      "[Trial 138] Epoch 5/60, Training Loss: 0.1281, Validation Loss: 0.1854\n",
      "[Trial 139] Epoch 5/60, Training Loss: 0.1279, Validation Loss: 0.1350\n",
      "[Trial 135] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1252\n",
      "[Trial 137] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1287\n",
      "[Trial 134] Epoch 30/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 133] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 140] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1303\n",
      "[Trial 138] Epoch 6/60, Training Loss: 0.1281, Validation Loss: 0.1627\n",
      "[Trial 137] Epoch 14/60, Training Loss: 0.1256, Validation Loss: 0.1289\n",
      "[Trial 139] Epoch 6/60, Training Loss: 0.1271, Validation Loss: 0.1332\n",
      "[Trial 135] Epoch 18/60, Training Loss: 0.1261, Validation Loss: 0.1252\n",
      "[Trial 134] Epoch 31/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 133] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 137] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1281\n",
      "[Trial 140] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1272\n",
      "[Trial 138] Epoch 7/60, Training Loss: 0.1273, Validation Loss: 0.1548\n",
      "[Trial 139] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1330\n",
      "[Trial 137] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1278\n",
      "[Trial 135] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 134] Epoch 32/60, Training Loss: 0.1251, Validation Loss: 0.1266\n",
      "[Trial 133] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 137] Epoch 17/60, Training Loss: 0.1255, Validation Loss: 0.1278\n",
      "[Trial 140] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1261\n",
      "[Trial 138] Epoch 8/60, Training Loss: 0.1274, Validation Loss: 0.1497\n",
      "[Trial 139] Epoch 8/60, Training Loss: 0.1268, Validation Loss: 0.1304\n",
      "[Trial 134] Epoch 33/60, Training Loss: 0.1251, Validation Loss: 0.1266\n",
      "[Trial 137] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1277\n",
      "[Trial 135] Epoch 20/60, Training Loss: 0.1260, Validation Loss: 0.1246\n",
      "[Trial 133] Epoch 37/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 140] Epoch 7/60, Training Loss: 0.1263, Validation Loss: 0.1255\n",
      "[Trial 138] Epoch 9/60, Training Loss: 0.1274, Validation Loss: 0.1457\n",
      "[Trial 137] Epoch 19/60, Training Loss: 0.1255, Validation Loss: 0.1284\n",
      "[Trial 139] Epoch 9/60, Training Loss: 0.1269, Validation Loss: 0.1280\n",
      "[Trial 134] Epoch 34/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 135] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 133] Epoch 38/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 137] Epoch 20/60, Training Loss: 0.1253, Validation Loss: 0.1277\n",
      "[Trial 140] Epoch 8/60, Training Loss: 0.1261, Validation Loss: 0.1254\n",
      "[Trial 138] Epoch 10/60, Training Loss: 0.1276, Validation Loss: 0.1447\n",
      "[Trial 134] Epoch 35/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 137] Epoch 21/60, Training Loss: 0.1252, Validation Loss: 0.1275\n",
      "[Trial 139] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1285\n",
      "[Trial 133] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 135] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1250\n",
      "[Trial 137] Epoch 22/60, Training Loss: 0.1259, Validation Loss: 0.1311\n",
      "[Trial 140] Epoch 9/60, Training Loss: 0.1266, Validation Loss: 0.1252\n",
      "[Trial 138] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:37:13,039] Trial 134 finished with value: 0.12655464497705302 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0019318440243979889, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 134] Epoch 36/60, Training Loss: 0.1251, Validation Loss: 0.1266\n",
      "[Trial 134] Early stopping after 36 epochs.\n",
      "[Trial 139] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1269\n",
      "[Trial 133] Epoch 40/60, Training Loss: 0.1252, Validation Loss: 0.1250\n",
      "[Trial 137] Epoch 23/60, Training Loss: 0.1412, Validation Loss: 0.1277\n",
      "[Trial 135] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1247\n",
      "[Trial 140] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:37:33,470] Trial 137 finished with value: 0.12753269001841544 and parameters: {'hidden_dim': 320, 'latent_dim': 32, 'learning_rate': 0.0035803042878976552, 'batch_size': 32, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 137] Epoch 24/60, Training Loss: 0.1251, Validation Loss: 0.1276\n",
      "[Trial 137] Early stopping after 24 epochs.\n",
      "[Trial 138] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1344\n",
      "[Trial 141] Epoch 1/60, Training Loss: 1.1277, Validation Loss: 0.1706\n",
      "[Trial 139] Epoch 12/60, Training Loss: 0.1265, Validation Loss: 0.1272\n",
      "[Trial 133] Epoch 41/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 135] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1248\n",
      "[Trial 140] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 1/60, Training Loss: 0.3022, Validation Loss: 0.2189\n",
      "[Trial 141] Epoch 2/60, Training Loss: 0.1283, Validation Loss: 0.1453\n",
      "[Trial 138] Epoch 13/60, Training Loss: 0.1267, Validation Loss: 0.1328\n",
      "[Trial 133] Epoch 42/60, Training Loss: 0.1252, Validation Loss: 0.1251\n",
      "[Trial 139] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1263\n",
      "[Trial 135] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1251\n",
      "[Trial 140] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 2/60, Training Loss: 0.1313, Validation Loss: 0.2092\n",
      "[Trial 141] Epoch 3/60, Training Loss: 0.1271, Validation Loss: 0.1360\n",
      "[Trial 138] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1306\n",
      "[Trial 133] Epoch 43/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 139] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1258\n",
      "[Trial 135] Epoch 26/60, Training Loss: 0.1256, Validation Loss: 0.1246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:38:37,142] Trial 140 finished with value: 0.1251822610696157 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.005962198078720498, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 140] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1252\n",
      "[Trial 140] Early stopping after 13 epochs.\n",
      "[Trial 142] Epoch 3/60, Training Loss: 0.1288, Validation Loss: 0.1969\n",
      "[Trial 141] Epoch 4/60, Training Loss: 0.1267, Validation Loss: 0.1298\n",
      "[Trial 138] Epoch 15/60, Training Loss: 0.1270, Validation Loss: 0.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:38:43,059] Trial 133 finished with value: 0.1250444715221723 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.002006272394287078, 'batch_size': 16, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 133] Epoch 44/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 133] Early stopping after 44 epochs.\n",
      "[Trial 139] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 135] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1247\n",
      "[Trial 143] Epoch 1/60, Training Loss: 0.3180, Validation Loss: 0.2130\n",
      "[Trial 142] Epoch 4/60, Training Loss: 0.1278, Validation Loss: 0.1875\n",
      "[Trial 141] Epoch 5/60, Training Loss: 0.1265, Validation Loss: 0.1279\n",
      "[Trial 138] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1284\n",
      "[Trial 144] Epoch 1/60, Training Loss: 0.2917, Validation Loss: 0.2261\n",
      "[Trial 139] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1256\n",
      "[Trial 135] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1246\n",
      "[Trial 141] Epoch 6/60, Training Loss: 0.1258, Validation Loss: 0.1261\n",
      "[Trial 143] Epoch 2/60, Training Loss: 0.1291, Validation Loss: 0.1998\n",
      "[Trial 142] Epoch 5/60, Training Loss: 0.1275, Validation Loss: 0.1739\n",
      "[Trial 138] Epoch 17/60, Training Loss: 0.1266, Validation Loss: 0.1276\n",
      "[Trial 144] Epoch 2/60, Training Loss: 0.1313, Validation Loss: 0.2203\n",
      "[Trial 139] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 135] Epoch 29/60, Training Loss: 0.1258, Validation Loss: 0.1247\n",
      "[Trial 141] Epoch 7/60, Training Loss: 0.1262, Validation Loss: 0.1260\n",
      "[Trial 142] Epoch 6/60, Training Loss: 0.1272, Validation Loss: 0.1667\n",
      "[Trial 143] Epoch 3/60, Training Loss: 0.1281, Validation Loss: 0.1924\n",
      "[Trial 138] Epoch 18/60, Training Loss: 0.1265, Validation Loss: 0.1266\n",
      "[Trial 144] Epoch 3/60, Training Loss: 0.1289, Validation Loss: 0.2031\n",
      "[Trial 139] Epoch 18/60, Training Loss: 0.1263, Validation Loss: 0.1256\n",
      "[Trial 141] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1258\n",
      "[Trial 142] Epoch 7/60, Training Loss: 0.1278, Validation Loss: 0.1611\n",
      "[Trial 135] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 4/60, Training Loss: 0.1275, Validation Loss: 0.1803\n",
      "[Trial 138] Epoch 19/60, Training Loss: 0.1289, Validation Loss: 0.1257\n",
      "[Trial 144] Epoch 4/60, Training Loss: 0.1284, Validation Loss: 0.1938\n",
      "[Trial 139] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1253\n",
      "[Trial 141] Epoch 9/60, Training Loss: 0.1262, Validation Loss: 0.1259\n",
      "[Trial 142] Epoch 8/60, Training Loss: 0.1269, Validation Loss: 0.1529\n",
      "[Trial 135] Epoch 31/60, Training Loss: 0.1255, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1695\n",
      "[Trial 138] Epoch 20/60, Training Loss: 0.1264, Validation Loss: 0.1258\n",
      "[Trial 144] Epoch 5/60, Training Loss: 0.1287, Validation Loss: 0.1759\n",
      "[Trial 139] Epoch 20/60, Training Loss: 0.1261, Validation Loss: 0.1259\n",
      "[Trial 141] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1388\n",
      "[Trial 142] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1455\n",
      "[Trial 143] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1604\n",
      "[Trial 135] Epoch 32/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 144] Epoch 6/60, Training Loss: 0.1277, Validation Loss: 0.1669\n",
      "[Trial 138] Epoch 21/60, Training Loss: 0.1266, Validation Loss: 0.1257\n",
      "[Trial 139] Epoch 21/60, Training Loss: 0.1262, Validation Loss: 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:41:08,956] Trial 141 finished with value: 0.12582589412728945 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.0055678863570863225, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 141] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1259\n",
      "[Trial 141] Early stopping after 11 epochs.\n",
      "[Trial 142] Epoch 10/60, Training Loss: 0.1268, Validation Loss: 0.1406\n",
      "[Trial 143] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1546\n",
      "[Trial 135] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1246\n",
      "[Trial 144] Epoch 7/60, Training Loss: 0.1280, Validation Loss: 0.1597\n",
      "[Trial 138] Epoch 22/60, Training Loss: 0.1267, Validation Loss: 0.1249\n",
      "[Trial 139] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 145] Epoch 1/60, Training Loss: 0.3085, Validation Loss: 0.2142\n",
      "[Trial 142] Epoch 11/60, Training Loss: 0.1263, Validation Loss: 0.1378\n",
      "[Trial 143] Epoch 8/60, Training Loss: 0.1261, Validation Loss: 0.1505\n",
      "[Trial 144] Epoch 8/60, Training Loss: 0.1274, Validation Loss: 0.1504\n",
      "[Trial 135] Epoch 34/60, Training Loss: 0.1256, Validation Loss: 0.1246\n",
      "[Trial 138] Epoch 23/60, Training Loss: 0.1264, Validation Loss: 0.1249\n",
      "[Trial 139] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1257\n",
      "[Trial 145] Epoch 2/60, Training Loss: 0.1306, Validation Loss: 0.1988\n",
      "[Trial 142] Epoch 12/60, Training Loss: 0.1267, Validation Loss: 0.1409\n",
      "[Trial 143] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1465\n",
      "[Trial 144] Epoch 9/60, Training Loss: 0.1273, Validation Loss: 0.1465\n",
      "[Trial 138] Epoch 24/60, Training Loss: 0.1264, Validation Loss: 0.1245\n",
      "[Trial 135] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 24/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 145] Epoch 3/60, Training Loss: 0.1294, Validation Loss: 0.1882\n",
      "[Trial 142] Epoch 13/60, Training Loss: 0.1268, Validation Loss: 0.1353\n",
      "[Trial 143] Epoch 10/60, Training Loss: 0.1264, Validation Loss: 0.1401\n",
      "[Trial 144] Epoch 10/60, Training Loss: 0.1273, Validation Loss: 0.1392\n",
      "[Trial 138] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 135] Epoch 36/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 25/60, Training Loss: 0.1259, Validation Loss: 0.1255\n",
      "[Trial 145] Epoch 4/60, Training Loss: 0.1277, Validation Loss: 0.1751\n",
      "[Trial 142] Epoch 14/60, Training Loss: 0.1260, Validation Loss: 0.1321\n",
      "[Trial 143] Epoch 11/60, Training Loss: 0.1257, Validation Loss: 0.1380\n",
      "[Trial 144] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1361\n",
      "[Trial 138] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1243\n",
      "[Trial 135] Epoch 37/60, Training Loss: 0.1260, Validation Loss: 0.1246\n",
      "[Trial 145] Epoch 5/60, Training Loss: 0.1281, Validation Loss: 0.1727\n",
      "[Trial 139] Epoch 26/60, Training Loss: 0.1261, Validation Loss: 0.1255\n",
      "[Trial 142] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1306\n",
      "[Trial 143] Epoch 12/60, Training Loss: 0.1260, Validation Loss: 0.1356\n",
      "[Trial 144] Epoch 12/60, Training Loss: 0.1266, Validation Loss: 0.1313\n",
      "[Trial 138] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1267\n",
      "[Trial 135] Epoch 38/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 6/60, Training Loss: 0.1274, Validation Loss: 0.1627\n",
      "[Trial 139] Epoch 27/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 142] Epoch 16/60, Training Loss: 0.1260, Validation Loss: 0.1306\n",
      "[Trial 143] Epoch 13/60, Training Loss: 0.1253, Validation Loss: 0.1343\n",
      "[Trial 144] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1310\n",
      "[Trial 138] Epoch 28/60, Training Loss: 0.1268, Validation Loss: 0.1241\n",
      "[Trial 135] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1246\n",
      "[Trial 145] Epoch 7/60, Training Loss: 0.1272, Validation Loss: 0.1556\n",
      "[Trial 139] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 17/60, Training Loss: 0.1264, Validation Loss: 0.1313\n",
      "[Trial 143] Epoch 14/60, Training Loss: 0.1254, Validation Loss: 0.1326\n",
      "[Trial 144] Epoch 14/60, Training Loss: 0.1266, Validation Loss: 0.1288\n",
      "[Trial 138] Epoch 29/60, Training Loss: 0.1262, Validation Loss: 0.1247\n",
      "[Trial 135] Epoch 40/60, Training Loss: 0.1254, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 8/60, Training Loss: 0.1276, Validation Loss: 0.1467\n",
      "[Trial 139] Epoch 29/60, Training Loss: 0.1257, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 18/60, Training Loss: 0.1261, Validation Loss: 0.1293\n",
      "[Trial 143] Epoch 15/60, Training Loss: 0.1251, Validation Loss: 0.1319\n",
      "[Trial 144] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1273\n",
      "[Trial 138] Epoch 30/60, Training Loss: 0.1267, Validation Loss: 0.1243\n",
      "[Trial 135] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 9/60, Training Loss: 0.1271, Validation Loss: 0.1417\n",
      "[Trial 139] Epoch 30/60, Training Loss: 0.1261, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 19/60, Training Loss: 0.1260, Validation Loss: 0.1302\n",
      "[Trial 143] Epoch 16/60, Training Loss: 0.1252, Validation Loss: 0.1308\n",
      "[Trial 144] Epoch 16/60, Training Loss: 0.1264, Validation Loss: 0.1278\n",
      "[Trial 138] Epoch 31/60, Training Loss: 0.1265, Validation Loss: 0.1241\n",
      "[Trial 145] Epoch 10/60, Training Loss: 0.1265, Validation Loss: 0.1388\n",
      "[Trial 135] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 20/60, Training Loss: 0.1258, Validation Loss: 0.1285\n",
      "[Trial 143] Epoch 17/60, Training Loss: 0.1253, Validation Loss: 0.1303\n",
      "[Trial 144] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1263\n",
      "[Trial 138] Epoch 32/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 145] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.1376\n",
      "[Trial 135] Epoch 43/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 139] Epoch 32/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1283\n",
      "[Trial 143] Epoch 18/60, Training Loss: 0.1251, Validation Loss: 0.1300\n",
      "[Trial 144] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1267\n",
      "[Trial 138] Epoch 33/60, Training Loss: 0.1263, Validation Loss: 0.1240\n",
      "[Trial 145] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1346\n",
      "[Trial 135] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 33/60, Training Loss: 0.1259, Validation Loss: 0.1252\n",
      "[Trial 142] Epoch 22/60, Training Loss: 0.1261, Validation Loss: 0.1279\n",
      "[Trial 143] Epoch 19/60, Training Loss: 0.1252, Validation Loss: 0.1296\n",
      "[Trial 144] Epoch 19/60, Training Loss: 0.1268, Validation Loss: 0.1261\n",
      "[Trial 138] Epoch 34/60, Training Loss: 0.1262, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 13/60, Training Loss: 0.1265, Validation Loss: 0.1331\n",
      "[Trial 135] Epoch 45/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 23/60, Training Loss: 0.1258, Validation Loss: 0.1278\n",
      "[Trial 139] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 143] Epoch 20/60, Training Loss: 0.1251, Validation Loss: 0.1308\n",
      "[Trial 144] Epoch 20/60, Training Loss: 0.1263, Validation Loss: 0.1257\n",
      "[Trial 138] Epoch 35/60, Training Loss: 0.1266, Validation Loss: 0.1241\n",
      "[Trial 145] Epoch 14/60, Training Loss: 0.1264, Validation Loss: 0.1320\n",
      "[Trial 135] Epoch 46/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 142] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1279\n",
      "[Trial 139] Epoch 35/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 143] Epoch 21/60, Training Loss: 0.1257, Validation Loss: 0.1292\n",
      "[Trial 144] Epoch 21/60, Training Loss: 0.1264, Validation Loss: 0.1269\n",
      "[Trial 145] Epoch 15/60, Training Loss: 0.1265, Validation Loss: 0.1293\n",
      "[Trial 138] Epoch 36/60, Training Loss: 0.1263, Validation Loss: 0.1245\n",
      "[Trial 135] Epoch 47/60, Training Loss: 0.1253, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1289\n",
      "[Trial 139] Epoch 36/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 22/60, Training Loss: 0.1266, Validation Loss: 0.1253\n",
      "[Trial 145] Epoch 16/60, Training Loss: 0.1261, Validation Loss: 0.1298\n",
      "[Trial 143] Epoch 22/60, Training Loss: 0.1252, Validation Loss: 0.1290\n",
      "[Trial 138] Epoch 37/60, Training Loss: 0.1260, Validation Loss: 0.1249\n",
      "[Trial 135] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 26/60, Training Loss: 0.1310, Validation Loss: 0.1275\n",
      "[Trial 139] Epoch 37/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 145] Epoch 17/60, Training Loss: 0.1262, Validation Loss: 0.1288\n",
      "[Trial 144] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1255\n",
      "[Trial 143] Epoch 23/60, Training Loss: 0.1267, Validation Loss: 0.1357\n",
      "[Trial 138] Epoch 38/60, Training Loss: 0.1262, Validation Loss: 0.1239\n",
      "[Trial 135] Epoch 49/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 27/60, Training Loss: 0.1260, Validation Loss: 0.1271\n",
      "[Trial 139] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 145] Epoch 18/60, Training Loss: 0.1261, Validation Loss: 0.1275\n",
      "[Trial 144] Epoch 24/60, Training Loss: 0.1265, Validation Loss: 0.1249\n",
      "[Trial 143] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1290\n",
      "[Trial 138] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 135] Epoch 50/60, Training Loss: 0.1254, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1272\n",
      "[Trial 139] Epoch 39/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 145] Epoch 19/60, Training Loss: 0.1261, Validation Loss: 0.1267\n",
      "[Trial 144] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1249\n",
      "[Trial 143] Epoch 25/60, Training Loss: 0.1248, Validation Loss: 0.1288\n",
      "[Trial 138] Epoch 40/60, Training Loss: 0.1260, Validation Loss: 0.1239\n",
      "[Trial 135] Epoch 51/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 29/60, Training Loss: 0.1253, Validation Loss: 0.1270\n",
      "[Trial 139] Epoch 40/60, Training Loss: 0.1255, Validation Loss: 0.1251\n",
      "[Trial 145] Epoch 20/60, Training Loss: 0.1262, Validation Loss: 0.1263\n",
      "[Trial 144] Epoch 26/60, Training Loss: 0.1262, Validation Loss: 0.1249\n",
      "[Trial 143] Epoch 26/60, Training Loss: 0.1251, Validation Loss: 0.1288\n",
      "[Trial 138] Epoch 41/60, Training Loss: 0.1260, Validation Loss: 0.1240\n",
      "[Trial 135] Epoch 52/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1271\n",
      "[Trial 139] Epoch 41/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 145] Epoch 21/60, Training Loss: 0.1267, Validation Loss: 0.1265\n",
      "[Trial 144] Epoch 27/60, Training Loss: 0.1337, Validation Loss: 0.1248\n",
      "[Trial 143] Epoch 27/60, Training Loss: 0.1250, Validation Loss: 0.1292\n",
      "[Trial 138] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1239\n",
      "[Trial 135] Epoch 53/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 142] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1270\n",
      "[Trial 139] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 145] Epoch 22/60, Training Loss: 0.1260, Validation Loss: 0.1265\n",
      "[Trial 144] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1247\n",
      "[Trial 143] Epoch 28/60, Training Loss: 0.1252, Validation Loss: 0.1287\n",
      "[Trial 138] Epoch 43/60, Training Loss: 0.1260, Validation Loss: 0.1240\n",
      "[Trial 142] Epoch 32/60, Training Loss: 0.1258, Validation Loss: 0.1277\n",
      "[Trial 135] Epoch 54/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 23/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 139] Epoch 43/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 29/60, Training Loss: 0.1262, Validation Loss: 0.1248\n",
      "[Trial 143] Epoch 29/60, Training Loss: 0.1251, Validation Loss: 0.1287\n",
      "[Trial 138] Epoch 44/60, Training Loss: 0.1258, Validation Loss: 0.1240\n",
      "[Trial 142] Epoch 33/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 135] Epoch 55/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 24/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 139] Epoch 44/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 30/60, Training Loss: 0.1263, Validation Loss: 0.1248\n",
      "[Trial 143] Epoch 30/60, Training Loss: 0.1248, Validation Loss: 0.1292\n",
      "[Trial 138] Epoch 45/60, Training Loss: 0.1258, Validation Loss: 0.1240\n",
      "[Trial 142] Epoch 34/60, Training Loss: 0.1250, Validation Loss: 0.1269\n",
      "[Trial 135] Epoch 56/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 145] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 139] Epoch 45/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1248\n",
      "[Trial 143] Epoch 31/60, Training Loss: 0.1249, Validation Loss: 0.1288\n",
      "[Trial 138] Epoch 46/60, Training Loss: 0.1257, Validation Loss: 0.1240\n",
      "[Trial 142] Epoch 35/60, Training Loss: 0.1251, Validation Loss: 0.1270\n",
      "[Trial 145] Epoch 26/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 135] Epoch 57/60, Training Loss: 0.1255, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1247\n",
      "[Trial 143] Epoch 32/60, Training Loss: 0.1251, Validation Loss: 0.1288\n",
      "[Trial 138] Epoch 47/60, Training Loss: 0.1258, Validation Loss: 0.1240\n",
      "[Trial 142] Epoch 36/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 145] Epoch 27/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 135] Epoch 58/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 47/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 33/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 143] Epoch 33/60, Training Loss: 0.1251, Validation Loss: 0.1295\n",
      "[Trial 138] Epoch 48/60, Training Loss: 0.1256, Validation Loss: 0.1240\n",
      "[Trial 145] Epoch 28/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 142] Epoch 37/60, Training Loss: 0.1251, Validation Loss: 0.1269\n",
      "[Trial 135] Epoch 59/60, Training Loss: 0.1254, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 34/60, Training Loss: 0.1257, Validation Loss: 0.1249\n",
      "[Trial 143] Epoch 34/60, Training Loss: 0.1246, Validation Loss: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:51:20,350] Trial 138 finished with value: 0.12392217169205348 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.0014567456523623679, 'batch_size': 16, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 138] Epoch 49/60, Training Loss: 0.1256, Validation Loss: 0.1240\n",
      "[Trial 138] Early stopping after 49 epochs.\n",
      "[Trial 145] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 142] Epoch 38/60, Training Loss: 0.1252, Validation Loss: 0.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:51:33,941] Trial 135 finished with value: 0.12448540888726711 and parameters: {'hidden_dim': 320, 'latent_dim': 32, 'learning_rate': 0.0014541036584657062, 'batch_size': 16, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 135] Epoch 60/60, Training Loss: 0.1256, Validation Loss: 0.1245\n",
      "[Trial 139] Epoch 49/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 146] Epoch 1/60, Training Loss: 9.5000, Validation Loss: 0.2837\n",
      "[Trial 144] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1248\n",
      "[Trial 143] Epoch 35/60, Training Loss: 0.1248, Validation Loss: 0.1286\n",
      "[Trial 145] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 142] Epoch 39/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 139] Epoch 50/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 146] Epoch 2/60, Training Loss: 0.1847, Validation Loss: 0.2377\n",
      "[Trial 144] Epoch 36/60, Training Loss: 0.1261, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 36/60, Training Loss: 0.1244, Validation Loss: 0.1287\n",
      "[Trial 147] Epoch 1/60, Training Loss: 0.3944, Validation Loss: 0.1664\n",
      "[Trial 145] Epoch 31/60, Training Loss: 0.1256, Validation Loss: 0.1264\n",
      "[Trial 142] Epoch 40/60, Training Loss: 0.1256, Validation Loss: 0.1269\n",
      "[Trial 146] Epoch 3/60, Training Loss: 0.1562, Validation Loss: 0.2168\n",
      "[Trial 139] Epoch 51/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 144] Epoch 37/60, Training Loss: 0.1259, Validation Loss: 0.1247\n",
      "[Trial 143] Epoch 37/60, Training Loss: 0.1248, Validation Loss: 0.1286\n",
      "[Trial 145] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1256\n",
      "[Trial 142] Epoch 41/60, Training Loss: 0.1252, Validation Loss: 0.1269\n",
      "[Trial 146] Epoch 4/60, Training Loss: 0.1451, Validation Loss: 0.2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:52:40,949] Trial 139 finished with value: 0.1250578290472428 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.0014369064672710107, 'batch_size': 16, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 139] Epoch 52/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 139] Early stopping after 52 epochs.\n",
      "[Trial 147] Epoch 2/60, Training Loss: 0.1281, Validation Loss: 0.1467\n",
      "[Trial 144] Epoch 38/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 38/60, Training Loss: 0.1248, Validation Loss: 0.1288\n",
      "[Trial 145] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1256\n",
      "[Trial 146] Epoch 5/60, Training Loss: 0.1389, Validation Loss: 0.1917\n",
      "[Trial 142] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1269\n",
      "[Trial 144] Epoch 39/60, Training Loss: 0.1259, Validation Loss: 0.1247\n",
      "[Trial 143] Epoch 39/60, Training Loss: 0.1249, Validation Loss: 0.1287\n",
      "[Trial 145] Epoch 34/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 146] Epoch 6/60, Training Loss: 0.1355, Validation Loss: 0.1843\n",
      "[Trial 148] Epoch 1/60, Training Loss: 0.3002, Validation Loss: 0.1767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:53:16,443] Trial 142 finished with value: 0.12684404427806537 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.001386296951311945, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 142] Epoch 43/60, Training Loss: 0.1250, Validation Loss: 0.1269\n",
      "[Trial 142] Early stopping after 43 epochs.\n",
      "[Trial 147] Epoch 3/60, Training Loss: 0.1268, Validation Loss: 0.1375\n",
      "[Trial 144] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 40/60, Training Loss: 0.1246, Validation Loss: 0.1286\n",
      "[Trial 145] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1256\n",
      "[Trial 146] Epoch 7/60, Training Loss: 0.1334, Validation Loss: 0.1769\n",
      "[Trial 144] Epoch 41/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 146] Epoch 8/60, Training Loss: 0.1317, Validation Loss: 0.1706\n",
      "[Trial 148] Epoch 2/60, Training Loss: 0.1288, Validation Loss: 0.1481\n",
      "[Trial 149] Epoch 1/60, Training Loss: 0.3002, Validation Loss: 0.1703\n",
      "[Trial 145] Epoch 36/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 143] Epoch 41/60, Training Loss: 0.1245, Validation Loss: 0.1286\n",
      "[Trial 147] Epoch 4/60, Training Loss: 0.1268, Validation Loss: 0.1325\n",
      "[Trial 146] Epoch 9/60, Training Loss: 0.1306, Validation Loss: 0.1658\n",
      "[Trial 144] Epoch 42/60, Training Loss: 0.1260, Validation Loss: 0.1246\n",
      "[Trial 145] Epoch 37/60, Training Loss: 0.1252, Validation Loss: 0.1255\n",
      "[Trial 143] Epoch 42/60, Training Loss: 0.1247, Validation Loss: 0.1286\n",
      "[Trial 149] Epoch 2/60, Training Loss: 0.1289, Validation Loss: 0.1405\n",
      "[Trial 148] Epoch 3/60, Training Loss: 0.1271, Validation Loss: 0.1342\n",
      "[Trial 146] Epoch 10/60, Training Loss: 0.1297, Validation Loss: 0.1614\n",
      "[Trial 147] Epoch 5/60, Training Loss: 0.1261, Validation Loss: 0.1310\n",
      "[Trial 145] Epoch 38/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 144] Epoch 43/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 43/60, Training Loss: 0.1246, Validation Loss: 0.1286\n",
      "[Trial 146] Epoch 11/60, Training Loss: 0.1287, Validation Loss: 0.1574\n",
      "[Trial 145] Epoch 39/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 144] Epoch 44/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 44/60, Training Loss: 0.1250, Validation Loss: 0.1286\n",
      "[Trial 149] Epoch 3/60, Training Loss: 0.1272, Validation Loss: 0.1327\n",
      "[Trial 148] Epoch 4/60, Training Loss: 0.1264, Validation Loss: 0.1298\n",
      "[Trial 147] Epoch 6/60, Training Loss: 0.1259, Validation Loss: 0.1294\n",
      "[Trial 146] Epoch 12/60, Training Loss: 0.1281, Validation Loss: 0.1546\n",
      "[Trial 145] Epoch 40/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 144] Epoch 45/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 45/60, Training Loss: 0.1245, Validation Loss: 0.1286\n",
      "[Trial 146] Epoch 13/60, Training Loss: 0.1280, Validation Loss: 0.1502\n",
      "[Trial 145] Epoch 41/60, Training Loss: 0.1252, Validation Loss: 0.1255\n",
      "[Trial 149] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1296\n",
      "[Trial 148] Epoch 5/60, Training Loss: 0.1262, Validation Loss: 0.1287\n",
      "[Trial 144] Epoch 46/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 147] Epoch 7/60, Training Loss: 0.1258, Validation Loss: 0.1334\n",
      "[Trial 143] Epoch 46/60, Training Loss: 0.1246, Validation Loss: 0.1286\n",
      "[Trial 146] Epoch 14/60, Training Loss: 0.1276, Validation Loss: 0.1470\n",
      "[Trial 145] Epoch 42/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 144] Epoch 47/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 143] Epoch 47/60, Training Loss: 0.1242, Validation Loss: 0.1286\n",
      "[Trial 146] Epoch 15/60, Training Loss: 0.1274, Validation Loss: 0.1445\n",
      "[Trial 149] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1270\n",
      "[Trial 148] Epoch 6/60, Training Loss: 0.1261, Validation Loss: 0.1289\n",
      "[Trial 145] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 147] Epoch 8/60, Training Loss: 0.1255, Validation Loss: 0.1286\n",
      "[Trial 144] Epoch 48/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 146] Epoch 16/60, Training Loss: 0.1274, Validation Loss: 0.1419\n",
      "[Trial 143] Epoch 48/60, Training Loss: 0.1247, Validation Loss: 0.1286\n",
      "[Trial 145] Epoch 44/60, Training Loss: 0.1253, Validation Loss: 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:56:33,958] Trial 144 finished with value: 0.12457080086072286 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.0014573458968924163, 'batch_size': 16, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 144] Epoch 49/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 144] Early stopping after 49 epochs.\n",
      "[Trial 149] Epoch 6/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 146] Epoch 17/60, Training Loss: 0.1269, Validation Loss: 0.1407\n",
      "[Trial 143] Epoch 49/60, Training Loss: 0.1245, Validation Loss: 0.1286\n",
      "[Trial 148] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 147] Epoch 9/60, Training Loss: 0.1253, Validation Loss: 0.1286\n",
      "[Trial 145] Epoch 45/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 146] Epoch 18/60, Training Loss: 0.1271, Validation Loss: 0.1374\n",
      "[Trial 143] Epoch 50/60, Training Loss: 0.1249, Validation Loss: 0.1286\n",
      "[Trial 145] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 150] Epoch 1/60, Training Loss: 0.2158, Validation Loss: 0.1449\n",
      "[Trial 149] Epoch 7/60, Training Loss: 0.1263, Validation Loss: 0.1262\n",
      "[Trial 146] Epoch 19/60, Training Loss: 0.1270, Validation Loss: 0.1366\n",
      "[Trial 148] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1269\n",
      "[Trial 147] Epoch 10/60, Training Loss: 0.1256, Validation Loss: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:57:21,304] Trial 143 finished with value: 0.12855150811374189 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.001409818672061312, 'batch_size': 16, 'patience': 9}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 143] Epoch 51/60, Training Loss: 0.1244, Validation Loss: 0.1286\n",
      "[Trial 143] Early stopping after 51 epochs.\n",
      "[Trial 145] Epoch 47/60, Training Loss: 0.1253, Validation Loss: 0.1255\n",
      "[Trial 146] Epoch 20/60, Training Loss: 0.1266, Validation Loss: 0.1356\n",
      "[Trial 150] Epoch 2/60, Training Loss: 0.1270, Validation Loss: 0.1344\n",
      "[Trial 149] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1266\n",
      "[Trial 145] Epoch 48/60, Training Loss: 0.1252, Validation Loss: 0.1255\n",
      "[Trial 146] Epoch 21/60, Training Loss: 0.1266, Validation Loss: 0.1342\n",
      "[Trial 148] Epoch 9/60, Training Loss: 0.1261, Validation Loss: 0.1262\n",
      "[Trial 147] Epoch 11/60, Training Loss: 0.1252, Validation Loss: 0.1284\n",
      "[Trial 151] Epoch 1/60, Training Loss: 0.3741, Validation Loss: 0.1868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:58:01,646] Trial 145 finished with value: 0.12548620936771235 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'learning_rate': 0.0015334838407745385, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 145] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 145] Early stopping after 49 epochs.\n",
      "[Trial 146] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1329\n",
      "[Trial 150] Epoch 3/60, Training Loss: 0.1265, Validation Loss: 0.1300\n",
      "[Trial 149] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1260\n",
      "[Trial 148] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1263\n",
      "[Trial 146] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1320\n",
      "[Trial 147] Epoch 12/60, Training Loss: 0.1248, Validation Loss: 0.1286\n",
      "[Trial 151] Epoch 2/60, Training Loss: 0.1296, Validation Loss: 0.1551\n",
      "[Trial 152] Epoch 1/60, Training Loss: 0.3615, Validation Loss: 0.1743\n",
      "[Trial 146] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1304\n",
      "[Trial 150] Epoch 4/60, Training Loss: 0.1260, Validation Loss: 0.1281\n",
      "[Trial 149] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1260\n",
      "[Trial 148] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1261\n",
      "[Trial 147] Epoch 13/60, Training Loss: 0.1250, Validation Loss: 0.1283\n",
      "[Trial 151] Epoch 3/60, Training Loss: 0.1278, Validation Loss: 0.1365\n",
      "[Trial 146] Epoch 25/60, Training Loss: 0.1267, Validation Loss: 0.1301\n",
      "[Trial 152] Epoch 2/60, Training Loss: 0.1288, Validation Loss: 0.1473\n",
      "[Trial 150] Epoch 5/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 149] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1259\n",
      "[Trial 146] Epoch 26/60, Training Loss: 0.1267, Validation Loss: 0.1297\n",
      "[Trial 148] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1268\n",
      "[Trial 152] Epoch 3/60, Training Loss: 0.1273, Validation Loss: 0.1364\n",
      "[Trial 151] Epoch 4/60, Training Loss: 0.1274, Validation Loss: 0.1312\n",
      "[Trial 147] Epoch 14/60, Training Loss: 0.1253, Validation Loss: 0.1283\n",
      "[Trial 146] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1299\n",
      "[Trial 150] Epoch 6/60, Training Loss: 0.1254, Validation Loss: 0.1272\n",
      "[Trial 149] Epoch 12/60, Training Loss: 0.1268, Validation Loss: 0.1257\n",
      "[Trial 146] Epoch 28/60, Training Loss: 0.1264, Validation Loss: 0.1309\n",
      "[Trial 152] Epoch 4/60, Training Loss: 0.1265, Validation Loss: 0.1304\n",
      "[Trial 148] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1259\n",
      "[Trial 151] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1276\n",
      "[Trial 147] Epoch 15/60, Training Loss: 0.1250, Validation Loss: 0.1287\n",
      "[Trial 146] Epoch 29/60, Training Loss: 0.1261, Validation Loss: 0.1283\n",
      "[Trial 150] Epoch 7/60, Training Loss: 0.1257, Validation Loss: 0.1270\n",
      "[Trial 149] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1256\n",
      "[Trial 152] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1279\n",
      "[Trial 146] Epoch 30/60, Training Loss: 0.1257, Validation Loss: 0.1279\n",
      "[Trial 148] Epoch 14/60, Training Loss: 0.1263, Validation Loss: 0.1261\n",
      "[Trial 151] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1261\n",
      "[Trial 147] Epoch 16/60, Training Loss: 0.1249, Validation Loss: 0.1285\n",
      "[Trial 146] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1279\n",
      "[Trial 150] Epoch 8/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 149] Epoch 14/60, Training Loss: 0.1257, Validation Loss: 0.1262\n",
      "[Trial 152] Epoch 6/60, Training Loss: 0.1262, Validation Loss: 0.1273\n",
      "[Trial 146] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1277\n",
      "[Trial 148] Epoch 15/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 151] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:01:14,709] Trial 147 finished with value: 0.12825747275104124 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0009839381267995671, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 147] Epoch 17/60, Training Loss: 0.1250, Validation Loss: 0.1283\n",
      "[Trial 147] Early stopping after 17 epochs.\n",
      "[Trial 146] Epoch 33/60, Training Loss: 0.1259, Validation Loss: 0.1274\n",
      "[Trial 150] Epoch 9/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 149] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 152] Epoch 7/60, Training Loss: 0.1262, Validation Loss: 0.1266\n",
      "[Trial 146] Epoch 34/60, Training Loss: 0.1257, Validation Loss: 0.1275\n",
      "[Trial 148] Epoch 16/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 151] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1251\n",
      "[Trial 153] Epoch 1/60, Training Loss: 0.3879, Validation Loss: 0.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:01:58,291] Trial 149 finished with value: 0.12563148302336533 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0010695770526084978, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 150] Epoch 10/60, Training Loss: 0.1251, Validation Loss: 0.1271\n",
      "[Trial 152] Epoch 8/60, Training Loss: 0.1259, Validation Loss: 0.1265\n",
      "[Trial 149] Epoch 16/60, Training Loss: 0.1255, Validation Loss: 0.1257\n",
      "[Trial 149] Early stopping after 16 epochs.\n",
      "[Trial 146] Epoch 35/60, Training Loss: 0.1257, Validation Loss: 0.1272\n",
      "[Trial 154] Epoch 1/60, Training Loss: 85.5783, Validation Loss: 1.9644\n",
      "[Trial 154] Epoch 2/60, Training Loss: 0.8745, Validation Loss: 0.5830\n",
      "[Trial 151] Epoch 9/60, Training Loss: 0.1269, Validation Loss: 0.1241\n",
      "[Trial 148] Epoch 17/60, Training Loss: 0.1255, Validation Loss: 0.1259\n",
      "[Trial 154] Epoch 3/60, Training Loss: 0.4166, Validation Loss: 0.4224\n",
      "[Trial 146] Epoch 36/60, Training Loss: 0.1259, Validation Loss: 0.1272\n",
      "[Trial 153] Epoch 2/60, Training Loss: 0.1304, Validation Loss: 0.1628\n",
      "[Trial 154] Epoch 4/60, Training Loss: 0.3061, Validation Loss: 0.3454\n",
      "[Trial 152] Epoch 9/60, Training Loss: 0.1258, Validation Loss: 0.1264\n",
      "[Trial 150] Epoch 11/60, Training Loss: 0.1253, Validation Loss: 0.1270\n",
      "[Trial 154] Epoch 5/60, Training Loss: 0.2508, Validation Loss: 0.2991\n",
      "[Trial 146] Epoch 37/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 154] Epoch 6/60, Training Loss: 0.2169, Validation Loss: 0.2675\n",
      "[Trial 154] Epoch 7/60, Training Loss: 0.1959, Validation Loss: 0.2464\n",
      "[Trial 151] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1242\n",
      "[Trial 154] Epoch 8/60, Training Loss: 0.1826, Validation Loss: 0.2306\n",
      "[Trial 148] Epoch 18/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 146] Epoch 38/60, Training Loss: 0.1258, Validation Loss: 0.1267\n",
      "[Trial 153] Epoch 3/60, Training Loss: 0.1277, Validation Loss: 0.1485\n",
      "[Trial 152] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1262\n",
      "[Trial 154] Epoch 9/60, Training Loss: 0.1734, Validation Loss: 0.2184\n",
      "[Trial 150] Epoch 12/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 154] Epoch 10/60, Training Loss: 0.1662, Validation Loss: 0.2070\n",
      "[Trial 154] Epoch 11/60, Training Loss: 0.1606, Validation Loss: 0.1977\n",
      "[Trial 146] Epoch 39/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 154] Epoch 12/60, Training Loss: 0.1562, Validation Loss: 0.1901\n",
      "[Trial 151] Epoch 11/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 154] Epoch 13/60, Training Loss: 0.1529, Validation Loss: 0.1826\n",
      "[Trial 148] Epoch 19/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 152] Epoch 11/60, Training Loss: 0.1261, Validation Loss: 0.1262\n",
      "[Trial 153] Epoch 4/60, Training Loss: 0.1270, Validation Loss: 0.1400\n",
      "[Trial 146] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1266\n",
      "[Trial 154] Epoch 14/60, Training Loss: 0.1500, Validation Loss: 0.1788\n",
      "[Trial 150] Epoch 13/60, Training Loss: 0.1252, Validation Loss: 0.1267\n",
      "[Trial 154] Epoch 15/60, Training Loss: 0.1476, Validation Loss: 0.1728\n",
      "[Trial 154] Epoch 16/60, Training Loss: 0.1456, Validation Loss: 0.1691\n",
      "[Trial 146] Epoch 41/60, Training Loss: 0.1263, Validation Loss: 0.1263\n",
      "[Trial 154] Epoch 17/60, Training Loss: 0.1438, Validation Loss: 0.1655\n",
      "[Trial 151] Epoch 12/60, Training Loss: 0.1261, Validation Loss: 0.1239\n",
      "[Trial 152] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1265\n",
      "[Trial 154] Epoch 18/60, Training Loss: 0.1425, Validation Loss: 0.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:04:02,063] Trial 148 finished with value: 0.12589615490287542 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0010626282031101517, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 148] Epoch 20/60, Training Loss: 0.1252, Validation Loss: 0.1259\n",
      "[Trial 148] Early stopping after 20 epochs.\n",
      "[Trial 153] Epoch 5/60, Training Loss: 0.1262, Validation Loss: 0.1356\n",
      "[Trial 154] Epoch 19/60, Training Loss: 0.1411, Validation Loss: 0.1587\n",
      "[Trial 155] Epoch 1/60, Training Loss: 88.6921, Validation Loss: 1.8225\n",
      "[Trial 150] Epoch 14/60, Training Loss: 0.1249, Validation Loss: 0.1267\n",
      "[Trial 146] Epoch 42/60, Training Loss: 0.1260, Validation Loss: 0.1261\n",
      "[Trial 154] Epoch 20/60, Training Loss: 0.1399, Validation Loss: 0.1554\n",
      "[Trial 155] Epoch 2/60, Training Loss: 0.9079, Validation Loss: 0.6479\n",
      "[Trial 154] Epoch 21/60, Training Loss: 0.1388, Validation Loss: 0.1535\n",
      "[Trial 155] Epoch 3/60, Training Loss: 0.4586, Validation Loss: 0.4624\n",
      "[Trial 154] Epoch 22/60, Training Loss: 0.1380, Validation Loss: 0.1503\n",
      "[Trial 146] Epoch 43/60, Training Loss: 0.1260, Validation Loss: 0.1262\n",
      "[Trial 155] Epoch 4/60, Training Loss: 0.3341, Validation Loss: 0.3770\n",
      "[Trial 152] Epoch 13/60, Training Loss: 0.1257, Validation Loss: 0.1263\n",
      "[Trial 151] Epoch 13/60, Training Loss: 0.1261, Validation Loss: 0.1241\n",
      "[Trial 154] Epoch 23/60, Training Loss: 0.1371, Validation Loss: 0.1493\n",
      "[Trial 155] Epoch 5/60, Training Loss: 0.2714, Validation Loss: 0.3250\n",
      "[Trial 153] Epoch 6/60, Training Loss: 0.1261, Validation Loss: 0.1322\n",
      "[Trial 154] Epoch 24/60, Training Loss: 0.1363, Validation Loss: 0.1475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:04:44,467] Trial 150 finished with value: 0.1266436818987131 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.003358681381087898, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 150] Epoch 15/60, Training Loss: 0.1252, Validation Loss: 0.1267\n",
      "[Trial 150] Early stopping after 15 epochs.\n",
      "[Trial 155] Epoch 6/60, Training Loss: 0.2342, Validation Loss: 0.2903\n",
      "[Trial 146] Epoch 44/60, Training Loss: 0.1260, Validation Loss: 0.1263\n",
      "[Trial 154] Epoch 25/60, Training Loss: 0.1358, Validation Loss: 0.1467\n",
      "[Trial 156] Epoch 1/60, Training Loss: 95.3749, Validation Loss: 2.5547\n",
      "[Trial 155] Epoch 7/60, Training Loss: 0.2088, Validation Loss: 0.2641\n",
      "[Trial 154] Epoch 26/60, Training Loss: 0.1350, Validation Loss: 0.1442\n",
      "[Trial 156] Epoch 2/60, Training Loss: 0.8385, Validation Loss: 0.5371\n",
      "[Trial 155] Epoch 8/60, Training Loss: 0.1916, Validation Loss: 0.2456\n",
      "[Trial 152] Epoch 14/60, Training Loss: 0.1257, Validation Loss: 0.1261\n",
      "[Trial 154] Epoch 27/60, Training Loss: 0.1345, Validation Loss: 0.1425\n",
      "[Trial 156] Epoch 3/60, Training Loss: 0.3680, Validation Loss: 0.3990\n",
      "[Trial 155] Epoch 9/60, Training Loss: 0.1800, Validation Loss: 0.2301\n",
      "[Trial 146] Epoch 45/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 151] Epoch 14/60, Training Loss: 0.1261, Validation Loss: 0.1241\n",
      "[Trial 154] Epoch 28/60, Training Loss: 0.1340, Validation Loss: 0.1416\n",
      "[Trial 156] Epoch 4/60, Training Loss: 0.2768, Validation Loss: 0.3337\n",
      "[Trial 155] Epoch 10/60, Training Loss: 0.1713, Validation Loss: 0.2172\n",
      "[Trial 153] Epoch 7/60, Training Loss: 0.1257, Validation Loss: 0.1311\n",
      "[Trial 154] Epoch 29/60, Training Loss: 0.1333, Validation Loss: 0.1410\n",
      "[Trial 156] Epoch 5/60, Training Loss: 0.2318, Validation Loss: 0.2951\n",
      "[Trial 155] Epoch 11/60, Training Loss: 0.1649, Validation Loss: 0.2068\n",
      "[Trial 154] Epoch 30/60, Training Loss: 0.1331, Validation Loss: 0.1393\n",
      "[Trial 156] Epoch 6/60, Training Loss: 0.2063, Validation Loss: 0.2702\n",
      "[Trial 146] Epoch 46/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 155] Epoch 12/60, Training Loss: 0.1595, Validation Loss: 0.1978\n",
      "[Trial 154] Epoch 31/60, Training Loss: 0.1325, Validation Loss: 0.1383\n",
      "[Trial 156] Epoch 7/60, Training Loss: 0.1899, Validation Loss: 0.2477\n",
      "[Trial 152] Epoch 15/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 155] Epoch 13/60, Training Loss: 0.1554, Validation Loss: 0.1895\n",
      "[Trial 154] Epoch 32/60, Training Loss: 0.1321, Validation Loss: 0.1379\n",
      "[Trial 156] Epoch 8/60, Training Loss: 0.1780, Validation Loss: 0.2321\n",
      "[Trial 151] Epoch 15/60, Training Loss: 0.1262, Validation Loss: 0.1239\n",
      "[Trial 155] Epoch 14/60, Training Loss: 0.1521, Validation Loss: 0.1837\n",
      "[Trial 146] Epoch 47/60, Training Loss: 0.1256, Validation Loss: 0.1257\n",
      "[Trial 154] Epoch 33/60, Training Loss: 0.1318, Validation Loss: 0.1373\n",
      "[Trial 156] Epoch 9/60, Training Loss: 0.1698, Validation Loss: 0.2195\n",
      "[Trial 155] Epoch 15/60, Training Loss: 0.1492, Validation Loss: 0.1777\n",
      "[Trial 153] Epoch 8/60, Training Loss: 0.1257, Validation Loss: 0.1300\n",
      "[Trial 154] Epoch 34/60, Training Loss: 0.1314, Validation Loss: 0.1363\n",
      "[Trial 156] Epoch 10/60, Training Loss: 0.1631, Validation Loss: 0.2072\n",
      "[Trial 155] Epoch 16/60, Training Loss: 0.1469, Validation Loss: 0.1711\n",
      "[Trial 154] Epoch 35/60, Training Loss: 0.1312, Validation Loss: 0.1357\n",
      "[Trial 156] Epoch 11/60, Training Loss: 0.1570, Validation Loss: 0.1968\n",
      "[Trial 146] Epoch 48/60, Training Loss: 0.1257, Validation Loss: 0.1259\n",
      "[Trial 155] Epoch 17/60, Training Loss: 0.1448, Validation Loss: 0.1665\n",
      "[Trial 152] Epoch 16/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 154] Epoch 36/60, Training Loss: 0.1308, Validation Loss: 0.1351\n",
      "[Trial 156] Epoch 12/60, Training Loss: 0.1526, Validation Loss: 0.1879\n",
      "[Trial 155] Epoch 18/60, Training Loss: 0.1429, Validation Loss: 0.1626\n",
      "[Trial 154] Epoch 37/60, Training Loss: 0.1306, Validation Loss: 0.1350\n",
      "[Trial 156] Epoch 13/60, Training Loss: 0.1491, Validation Loss: 0.1798\n",
      "[Trial 151] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1240\n",
      "[Trial 155] Epoch 19/60, Training Loss: 0.1416, Validation Loss: 0.1590\n",
      "[Trial 154] Epoch 38/60, Training Loss: 0.1302, Validation Loss: 0.1346\n",
      "[Trial 156] Epoch 14/60, Training Loss: 0.1463, Validation Loss: 0.1743\n",
      "[Trial 146] Epoch 49/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 153] Epoch 9/60, Training Loss: 0.1255, Validation Loss: 0.1291\n",
      "[Trial 155] Epoch 20/60, Training Loss: 0.1402, Validation Loss: 0.1559\n",
      "[Trial 154] Epoch 39/60, Training Loss: 0.1299, Validation Loss: 0.1335\n",
      "[Trial 156] Epoch 15/60, Training Loss: 0.1442, Validation Loss: 0.1688\n",
      "[Trial 155] Epoch 21/60, Training Loss: 0.1390, Validation Loss: 0.1533\n",
      "[Trial 154] Epoch 40/60, Training Loss: 0.1298, Validation Loss: 0.1331\n",
      "[Trial 156] Epoch 16/60, Training Loss: 0.1423, Validation Loss: 0.1659\n",
      "[Trial 152] Epoch 17/60, Training Loss: 0.1257, Validation Loss: 0.1263\n",
      "[Trial 155] Epoch 22/60, Training Loss: 0.1381, Validation Loss: 0.1497\n",
      "[Trial 154] Epoch 41/60, Training Loss: 0.1296, Validation Loss: 0.1330\n",
      "[Trial 156] Epoch 17/60, Training Loss: 0.1410, Validation Loss: 0.1604\n",
      "[Trial 146] Epoch 50/60, Training Loss: 0.1259, Validation Loss: 0.1257\n",
      "[Trial 155] Epoch 23/60, Training Loss: 0.1373, Validation Loss: 0.1483\n",
      "[Trial 154] Epoch 42/60, Training Loss: 0.1293, Validation Loss: 0.1324\n",
      "[Trial 156] Epoch 18/60, Training Loss: 0.1394, Validation Loss: 0.1578\n",
      "[Trial 151] Epoch 17/60, Training Loss: 0.1259, Validation Loss: 0.1240\n",
      "[Trial 154] Epoch 43/60, Training Loss: 0.1291, Validation Loss: 0.1324\n",
      "[Trial 155] Epoch 24/60, Training Loss: 0.1362, Validation Loss: 0.1472\n",
      "[Trial 156] Epoch 19/60, Training Loss: 0.1383, Validation Loss: 0.1547\n",
      "[Trial 153] Epoch 10/60, Training Loss: 0.1256, Validation Loss: 0.1287\n",
      "[Trial 146] Epoch 51/60, Training Loss: 0.1260, Validation Loss: 0.1258\n",
      "[Trial 154] Epoch 44/60, Training Loss: 0.1291, Validation Loss: 0.1318\n",
      "[Trial 155] Epoch 25/60, Training Loss: 0.1358, Validation Loss: 0.1442\n",
      "[Trial 156] Epoch 20/60, Training Loss: 0.1374, Validation Loss: 0.1524\n",
      "[Trial 154] Epoch 45/60, Training Loss: 0.1289, Validation Loss: 0.1317\n",
      "[Trial 152] Epoch 18/60, Training Loss: 0.1257, Validation Loss: 0.1261\n",
      "[Trial 156] Epoch 21/60, Training Loss: 0.1364, Validation Loss: 0.1501\n",
      "[Trial 155] Epoch 26/60, Training Loss: 0.1350, Validation Loss: 0.1428\n",
      "[Trial 154] Epoch 46/60, Training Loss: 0.1287, Validation Loss: 0.1313\n",
      "[Trial 156] Epoch 22/60, Training Loss: 0.1356, Validation Loss: 0.1482\n",
      "[Trial 155] Epoch 27/60, Training Loss: 0.1344, Validation Loss: 0.1419\n",
      "[Trial 146] Epoch 52/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 154] Epoch 47/60, Training Loss: 0.1287, Validation Loss: 0.1310\n",
      "[Trial 156] Epoch 23/60, Training Loss: 0.1348, Validation Loss: 0.1459\n",
      "[Trial 155] Epoch 28/60, Training Loss: 0.1337, Validation Loss: 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:07:30,422] Trial 151 finished with value: 0.1239270164941748 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0009984250092738046, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 151] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1241\n",
      "[Trial 151] Early stopping after 18 epochs.\n",
      "[Trial 154] Epoch 48/60, Training Loss: 0.1284, Validation Loss: 0.1308\n",
      "[Trial 156] Epoch 24/60, Training Loss: 0.1341, Validation Loss: 0.1451\n",
      "[Trial 155] Epoch 29/60, Training Loss: 0.1332, Validation Loss: 0.1389\n",
      "[Trial 157] Epoch 1/60, Training Loss: 31.6441, Validation Loss: 0.3923\n",
      "[Trial 153] Epoch 11/60, Training Loss: 0.1256, Validation Loss: 0.1291\n",
      "[Trial 154] Epoch 49/60, Training Loss: 0.1282, Validation Loss: 0.1310\n",
      "[Trial 156] Epoch 25/60, Training Loss: 0.1336, Validation Loss: 0.1444\n",
      "[Trial 155] Epoch 30/60, Training Loss: 0.1327, Validation Loss: 0.1368\n",
      "[Trial 146] Epoch 53/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 157] Epoch 2/60, Training Loss: 0.1890, Validation Loss: 0.2397\n",
      "[Trial 152] Epoch 19/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 154] Epoch 50/60, Training Loss: 0.1282, Validation Loss: 0.1307\n",
      "[Trial 156] Epoch 26/60, Training Loss: 0.1333, Validation Loss: 0.1421\n",
      "[Trial 155] Epoch 31/60, Training Loss: 0.1322, Validation Loss: 0.1365\n",
      "[Trial 157] Epoch 3/60, Training Loss: 0.1521, Validation Loss: 0.2114\n",
      "[Trial 154] Epoch 51/60, Training Loss: 0.1280, Validation Loss: 0.1309\n",
      "[Trial 156] Epoch 27/60, Training Loss: 0.1327, Validation Loss: 0.1413\n",
      "[Trial 155] Epoch 32/60, Training Loss: 0.1317, Validation Loss: 0.1361\n",
      "[Trial 157] Epoch 4/60, Training Loss: 0.1433, Validation Loss: 0.1883\n",
      "[Trial 146] Epoch 54/60, Training Loss: 0.1261, Validation Loss: 0.1256\n",
      "[Trial 154] Epoch 52/60, Training Loss: 0.1279, Validation Loss: 0.1303\n",
      "[Trial 156] Epoch 28/60, Training Loss: 0.1321, Validation Loss: 0.1410\n",
      "[Trial 155] Epoch 33/60, Training Loss: 0.1316, Validation Loss: 0.1347\n",
      "[Trial 157] Epoch 5/60, Training Loss: 0.1386, Validation Loss: 0.1756\n",
      "[Trial 154] Epoch 53/60, Training Loss: 0.1277, Validation Loss: 0.1299\n",
      "[Trial 156] Epoch 29/60, Training Loss: 0.1317, Validation Loss: 0.1394\n",
      "[Trial 155] Epoch 34/60, Training Loss: 0.1310, Validation Loss: 0.1344\n",
      "[Trial 157] Epoch 6/60, Training Loss: 0.1358, Validation Loss: 0.1649\n",
      "[Trial 153] Epoch 12/60, Training Loss: 0.1255, Validation Loss: 0.1283\n",
      "[Trial 154] Epoch 54/60, Training Loss: 0.1278, Validation Loss: 0.1295\n",
      "[Trial 156] Epoch 30/60, Training Loss: 0.1314, Validation Loss: 0.1388\n",
      "[Trial 152] Epoch 20/60, Training Loss: 0.1255, Validation Loss: 0.1260\n",
      "[Trial 155] Epoch 35/60, Training Loss: 0.1308, Validation Loss: 0.1332\n",
      "[Trial 157] Epoch 7/60, Training Loss: 0.1338, Validation Loss: 0.1552\n",
      "[Trial 146] Epoch 55/60, Training Loss: 0.1259, Validation Loss: 0.1255\n",
      "[Trial 154] Epoch 55/60, Training Loss: 0.1289, Validation Loss: 0.1298\n",
      "[Trial 156] Epoch 31/60, Training Loss: 0.1309, Validation Loss: 0.1377\n",
      "[Trial 155] Epoch 36/60, Training Loss: 0.1303, Validation Loss: 0.1327\n",
      "[Trial 157] Epoch 8/60, Training Loss: 0.1320, Validation Loss: 0.1517\n",
      "[Trial 154] Epoch 56/60, Training Loss: 0.1280, Validation Loss: 0.1294\n",
      "[Trial 156] Epoch 32/60, Training Loss: 0.1305, Validation Loss: 0.1370\n",
      "[Trial 155] Epoch 37/60, Training Loss: 0.1302, Validation Loss: 0.1323\n",
      "[Trial 157] Epoch 9/60, Training Loss: 0.1307, Validation Loss: 0.1468\n",
      "[Trial 154] Epoch 57/60, Training Loss: 0.1275, Validation Loss: 0.1296\n",
      "[Trial 156] Epoch 33/60, Training Loss: 0.1303, Validation Loss: 0.1364\n",
      "[Trial 146] Epoch 56/60, Training Loss: 0.1254, Validation Loss: 0.1255\n",
      "[Trial 155] Epoch 38/60, Training Loss: 0.1299, Validation Loss: 0.1321\n",
      "[Trial 157] Epoch 10/60, Training Loss: 0.1299, Validation Loss: 0.1439\n",
      "[Trial 154] Epoch 58/60, Training Loss: 0.1274, Validation Loss: 0.1293\n",
      "[Trial 156] Epoch 34/60, Training Loss: 0.1301, Validation Loss: 0.1353\n",
      "[Trial 152] Epoch 21/60, Training Loss: 0.1251, Validation Loss: 0.1260\n",
      "[Trial 157] Epoch 11/60, Training Loss: 0.1294, Validation Loss: 0.1420\n",
      "[Trial 155] Epoch 39/60, Training Loss: 0.1295, Validation Loss: 0.1318\n",
      "[Trial 153] Epoch 13/60, Training Loss: 0.1256, Validation Loss: 0.1298\n",
      "[Trial 154] Epoch 59/60, Training Loss: 0.1273, Validation Loss: 0.1291\n",
      "[Trial 156] Epoch 35/60, Training Loss: 0.1298, Validation Loss: 0.1345\n",
      "[Trial 157] Epoch 12/60, Training Loss: 0.1289, Validation Loss: 0.1403\n",
      "[Trial 155] Epoch 40/60, Training Loss: 0.1295, Validation Loss: 0.1305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:09:01,954] Trial 154 finished with value: 0.12879873712857565 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.0005857122489470621, 'batch_size': 64, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 154] Epoch 60/60, Training Loss: 0.1273, Validation Loss: 0.1288\n",
      "[Trial 156] Epoch 36/60, Training Loss: 0.1296, Validation Loss: 0.1344\n",
      "[Trial 146] Epoch 57/60, Training Loss: 0.1261, Validation Loss: 0.1256\n",
      "[Trial 157] Epoch 13/60, Training Loss: 0.1288, Validation Loss: 0.1369\n",
      "[Trial 155] Epoch 41/60, Training Loss: 0.1297, Validation Loss: 0.1303\n",
      "[Trial 158] Epoch 1/60, Training Loss: 25.5382, Validation Loss: 0.3724\n",
      "[Trial 156] Epoch 37/60, Training Loss: 0.1294, Validation Loss: 0.1345\n",
      "[Trial 157] Epoch 14/60, Training Loss: 0.1282, Validation Loss: 0.1364\n",
      "[Trial 155] Epoch 42/60, Training Loss: 0.1291, Validation Loss: 0.1308\n",
      "[Trial 158] Epoch 2/60, Training Loss: 0.1800, Validation Loss: 0.2292\n",
      "[Trial 156] Epoch 38/60, Training Loss: 0.1291, Validation Loss: 0.1340\n",
      "[Trial 146] Epoch 58/60, Training Loss: 0.1260, Validation Loss: 0.1256\n",
      "[Trial 157] Epoch 15/60, Training Loss: 0.1279, Validation Loss: 0.1363\n",
      "[Trial 155] Epoch 43/60, Training Loss: 0.1292, Validation Loss: 0.1299\n",
      "[Trial 158] Epoch 3/60, Training Loss: 0.1466, Validation Loss: 0.2044\n",
      "[Trial 156] Epoch 39/60, Training Loss: 0.1288, Validation Loss: 0.1334\n",
      "[Trial 152] Epoch 22/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 157] Epoch 16/60, Training Loss: 0.1277, Validation Loss: 0.1345\n",
      "[Trial 153] Epoch 14/60, Training Loss: 0.1252, Validation Loss: 0.1284\n",
      "[Trial 158] Epoch 4/60, Training Loss: 0.1389, Validation Loss: 0.1903\n",
      "[Trial 155] Epoch 44/60, Training Loss: 0.1291, Validation Loss: 0.1303\n",
      "[Trial 156] Epoch 40/60, Training Loss: 0.1287, Validation Loss: 0.1341\n",
      "[Trial 157] Epoch 17/60, Training Loss: 0.1280, Validation Loss: 0.1333\n",
      "[Trial 158] Epoch 5/60, Training Loss: 0.1352, Validation Loss: 0.1731\n",
      "[Trial 155] Epoch 45/60, Training Loss: 0.1293, Validation Loss: 0.1294\n",
      "[Trial 156] Epoch 41/60, Training Loss: 0.1288, Validation Loss: 0.1325\n",
      "[Trial 146] Epoch 59/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 157] Epoch 18/60, Training Loss: 0.1276, Validation Loss: 0.1327\n",
      "[Trial 158] Epoch 6/60, Training Loss: 0.1328, Validation Loss: 0.1665\n",
      "[Trial 156] Epoch 42/60, Training Loss: 0.1286, Validation Loss: 0.1329\n",
      "[Trial 155] Epoch 46/60, Training Loss: 0.1285, Validation Loss: 0.1291\n",
      "[Trial 157] Epoch 19/60, Training Loss: 0.1273, Validation Loss: 0.1331\n",
      "[Trial 158] Epoch 7/60, Training Loss: 0.1313, Validation Loss: 0.1591\n",
      "[Trial 156] Epoch 43/60, Training Loss: 0.1285, Validation Loss: 0.1321\n",
      "[Trial 155] Epoch 47/60, Training Loss: 0.1284, Validation Loss: 0.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:09:57,338] Trial 152 finished with value: 0.1259859596689542 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0009382620765807248, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 152] Epoch 23/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 152] Early stopping after 23 epochs.\n",
      "[Trial 157] Epoch 20/60, Training Loss: 0.1276, Validation Loss: 0.1325\n",
      "[Trial 158] Epoch 8/60, Training Loss: 0.1306, Validation Loss: 0.1603\n",
      "[Trial 156] Epoch 44/60, Training Loss: 0.1280, Validation Loss: 0.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:10:01,703] Trial 146 finished with value: 0.12551531692345938 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'learning_rate': 0.00024995710978196334, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 155] Epoch 48/60, Training Loss: 0.1285, Validation Loss: 0.1287\n",
      "[Trial 146] Epoch 60/60, Training Loss: 0.1257, Validation Loss: 0.1255\n",
      "[Trial 159] Epoch 1/60, Training Loss: 31.5503, Validation Loss: 0.4205\n",
      "[Trial 153] Epoch 15/60, Training Loss: 0.1252, Validation Loss: 0.1279\n",
      "[Trial 157] Epoch 21/60, Training Loss: 0.1290, Validation Loss: 0.1338\n",
      "[Trial 158] Epoch 9/60, Training Loss: 0.1296, Validation Loss: 0.1539\n",
      "[Trial 156] Epoch 45/60, Training Loss: 0.1280, Validation Loss: 0.1316\n",
      "[Trial 155] Epoch 49/60, Training Loss: 0.1283, Validation Loss: 0.1283\n",
      "[Trial 160] Epoch 1/60, Training Loss: 11.7520, Validation Loss: 0.2727\n",
      "[Trial 159] Epoch 2/60, Training Loss: 0.2126, Validation Loss: 0.2495\n",
      "[Trial 158] Epoch 10/60, Training Loss: 0.1289, Validation Loss: 0.1515\n",
      "[Trial 157] Epoch 22/60, Training Loss: 0.1280, Validation Loss: 0.1316\n",
      "[Trial 156] Epoch 46/60, Training Loss: 0.1278, Validation Loss: 0.1317\n",
      "[Trial 155] Epoch 50/60, Training Loss: 0.1280, Validation Loss: 0.1289\n",
      "[Trial 160] Epoch 2/60, Training Loss: 0.1449, Validation Loss: 0.2281\n",
      "[Trial 159] Epoch 3/60, Training Loss: 0.1651, Validation Loss: 0.2165\n",
      "[Trial 158] Epoch 11/60, Training Loss: 0.1293, Validation Loss: 0.1504\n",
      "[Trial 157] Epoch 23/60, Training Loss: 0.1273, Validation Loss: 0.1306\n",
      "[Trial 156] Epoch 47/60, Training Loss: 0.1279, Validation Loss: 0.1319\n",
      "[Trial 155] Epoch 51/60, Training Loss: 0.1279, Validation Loss: 0.1280\n",
      "[Trial 159] Epoch 4/60, Training Loss: 0.1519, Validation Loss: 0.1964\n",
      "[Trial 160] Epoch 3/60, Training Loss: 0.1339, Validation Loss: 0.2141\n",
      "[Trial 158] Epoch 12/60, Training Loss: 0.1282, Validation Loss: 0.1452\n",
      "[Trial 157] Epoch 24/60, Training Loss: 0.1268, Validation Loss: 0.1299\n",
      "[Trial 156] Epoch 48/60, Training Loss: 0.1277, Validation Loss: 0.1307\n",
      "[Trial 159] Epoch 5/60, Training Loss: 0.1449, Validation Loss: 0.1832\n",
      "[Trial 155] Epoch 52/60, Training Loss: 0.1289, Validation Loss: 0.1282\n",
      "[Trial 160] Epoch 4/60, Training Loss: 0.1314, Validation Loss: 0.2058\n",
      "[Trial 158] Epoch 13/60, Training Loss: 0.1280, Validation Loss: 0.1446\n",
      "[Trial 157] Epoch 25/60, Training Loss: 0.1279, Validation Loss: 0.1293\n",
      "[Trial 156] Epoch 49/60, Training Loss: 0.1279, Validation Loss: 0.1313\n",
      "[Trial 159] Epoch 6/60, Training Loss: 0.1403, Validation Loss: 0.1725\n",
      "[Trial 155] Epoch 53/60, Training Loss: 0.1278, Validation Loss: 0.1284\n",
      "[Trial 160] Epoch 5/60, Training Loss: 0.1302, Validation Loss: 0.1967\n",
      "[Trial 153] Epoch 16/60, Training Loss: 0.1250, Validation Loss: 0.1278\n",
      "[Trial 157] Epoch 26/60, Training Loss: 0.1272, Validation Loss: 0.1291\n",
      "[Trial 158] Epoch 14/60, Training Loss: 0.1272, Validation Loss: 0.1414\n",
      "[Trial 159] Epoch 7/60, Training Loss: 0.1371, Validation Loss: 0.1660\n",
      "[Trial 156] Epoch 50/60, Training Loss: 0.1287, Validation Loss: 0.1302\n",
      "[Trial 155] Epoch 54/60, Training Loss: 0.1275, Validation Loss: 0.1277\n",
      "[Trial 160] Epoch 6/60, Training Loss: 0.1293, Validation Loss: 0.1899\n",
      "[Trial 158] Epoch 15/60, Training Loss: 0.1275, Validation Loss: 0.1398\n",
      "[Trial 157] Epoch 27/60, Training Loss: 0.1282, Validation Loss: 0.1282\n",
      "[Trial 159] Epoch 8/60, Training Loss: 0.1345, Validation Loss: 0.1591\n",
      "[Trial 156] Epoch 51/60, Training Loss: 0.1280, Validation Loss: 0.1322\n",
      "[Trial 155] Epoch 55/60, Training Loss: 0.1273, Validation Loss: 0.1276\n",
      "[Trial 160] Epoch 7/60, Training Loss: 0.1286, Validation Loss: 0.1859\n",
      "[Trial 159] Epoch 9/60, Training Loss: 0.1325, Validation Loss: 0.1561\n",
      "[Trial 157] Epoch 28/60, Training Loss: 0.1267, Validation Loss: 0.1282\n",
      "[Trial 158] Epoch 16/60, Training Loss: 0.1342, Validation Loss: 0.1377\n",
      "[Trial 156] Epoch 52/60, Training Loss: 0.1281, Validation Loss: 0.1304\n",
      "[Trial 155] Epoch 56/60, Training Loss: 0.1272, Validation Loss: 0.1274\n",
      "[Trial 160] Epoch 8/60, Training Loss: 0.1284, Validation Loss: 0.1800\n",
      "[Trial 159] Epoch 10/60, Training Loss: 0.1313, Validation Loss: 0.1502\n",
      "[Trial 158] Epoch 17/60, Training Loss: 0.1290, Validation Loss: 0.1370\n",
      "[Trial 157] Epoch 29/60, Training Loss: 0.1265, Validation Loss: 0.1272\n",
      "[Trial 156] Epoch 53/60, Training Loss: 0.1279, Validation Loss: 0.1306\n",
      "[Trial 155] Epoch 57/60, Training Loss: 0.1273, Validation Loss: 0.1275\n",
      "[Trial 160] Epoch 9/60, Training Loss: 0.1283, Validation Loss: 0.1791\n",
      "[Trial 159] Epoch 11/60, Training Loss: 0.1308, Validation Loss: 0.1498\n",
      "[Trial 157] Epoch 30/60, Training Loss: 0.1362, Validation Loss: 0.1274\n",
      "[Trial 158] Epoch 18/60, Training Loss: 0.1274, Validation Loss: 0.1363\n",
      "[Trial 156] Epoch 54/60, Training Loss: 0.1274, Validation Loss: 0.1300\n",
      "[Trial 155] Epoch 58/60, Training Loss: 0.1272, Validation Loss: 0.1274\n",
      "[Trial 159] Epoch 12/60, Training Loss: 0.1297, Validation Loss: 0.1470\n",
      "[Trial 160] Epoch 10/60, Training Loss: 0.1285, Validation Loss: 0.1726\n",
      "[Trial 153] Epoch 17/60, Training Loss: 0.1251, Validation Loss: 0.1279\n",
      "[Trial 157] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1272\n",
      "[Trial 158] Epoch 19/60, Training Loss: 0.1268, Validation Loss: 0.1350\n",
      "[Trial 156] Epoch 55/60, Training Loss: 0.1273, Validation Loss: 0.1311\n",
      "[Trial 159] Epoch 13/60, Training Loss: 0.1289, Validation Loss: 0.1437\n",
      "[Trial 155] Epoch 59/60, Training Loss: 0.1272, Validation Loss: 0.1274\n",
      "[Trial 160] Epoch 11/60, Training Loss: 0.1278, Validation Loss: 0.1705\n",
      "[Trial 157] Epoch 32/60, Training Loss: 0.1264, Validation Loss: 0.1267\n",
      "[Trial 158] Epoch 20/60, Training Loss: 0.1389, Validation Loss: 0.1342\n",
      "[Trial 156] Epoch 56/60, Training Loss: 0.1278, Validation Loss: 0.1300\n",
      "[Trial 159] Epoch 14/60, Training Loss: 0.1288, Validation Loss: 0.1409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:11:37,573] Trial 155 finished with value: 0.1271711155772209 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.0005694707241354802, 'batch_size': 64, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 155] Epoch 60/60, Training Loss: 0.1270, Validation Loss: 0.1272\n",
      "[Trial 160] Epoch 12/60, Training Loss: 0.1276, Validation Loss: 0.1670\n",
      "[Trial 157] Epoch 33/60, Training Loss: 0.1263, Validation Loss: 0.1271\n",
      "[Trial 158] Epoch 21/60, Training Loss: 0.1309, Validation Loss: 0.1320\n",
      "[Trial 156] Epoch 57/60, Training Loss: 0.1276, Validation Loss: 0.1302\n",
      "[Trial 159] Epoch 15/60, Training Loss: 0.1282, Validation Loss: 0.1408\n",
      "[Trial 161] Epoch 1/60, Training Loss: 11.4660, Validation Loss: 0.2859\n",
      "[Trial 160] Epoch 13/60, Training Loss: 0.1276, Validation Loss: 0.1631\n",
      "[Trial 157] Epoch 34/60, Training Loss: 0.1262, Validation Loss: 0.1267\n",
      "[Trial 158] Epoch 22/60, Training Loss: 0.1267, Validation Loss: 0.1323\n",
      "[Trial 156] Epoch 58/60, Training Loss: 0.1269, Validation Loss: 0.1296\n",
      "[Trial 159] Epoch 16/60, Training Loss: 0.1282, Validation Loss: 0.1394\n",
      "[Trial 161] Epoch 2/60, Training Loss: 0.1442, Validation Loss: 0.2418\n",
      "[Trial 160] Epoch 14/60, Training Loss: 0.1274, Validation Loss: 0.1612\n",
      "[Trial 158] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1305\n",
      "[Trial 157] Epoch 35/60, Training Loss: 0.1262, Validation Loss: 0.1265\n",
      "[Trial 156] Epoch 59/60, Training Loss: 0.1278, Validation Loss: 0.1300\n",
      "[Trial 159] Epoch 17/60, Training Loss: 0.1278, Validation Loss: 0.1381\n",
      "[Trial 153] Epoch 18/60, Training Loss: 0.1250, Validation Loss: 0.1279\n",
      "[Trial 158] Epoch 24/60, Training Loss: 0.1261, Validation Loss: 0.1300\n",
      "[Trial 160] Epoch 15/60, Training Loss: 0.1272, Validation Loss: 0.1580\n",
      "[Trial 161] Epoch 3/60, Training Loss: 0.1349, Validation Loss: 0.2291\n",
      "[Trial 157] Epoch 36/60, Training Loss: 0.1259, Validation Loss: 0.1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:12:04,185] Trial 156 finished with value: 0.129616612692674 and parameters: {'hidden_dim': 64, 'latent_dim': 96, 'learning_rate': 0.00058327222523721, 'batch_size': 64, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 156] Epoch 60/60, Training Loss: 0.1303, Validation Loss: 0.1301\n",
      "[Trial 159] Epoch 18/60, Training Loss: 0.1275, Validation Loss: 0.1354\n",
      "[Trial 158] Epoch 25/60, Training Loss: 0.1466, Validation Loss: 0.1298\n",
      "[Trial 157] Epoch 37/60, Training Loss: 0.1259, Validation Loss: 0.1262\n",
      "[Trial 160] Epoch 16/60, Training Loss: 0.1279, Validation Loss: 0.1544\n",
      "[Trial 159] Epoch 19/60, Training Loss: 0.1271, Validation Loss: 0.1345\n",
      "[Trial 161] Epoch 4/60, Training Loss: 0.1325, Validation Loss: 0.2156\n",
      "[Trial 162] Epoch 1/60, Training Loss: 7.3993, Validation Loss: 0.2596\n",
      "[Trial 158] Epoch 26/60, Training Loss: 0.1271, Validation Loss: 0.1295\n",
      "[Trial 159] Epoch 20/60, Training Loss: 0.1271, Validation Loss: 0.1341\n",
      "[Trial 157] Epoch 38/60, Training Loss: 0.1260, Validation Loss: 0.1263\n",
      "[Trial 160] Epoch 17/60, Training Loss: 0.1278, Validation Loss: 0.1531\n",
      "[Trial 161] Epoch 5/60, Training Loss: 0.1311, Validation Loss: 0.2075\n",
      "[Trial 162] Epoch 2/60, Training Loss: 0.1372, Validation Loss: 0.2409\n",
      "[Trial 158] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1287\n",
      "[Trial 159] Epoch 21/60, Training Loss: 0.1267, Validation Loss: 0.1330\n",
      "[Trial 157] Epoch 39/60, Training Loss: 0.1265, Validation Loss: 0.1265\n",
      "[Trial 160] Epoch 18/60, Training Loss: 0.1271, Validation Loss: 0.1507\n",
      "[Trial 161] Epoch 6/60, Training Loss: 0.1309, Validation Loss: 0.2007\n",
      "[Trial 162] Epoch 3/60, Training Loss: 0.1318, Validation Loss: 0.2331\n",
      "[Trial 159] Epoch 22/60, Training Loss: 0.1266, Validation Loss: 0.1326\n",
      "[Trial 158] Epoch 28/60, Training Loss: 0.1259, Validation Loss: 0.1280\n",
      "[Trial 157] Epoch 40/60, Training Loss: 0.1276, Validation Loss: 0.1268\n",
      "[Trial 160] Epoch 19/60, Training Loss: 0.1279, Validation Loss: 0.1487\n",
      "[Trial 161] Epoch 7/60, Training Loss: 0.1299, Validation Loss: 0.1919\n",
      "[Trial 153] Epoch 19/60, Training Loss: 0.1250, Validation Loss: 0.1278\n",
      "[Trial 162] Epoch 4/60, Training Loss: 0.1298, Validation Loss: 0.2261\n",
      "[Trial 159] Epoch 23/60, Training Loss: 0.1268, Validation Loss: 0.1326\n",
      "[Trial 158] Epoch 29/60, Training Loss: 0.1261, Validation Loss: 0.1280\n",
      "[Trial 157] Epoch 41/60, Training Loss: 0.1283, Validation Loss: 0.1261\n",
      "[Trial 160] Epoch 20/60, Training Loss: 0.1270, Validation Loss: 0.1452\n",
      "[Trial 161] Epoch 8/60, Training Loss: 0.1294, Validation Loss: 0.1907\n",
      "[Trial 159] Epoch 24/60, Training Loss: 0.1328, Validation Loss: 0.1316\n",
      "[Trial 158] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1277\n",
      "[Trial 162] Epoch 5/60, Training Loss: 0.1287, Validation Loss: 0.2196\n",
      "[Trial 157] Epoch 42/60, Training Loss: 0.1260, Validation Loss: 0.1261\n",
      "[Trial 160] Epoch 21/60, Training Loss: 0.1268, Validation Loss: 0.1439\n",
      "[Trial 161] Epoch 9/60, Training Loss: 0.1295, Validation Loss: 0.1853\n",
      "[Trial 159] Epoch 25/60, Training Loss: 0.1269, Validation Loss: 0.1308\n",
      "[Trial 158] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1275\n",
      "[Trial 157] Epoch 43/60, Training Loss: 0.1271, Validation Loss: 0.1259\n",
      "[Trial 162] Epoch 6/60, Training Loss: 0.1283, Validation Loss: 0.2121\n",
      "[Trial 160] Epoch 22/60, Training Loss: 0.1269, Validation Loss: 0.1413\n",
      "[Trial 161] Epoch 10/60, Training Loss: 0.1291, Validation Loss: 0.1805\n",
      "[Trial 159] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1305\n",
      "[Trial 158] Epoch 32/60, Training Loss: 0.1258, Validation Loss: 0.1273\n",
      "[Trial 157] Epoch 44/60, Training Loss: 0.1266, Validation Loss: 0.1259\n",
      "[Trial 162] Epoch 7/60, Training Loss: 0.1281, Validation Loss: 0.2071\n",
      "[Trial 160] Epoch 23/60, Training Loss: 0.1269, Validation Loss: 0.1394\n",
      "[Trial 159] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1296\n",
      "[Trial 161] Epoch 11/60, Training Loss: 0.1290, Validation Loss: 0.1774\n",
      "[Trial 158] Epoch 33/60, Training Loss: 0.1283, Validation Loss: 0.1271\n",
      "[Trial 157] Epoch 45/60, Training Loss: 0.1469, Validation Loss: 0.1278\n",
      "[Trial 153] Epoch 20/60, Training Loss: 0.1249, Validation Loss: 0.1277\n",
      "[Trial 162] Epoch 8/60, Training Loss: 0.1287, Validation Loss: 0.2058\n",
      "[Trial 159] Epoch 28/60, Training Loss: 0.1344, Validation Loss: 0.1298\n",
      "[Trial 160] Epoch 24/60, Training Loss: 0.1268, Validation Loss: 0.1379\n",
      "[Trial 161] Epoch 12/60, Training Loss: 0.1287, Validation Loss: 0.1710\n",
      "[Trial 158] Epoch 34/60, Training Loss: 0.1319, Validation Loss: 0.1271\n",
      "[Trial 157] Epoch 46/60, Training Loss: 0.1266, Validation Loss: 0.1259\n",
      "[Trial 159] Epoch 29/60, Training Loss: 0.1282, Validation Loss: 0.1288\n",
      "[Trial 160] Epoch 25/60, Training Loss: 0.1266, Validation Loss: 0.1356\n",
      "[Trial 162] Epoch 9/60, Training Loss: 0.1281, Validation Loss: 0.2022\n",
      "[Trial 161] Epoch 13/60, Training Loss: 0.1287, Validation Loss: 0.1661\n",
      "[Trial 158] Epoch 35/60, Training Loss: 0.1259, Validation Loss: 0.1270\n",
      "[Trial 157] Epoch 47/60, Training Loss: 0.1258, Validation Loss: 0.1259\n",
      "[Trial 159] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1284\n",
      "[Trial 160] Epoch 26/60, Training Loss: 0.1269, Validation Loss: 0.1340\n",
      "[Trial 158] Epoch 36/60, Training Loss: 0.1267, Validation Loss: 0.1271\n",
      "[Trial 162] Epoch 10/60, Training Loss: 0.1273, Validation Loss: 0.1968\n",
      "[Trial 161] Epoch 14/60, Training Loss: 0.1284, Validation Loss: 0.1623\n",
      "[Trial 157] Epoch 48/60, Training Loss: 0.1260, Validation Loss: 0.1259\n",
      "[Trial 159] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1284\n",
      "[Trial 160] Epoch 27/60, Training Loss: 0.1272, Validation Loss: 0.1330\n",
      "[Trial 158] Epoch 37/60, Training Loss: 0.1295, Validation Loss: 0.1268\n",
      "[Trial 161] Epoch 15/60, Training Loss: 0.1281, Validation Loss: 0.1583\n",
      "[Trial 162] Epoch 11/60, Training Loss: 0.1270, Validation Loss: 0.1920\n",
      "[Trial 157] Epoch 49/60, Training Loss: 0.1258, Validation Loss: 0.1259\n",
      "[Trial 159] Epoch 32/60, Training Loss: 0.1257, Validation Loss: 0.1281\n",
      "[Trial 160] Epoch 28/60, Training Loss: 0.1267, Validation Loss: 0.1317\n",
      "[Trial 158] Epoch 38/60, Training Loss: 0.1258, Validation Loss: 0.1268\n",
      "[Trial 157] Epoch 50/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 161] Epoch 16/60, Training Loss: 0.1278, Validation Loss: 0.1552\n",
      "[Trial 153] Epoch 21/60, Training Loss: 0.1250, Validation Loss: 0.1277\n",
      "[Trial 159] Epoch 33/60, Training Loss: 0.1283, Validation Loss: 0.1278\n",
      "[Trial 162] Epoch 12/60, Training Loss: 0.1272, Validation Loss: 0.1859\n",
      "[Trial 158] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1301\n",
      "[Trial 160] Epoch 29/60, Training Loss: 0.1265, Validation Loss: 0.1306\n",
      "[Trial 157] Epoch 51/60, Training Loss: 0.1297, Validation Loss: 0.1258\n",
      "[Trial 159] Epoch 34/60, Training Loss: 0.1267, Validation Loss: 0.1276\n",
      "[Trial 161] Epoch 17/60, Training Loss: 0.1286, Validation Loss: 0.1514\n",
      "[Trial 162] Epoch 13/60, Training Loss: 0.1276, Validation Loss: 0.1847\n",
      "[Trial 158] Epoch 40/60, Training Loss: 0.1437, Validation Loss: 0.1270\n",
      "[Trial 160] Epoch 30/60, Training Loss: 0.1271, Validation Loss: 0.1295\n",
      "[Trial 159] Epoch 35/60, Training Loss: 0.1263, Validation Loss: 0.1275\n",
      "[Trial 157] Epoch 52/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 161] Epoch 18/60, Training Loss: 0.1275, Validation Loss: 0.1484\n",
      "[Trial 162] Epoch 14/60, Training Loss: 0.1272, Validation Loss: 0.1821\n",
      "[Trial 158] Epoch 41/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 160] Epoch 31/60, Training Loss: 0.1268, Validation Loss: 0.1288\n",
      "[Trial 159] Epoch 36/60, Training Loss: 0.1362, Validation Loss: 0.1274\n",
      "[Trial 157] Epoch 53/60, Training Loss: 0.1259, Validation Loss: 0.1259\n",
      "[Trial 161] Epoch 19/60, Training Loss: 0.1279, Validation Loss: 0.1453\n",
      "[Trial 158] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1269\n",
      "[Trial 162] Epoch 15/60, Training Loss: 0.1272, Validation Loss: 0.1781\n",
      "[Trial 159] Epoch 37/60, Training Loss: 0.1324, Validation Loss: 0.1271\n",
      "[Trial 160] Epoch 32/60, Training Loss: 0.1284, Validation Loss: 0.1281\n",
      "[Trial 157] Epoch 54/60, Training Loss: 0.1260, Validation Loss: 0.1258\n",
      "[Trial 161] Epoch 20/60, Training Loss: 0.1276, Validation Loss: 0.1431\n",
      "[Trial 158] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1269\n",
      "[Trial 159] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1272\n",
      "[Trial 153] Epoch 22/60, Training Loss: 0.1249, Validation Loss: 0.1278\n",
      "[Trial 162] Epoch 16/60, Training Loss: 0.1269, Validation Loss: 0.1737\n",
      "[Trial 160] Epoch 33/60, Training Loss: 0.1263, Validation Loss: 0.1275\n",
      "[Trial 157] Epoch 55/60, Training Loss: 0.1259, Validation Loss: 0.1260\n",
      "[Trial 161] Epoch 21/60, Training Loss: 0.1291, Validation Loss: 0.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:14:38,544] Trial 158 finished with value: 0.12684386869271597 and parameters: {'hidden_dim': 64, 'latent_dim': 128, 'learning_rate': 0.004154277606479838, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 158] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1269\n",
      "[Trial 158] Early stopping after 44 epochs.\n",
      "[Trial 159] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1271\n",
      "[Trial 157] Epoch 56/60, Training Loss: 0.1382, Validation Loss: 0.1284\n",
      "[Trial 160] Epoch 34/60, Training Loss: 0.1263, Validation Loss: 0.1267\n",
      "[Trial 162] Epoch 17/60, Training Loss: 0.1269, Validation Loss: 0.1696\n",
      "[Trial 163] Epoch 1/60, Training Loss: 9.5371, Validation Loss: 0.2567\n",
      "[Trial 159] Epoch 40/60, Training Loss: 0.1255, Validation Loss: 0.1270\n",
      "[Trial 161] Epoch 22/60, Training Loss: 0.1273, Validation Loss: 0.1373\n",
      "[Trial 157] Epoch 57/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 160] Epoch 35/60, Training Loss: 0.1261, Validation Loss: 0.1262\n",
      "[Trial 162] Epoch 18/60, Training Loss: 0.1268, Validation Loss: 0.1660\n",
      "[Trial 159] Epoch 41/60, Training Loss: 0.1257, Validation Loss: 0.1272\n",
      "[Trial 163] Epoch 2/60, Training Loss: 0.1413, Validation Loss: 0.2199\n",
      "[Trial 161] Epoch 23/60, Training Loss: 0.1273, Validation Loss: 0.1355\n",
      "[Trial 157] Epoch 58/60, Training Loss: 0.1256, Validation Loss: 0.1257\n",
      "[Trial 160] Epoch 36/60, Training Loss: 0.1261, Validation Loss: 0.1257\n",
      "[Trial 159] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1269\n",
      "[Trial 162] Epoch 19/60, Training Loss: 0.1269, Validation Loss: 0.1628\n",
      "[Trial 163] Epoch 3/60, Training Loss: 0.1335, Validation Loss: 0.2089\n",
      "[Trial 161] Epoch 24/60, Training Loss: 0.1275, Validation Loss: 0.1330\n",
      "[Trial 157] Epoch 59/60, Training Loss: 0.1256, Validation Loss: 0.1257\n",
      "[Trial 160] Epoch 37/60, Training Loss: 0.1302, Validation Loss: 0.1254\n",
      "[Trial 159] Epoch 43/60, Training Loss: 0.1254, Validation Loss: 0.1270\n",
      "[Trial 163] Epoch 4/60, Training Loss: 0.1312, Validation Loss: 0.1977\n",
      "[Trial 153] Epoch 23/60, Training Loss: 0.1247, Validation Loss: 0.1278\n",
      "[Trial 162] Epoch 20/60, Training Loss: 0.1268, Validation Loss: 0.1595\n",
      "[Trial 161] Epoch 25/60, Training Loss: 0.1275, Validation Loss: 0.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:15:14,741] Trial 157 finished with value: 0.12568021963040035 and parameters: {'hidden_dim': 64, 'latent_dim': 128, 'learning_rate': 0.0034230887899557726, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 157] Epoch 60/60, Training Loss: 0.1257, Validation Loss: 0.1257\n",
      "[Trial 159] Epoch 44/60, Training Loss: 0.1258, Validation Loss: 0.1272\n",
      "[Trial 160] Epoch 38/60, Training Loss: 0.1263, Validation Loss: 0.1252\n",
      "[Trial 163] Epoch 5/60, Training Loss: 0.1306, Validation Loss: 0.1923\n",
      "[Trial 162] Epoch 21/60, Training Loss: 0.1267, Validation Loss: 0.1570\n",
      "[Trial 161] Epoch 26/60, Training Loss: 0.1300, Validation Loss: 0.1299\n",
      "[Trial 164] Epoch 1/60, Training Loss: 419.5882, Validation Loss: 0.3412\n",
      "[Trial 159] Epoch 45/60, Training Loss: 0.1341, Validation Loss: 0.1363\n",
      "[Trial 160] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1249\n",
      "[Trial 163] Epoch 6/60, Training Loss: 0.1299, Validation Loss: 0.1861\n",
      "[Trial 161] Epoch 27/60, Training Loss: 0.1270, Validation Loss: 0.1285\n",
      "[Trial 162] Epoch 22/60, Training Loss: 0.1266, Validation Loss: 0.1542\n",
      "[Trial 159] Epoch 46/60, Training Loss: 0.1318, Validation Loss: 0.1272\n",
      "[Trial 164] Epoch 2/60, Training Loss: 0.1566, Validation Loss: 0.1850\n",
      "[Trial 160] Epoch 40/60, Training Loss: 0.1261, Validation Loss: 0.1248\n",
      "[Trial 163] Epoch 7/60, Training Loss: 0.1289, Validation Loss: 0.1787\n",
      "[Trial 159] Epoch 47/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 161] Epoch 28/60, Training Loss: 0.1269, Validation Loss: 0.1275\n",
      "[Trial 164] Epoch 3/60, Training Loss: 0.1303, Validation Loss: 0.1695\n",
      "[Trial 162] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1514\n",
      "[Trial 160] Epoch 41/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 163] Epoch 8/60, Training Loss: 0.1287, Validation Loss: 0.1715\n",
      "[Trial 159] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 161] Epoch 29/60, Training Loss: 0.1271, Validation Loss: 0.1264\n",
      "[Trial 164] Epoch 4/60, Training Loss: 0.1289, Validation Loss: 0.1566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:15:49,976] Trial 153 finished with value: 0.12772864438593387 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.000600627652672856, 'batch_size': 8, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 153] Epoch 24/60, Training Loss: 0.1249, Validation Loss: 0.1278\n",
      "[Trial 153] Early stopping after 24 epochs.\n",
      "[Trial 162] Epoch 24/60, Training Loss: 0.1264, Validation Loss: 0.1489\n",
      "[Trial 160] Epoch 42/60, Training Loss: 0.1305, Validation Loss: 0.1246\n",
      "[Trial 163] Epoch 9/60, Training Loss: 0.1286, Validation Loss: 0.1658\n",
      "[Trial 159] Epoch 49/60, Training Loss: 0.1254, Validation Loss: 0.1268\n",
      "[Trial 161] Epoch 30/60, Training Loss: 0.1267, Validation Loss: 0.1255\n",
      "[Trial 164] Epoch 5/60, Training Loss: 0.1279, Validation Loss: 0.1469\n",
      "[Trial 165] Epoch 1/60, Training Loss: 11.0491, Validation Loss: 0.2799\n",
      "[Trial 160] Epoch 43/60, Training Loss: 0.1262, Validation Loss: 0.1246\n",
      "[Trial 163] Epoch 10/60, Training Loss: 0.1283, Validation Loss: 0.1610\n",
      "[Trial 162] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1468\n",
      "[Trial 159] Epoch 50/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 164] Epoch 6/60, Training Loss: 0.2056, Validation Loss: 0.1841\n",
      "[Trial 161] Epoch 31/60, Training Loss: 0.1289, Validation Loss: 0.1251\n",
      "[Trial 165] Epoch 2/60, Training Loss: 0.1539, Validation Loss: 0.2251\n",
      "[Trial 163] Epoch 11/60, Training Loss: 0.1278, Validation Loss: 0.1562\n",
      "[Trial 160] Epoch 44/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 159] Epoch 51/60, Training Loss: 0.1253, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1443\n",
      "[Trial 164] Epoch 7/60, Training Loss: 0.1308, Validation Loss: 0.1342\n",
      "[Trial 161] Epoch 32/60, Training Loss: 0.1449, Validation Loss: 0.1244\n",
      "[Trial 163] Epoch 12/60, Training Loss: 0.1281, Validation Loss: 0.1510\n",
      "[Trial 165] Epoch 3/60, Training Loss: 0.1384, Validation Loss: 0.2119\n",
      "[Trial 160] Epoch 45/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 159] Epoch 52/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1425\n",
      "[Trial 164] Epoch 8/60, Training Loss: 0.1269, Validation Loss: 0.1309\n",
      "[Trial 161] Epoch 33/60, Training Loss: 0.1268, Validation Loss: 0.1238\n",
      "[Trial 163] Epoch 13/60, Training Loss: 0.1278, Validation Loss: 0.1473\n",
      "[Trial 160] Epoch 46/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 159] Epoch 53/60, Training Loss: 0.1252, Validation Loss: 0.1270\n",
      "[Trial 165] Epoch 4/60, Training Loss: 0.1346, Validation Loss: 0.2027\n",
      "[Trial 162] Epoch 28/60, Training Loss: 0.1264, Validation Loss: 0.1407\n",
      "[Trial 164] Epoch 9/60, Training Loss: 0.1268, Validation Loss: 0.1289\n",
      "[Trial 163] Epoch 14/60, Training Loss: 0.1280, Validation Loss: 0.1429\n",
      "[Trial 159] Epoch 54/60, Training Loss: 0.1251, Validation Loss: 0.1269\n",
      "[Trial 161] Epoch 34/60, Training Loss: 0.1265, Validation Loss: 0.1233\n",
      "[Trial 160] Epoch 47/60, Training Loss: 0.1259, Validation Loss: 0.1245\n",
      "[Trial 165] Epoch 5/60, Training Loss: 0.1326, Validation Loss: 0.1941\n",
      "[Trial 164] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1276\n",
      "[Trial 159] Epoch 55/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1389\n",
      "[Trial 163] Epoch 15/60, Training Loss: 0.1276, Validation Loss: 0.1388\n",
      "[Trial 160] Epoch 48/60, Training Loss: 0.1332, Validation Loss: 0.1249\n",
      "[Trial 161] Epoch 35/60, Training Loss: 0.1266, Validation Loss: 0.1230\n",
      "[Trial 165] Epoch 6/60, Training Loss: 0.1312, Validation Loss: 0.1871\n",
      "[Trial 159] Epoch 56/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 164] Epoch 11/60, Training Loss: 0.1261, Validation Loss: 0.1271\n",
      "[Trial 163] Epoch 16/60, Training Loss: 0.1275, Validation Loss: 0.1363\n",
      "[Trial 160] Epoch 49/60, Training Loss: 0.1265, Validation Loss: 0.1246\n",
      "[Trial 162] Epoch 30/60, Training Loss: 1.4188, Validation Loss: 0.2968\n",
      "[Trial 161] Epoch 36/60, Training Loss: 0.1265, Validation Loss: 0.1229\n",
      "[Trial 165] Epoch 7/60, Training Loss: 0.1302, Validation Loss: 0.1801\n",
      "[Trial 159] Epoch 57/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 164] Epoch 12/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 163] Epoch 17/60, Training Loss: 0.1273, Validation Loss: 0.1333\n",
      "[Trial 160] Epoch 50/60, Training Loss: 0.1259, Validation Loss: 0.1246\n",
      "[Trial 161] Epoch 37/60, Training Loss: 0.1267, Validation Loss: 0.1228\n",
      "[Trial 165] Epoch 8/60, Training Loss: 0.1298, Validation Loss: 0.1760\n",
      "[Trial 162] Epoch 31/60, Training Loss: 0.1508, Validation Loss: 0.1362\n",
      "[Trial 159] Epoch 58/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 164] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1268\n",
      "[Trial 163] Epoch 18/60, Training Loss: 0.1299, Validation Loss: 0.1310\n",
      "[Trial 160] Epoch 51/60, Training Loss: 0.1257, Validation Loss: 0.1246\n",
      "[Trial 161] Epoch 38/60, Training Loss: 0.1268, Validation Loss: 0.1228\n",
      "[Trial 165] Epoch 9/60, Training Loss: 0.1292, Validation Loss: 0.1739\n",
      "[Trial 162] Epoch 32/60, Training Loss: 0.1259, Validation Loss: 0.1343\n",
      "[Trial 159] Epoch 59/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 164] Epoch 14/60, Training Loss: 0.1259, Validation Loss: 0.1268\n",
      "[Trial 163] Epoch 19/60, Training Loss: 0.1268, Validation Loss: 0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:17:17,605] Trial 160 finished with value: 0.1245001196861267 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0037337968565317628, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 160] Epoch 52/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 160] Early stopping after 52 epochs.\n",
      "[Trial 161] Epoch 39/60, Training Loss: 0.1266, Validation Loss: 0.1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:17:20,579] Trial 159 finished with value: 0.12681973328193027 and parameters: {'hidden_dim': 64, 'latent_dim': 128, 'learning_rate': 0.003610675124620542, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 165] Epoch 10/60, Training Loss: 0.1290, Validation Loss: 0.1682\n",
      "[Trial 159] Epoch 60/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 33/60, Training Loss: 0.1258, Validation Loss: 0.1334\n",
      "[Trial 164] Epoch 15/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 163] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1275\n",
      "[Trial 166] Epoch 1/60, Training Loss: 17.2991, Validation Loss: 0.2640\n",
      "[Trial 161] Epoch 40/60, Training Loss: 0.1266, Validation Loss: 0.1228\n",
      "[Trial 165] Epoch 11/60, Training Loss: 0.1281, Validation Loss: 0.1666\n",
      "[Trial 162] Epoch 34/60, Training Loss: 0.1258, Validation Loss: 0.1324\n",
      "[Trial 164] Epoch 16/60, Training Loss: 0.1259, Validation Loss: 0.1269\n",
      "[Trial 163] Epoch 21/60, Training Loss: 0.1279, Validation Loss: 0.1266\n",
      "[Trial 166] Epoch 2/60, Training Loss: 0.1339, Validation Loss: 0.2370\n",
      "[Trial 161] Epoch 41/60, Training Loss: 0.1265, Validation Loss: 0.1227\n",
      "[Trial 165] Epoch 12/60, Training Loss: 0.1282, Validation Loss: 0.1629\n",
      "[Trial 162] Epoch 35/60, Training Loss: 0.1260, Validation Loss: 0.1312\n",
      "[Trial 164] Epoch 17/60, Training Loss: 0.1260, Validation Loss: 0.1270\n",
      "[Trial 163] Epoch 22/60, Training Loss: 0.1281, Validation Loss: 0.1258\n",
      "[Trial 166] Epoch 3/60, Training Loss: 0.1289, Validation Loss: 0.2283\n",
      "[Trial 161] Epoch 42/60, Training Loss: 0.1267, Validation Loss: 0.1226\n",
      "[Trial 165] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.1617\n",
      "[Trial 164] Epoch 18/60, Training Loss: 0.1257, Validation Loss: 0.1268\n",
      "[Trial 163] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1252\n",
      "[Trial 162] Epoch 36/60, Training Loss: 0.1258, Validation Loss: 0.1306\n",
      "[Trial 166] Epoch 4/60, Training Loss: 0.1277, Validation Loss: 0.2222\n",
      "[Trial 161] Epoch 43/60, Training Loss: 0.1267, Validation Loss: 0.1226\n",
      "[Trial 165] Epoch 14/60, Training Loss: 0.1281, Validation Loss: 0.1574\n",
      "[Trial 163] Epoch 24/60, Training Loss: 0.1266, Validation Loss: 0.1247\n",
      "[Trial 164] Epoch 19/60, Training Loss: 0.1255, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1300\n",
      "[Trial 165] Epoch 15/60, Training Loss: 0.1280, Validation Loss: 0.1556\n",
      "[Trial 161] Epoch 44/60, Training Loss: 0.1289, Validation Loss: 0.1227\n",
      "[Trial 166] Epoch 5/60, Training Loss: 0.1277, Validation Loss: 0.2149\n",
      "[Trial 167] Epoch 1/60, Training Loss: 9.9063, Validation Loss: 0.1726\n",
      "[Trial 163] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1245\n",
      "[Trial 164] Epoch 20/60, Training Loss: 0.1254, Validation Loss: 0.1269\n",
      "[Trial 162] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1295\n",
      "[Trial 161] Epoch 45/60, Training Loss: 0.1266, Validation Loss: 0.1227\n",
      "[Trial 165] Epoch 16/60, Training Loss: 0.1277, Validation Loss: 0.1512\n",
      "[Trial 166] Epoch 6/60, Training Loss: 0.1274, Validation Loss: 0.2093\n",
      "[Trial 163] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1243\n",
      "[Trial 164] Epoch 21/60, Training Loss: 0.1253, Validation Loss: 0.1267\n",
      "[Trial 162] Epoch 39/60, Training Loss: 0.1256, Validation Loss: 0.1290\n",
      "[Trial 165] Epoch 17/60, Training Loss: 0.1283, Validation Loss: 0.1501\n",
      "[Trial 161] Epoch 46/60, Training Loss: 0.1267, Validation Loss: 0.1227\n",
      "[Trial 166] Epoch 7/60, Training Loss: 0.1273, Validation Loss: 0.2033\n",
      "[Trial 163] Epoch 27/60, Training Loss: 0.1263, Validation Loss: 0.1241\n",
      "[Trial 164] Epoch 22/60, Training Loss: 0.1253, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1286\n",
      "[Trial 165] Epoch 18/60, Training Loss: 0.1294, Validation Loss: 0.1487\n",
      "[Trial 161] Epoch 47/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 163] Epoch 28/60, Training Loss: 0.1305, Validation Loss: 0.1243\n",
      "[Trial 166] Epoch 8/60, Training Loss: 0.1274, Validation Loss: 0.1984\n",
      "[Trial 164] Epoch 23/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 165] Epoch 19/60, Training Loss: 0.1296, Validation Loss: 0.1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:18:41,321] Trial 161 finished with value: 0.12264082332452138 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.004300215046244305, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 161] Epoch 48/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 161] Early stopping after 48 epochs.\n",
      "[Trial 162] Epoch 41/60, Training Loss: 0.1255, Validation Loss: 0.1282\n",
      "[Trial 163] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1241\n",
      "[Trial 164] Epoch 24/60, Training Loss: 0.1254, Validation Loss: 0.1269\n",
      "[Trial 166] Epoch 9/60, Training Loss: 0.1273, Validation Loss: 0.1954\n",
      "[Trial 165] Epoch 20/60, Training Loss: 0.1272, Validation Loss: 0.1426\n",
      "[Trial 168] Epoch 1/60, Training Loss: 10.0987, Validation Loss: 0.2721\n",
      "[Trial 163] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1241\n",
      "[Trial 162] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1279\n",
      "[Trial 164] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 167] Epoch 2/60, Training Loss: 0.1379, Validation Loss: 0.1311\n",
      "[Trial 166] Epoch 10/60, Training Loss: 0.1273, Validation Loss: 0.1904\n",
      "[Trial 165] Epoch 21/60, Training Loss: 0.1275, Validation Loss: 0.1403\n",
      "[Trial 163] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1240\n",
      "[Trial 168] Epoch 2/60, Training Loss: 0.1466, Validation Loss: 0.2284\n",
      "[Trial 164] Epoch 26/60, Training Loss: 0.1252, Validation Loss: 0.1268\n",
      "[Trial 162] Epoch 43/60, Training Loss: 0.1257, Validation Loss: 0.1278\n",
      "[Trial 166] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.1847\n",
      "[Trial 163] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1240\n",
      "[Trial 165] Epoch 22/60, Training Loss: 0.1364, Validation Loss: 0.1390\n",
      "[Trial 168] Epoch 3/60, Training Loss: 0.1359, Validation Loss: 0.2134\n",
      "[Trial 164] Epoch 27/60, Training Loss: 0.1252, Validation Loss: 0.1270\n",
      "[Trial 162] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1276\n",
      "[Trial 166] Epoch 12/60, Training Loss: 0.1274, Validation Loss: 0.1812\n",
      "[Trial 163] Epoch 33/60, Training Loss: 0.1287, Validation Loss: 0.1240\n",
      "[Trial 165] Epoch 23/60, Training Loss: 0.1273, Validation Loss: 0.1379\n",
      "[Trial 168] Epoch 4/60, Training Loss: 0.1324, Validation Loss: 0.2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:19:18,626] Trial 164 finished with value: 0.1267458771665891 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.019559346718549135, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 164] Epoch 28/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 164] Early stopping after 28 epochs.\n",
      "[Trial 162] Epoch 45/60, Training Loss: 0.1254, Validation Loss: 0.1275\n",
      "[Trial 166] Epoch 13/60, Training Loss: 0.1275, Validation Loss: 0.1761\n",
      "[Trial 163] Epoch 34/60, Training Loss: 0.1264, Validation Loss: 0.1240\n",
      "[Trial 165] Epoch 24/60, Training Loss: 0.1268, Validation Loss: 0.1354\n",
      "[Trial 168] Epoch 5/60, Training Loss: 0.1307, Validation Loss: 0.1924\n",
      "[Trial 169] Epoch 1/60, Training Loss: 9.6718, Validation Loss: 0.2732\n",
      "[Trial 162] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1273\n",
      "[Trial 163] Epoch 35/60, Training Loss: 0.1264, Validation Loss: 0.1240\n",
      "[Trial 166] Epoch 14/60, Training Loss: 0.1268, Validation Loss: 0.1715\n",
      "[Trial 165] Epoch 25/60, Training Loss: 0.1269, Validation Loss: 0.1332\n",
      "[Trial 168] Epoch 6/60, Training Loss: 0.1300, Validation Loss: 0.1856\n",
      "[Trial 169] Epoch 2/60, Training Loss: 0.1452, Validation Loss: 0.2305\n",
      "[Trial 167] Epoch 3/60, Training Loss: 0.1333, Validation Loss: 0.1242\n",
      "[Trial 163] Epoch 36/60, Training Loss: 0.1303, Validation Loss: 0.1240\n",
      "[Trial 162] Epoch 47/60, Training Loss: 0.1254, Validation Loss: 0.1273\n",
      "[Trial 165] Epoch 26/60, Training Loss: 0.1270, Validation Loss: 0.1326\n",
      "[Trial 168] Epoch 7/60, Training Loss: 0.1291, Validation Loss: 0.1805\n",
      "[Trial 169] Epoch 3/60, Training Loss: 0.1351, Validation Loss: 0.2165\n",
      "[Trial 166] Epoch 15/60, Training Loss: 0.1276, Validation Loss: 0.1670\n",
      "[Trial 163] Epoch 37/60, Training Loss: 0.1265, Validation Loss: 0.1239\n",
      "[Trial 165] Epoch 27/60, Training Loss: 0.1340, Validation Loss: 0.1313\n",
      "[Trial 162] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1273\n",
      "[Trial 169] Epoch 4/60, Training Loss: 0.1326, Validation Loss: 0.2067\n",
      "[Trial 168] Epoch 8/60, Training Loss: 0.1289, Validation Loss: 0.1766\n",
      "[Trial 166] Epoch 16/60, Training Loss: 0.1274, Validation Loss: 0.1607\n",
      "[Trial 163] Epoch 38/60, Training Loss: 0.1262, Validation Loss: 0.1239\n",
      "[Trial 169] Epoch 5/60, Training Loss: 0.1311, Validation Loss: 0.1980\n",
      "[Trial 165] Epoch 28/60, Training Loss: 0.1265, Validation Loss: 0.1301\n",
      "[Trial 168] Epoch 9/60, Training Loss: 0.1283, Validation Loss: 0.1704\n",
      "[Trial 162] Epoch 49/60, Training Loss: 0.1253, Validation Loss: 0.1272\n",
      "[Trial 166] Epoch 17/60, Training Loss: 0.1272, Validation Loss: 0.1564\n",
      "[Trial 163] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1240\n",
      "[Trial 169] Epoch 6/60, Training Loss: 0.1302, Validation Loss: 0.1891\n",
      "[Trial 165] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1290\n",
      "[Trial 168] Epoch 10/60, Training Loss: 0.1284, Validation Loss: 0.1669\n",
      "[Trial 162] Epoch 50/60, Training Loss: 0.1253, Validation Loss: 0.1272\n",
      "[Trial 163] Epoch 40/60, Training Loss: 0.1260, Validation Loss: 0.1240\n",
      "[Trial 166] Epoch 18/60, Training Loss: 0.1266, Validation Loss: 0.1525\n",
      "[Trial 169] Epoch 7/60, Training Loss: 0.1294, Validation Loss: 0.1831\n",
      "[Trial 168] Epoch 11/60, Training Loss: 0.1332, Validation Loss: 0.1619\n",
      "[Trial 165] Epoch 30/60, Training Loss: 0.1268, Validation Loss: 0.1283\n",
      "[Trial 162] Epoch 51/60, Training Loss: 0.1254, Validation Loss: 0.1272\n",
      "[Trial 163] Epoch 41/60, Training Loss: 0.1259, Validation Loss: 0.1240\n",
      "[Trial 166] Epoch 19/60, Training Loss: 0.1269, Validation Loss: 0.1488\n",
      "[Trial 169] Epoch 8/60, Training Loss: 0.1291, Validation Loss: 0.1784\n",
      "[Trial 167] Epoch 4/60, Training Loss: 0.1759, Validation Loss: 0.1241\n",
      "[Trial 168] Epoch 12/60, Training Loss: 0.1278, Validation Loss: 0.1573\n",
      "[Trial 165] Epoch 31/60, Training Loss: 0.1263, Validation Loss: 0.1275\n",
      "[Trial 163] Epoch 42/60, Training Loss: 0.1258, Validation Loss: 0.1240\n",
      "[Trial 162] Epoch 52/60, Training Loss: 0.1254, Validation Loss: 0.1272\n",
      "[Trial 166] Epoch 20/60, Training Loss: 0.1268, Validation Loss: 0.1455\n",
      "[Trial 169] Epoch 9/60, Training Loss: 0.1287, Validation Loss: 0.1718\n",
      "[Trial 168] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.1535\n",
      "[Trial 165] Epoch 32/60, Training Loss: 0.1472, Validation Loss: 0.1344\n",
      "[Trial 163] Epoch 43/60, Training Loss: 0.1259, Validation Loss: 0.1240\n",
      "[Trial 162] Epoch 53/60, Training Loss: 0.1255, Validation Loss: 0.1272\n",
      "[Trial 169] Epoch 10/60, Training Loss: 0.1281, Validation Loss: 0.1681\n",
      "[Trial 166] Epoch 21/60, Training Loss: 0.1269, Validation Loss: 0.1420\n",
      "[Trial 168] Epoch 14/60, Training Loss: 0.1320, Validation Loss: 0.1494\n",
      "[Trial 165] Epoch 33/60, Training Loss: 0.1277, Validation Loss: 0.1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:20:48,966] Trial 163 finished with value: 0.12394133061170579 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.005136298863790964, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 163] Epoch 44/60, Training Loss: 0.1258, Validation Loss: 0.1240\n",
      "[Trial 163] Early stopping after 44 epochs.\n",
      "[Trial 169] Epoch 11/60, Training Loss: 0.1280, Validation Loss: 0.1625\n",
      "[Trial 162] Epoch 54/60, Training Loss: 0.1255, Validation Loss: 0.1272\n",
      "[Trial 166] Epoch 22/60, Training Loss: 0.1267, Validation Loss: 0.1394\n",
      "[Trial 165] Epoch 34/60, Training Loss: 0.1265, Validation Loss: 0.1261\n",
      "[Trial 168] Epoch 15/60, Training Loss: 0.1282, Validation Loss: 0.1456\n",
      "[Trial 170] Epoch 1/60, Training Loss: 14.3410, Validation Loss: 0.3263\n",
      "[Trial 169] Epoch 12/60, Training Loss: 0.1283, Validation Loss: 0.1579\n",
      "[Trial 162] Epoch 55/60, Training Loss: 0.1252, Validation Loss: 0.1272\n",
      "[Trial 165] Epoch 35/60, Training Loss: 0.1263, Validation Loss: 0.1258\n",
      "[Trial 168] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1422\n",
      "[Trial 166] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1362\n",
      "[Trial 170] Epoch 2/60, Training Loss: 0.1885, Validation Loss: 0.2394\n",
      "[Trial 169] Epoch 13/60, Training Loss: 0.1276, Validation Loss: 0.1544\n",
      "[Trial 162] Epoch 56/60, Training Loss: 0.1250, Validation Loss: 0.1272\n",
      "[Trial 170] Epoch 3/60, Training Loss: 0.1567, Validation Loss: 0.2195\n",
      "[Trial 165] Epoch 36/60, Training Loss: 0.1262, Validation Loss: 0.1254\n",
      "[Trial 168] Epoch 17/60, Training Loss: 0.1267, Validation Loss: 0.1393\n",
      "[Trial 167] Epoch 5/60, Training Loss: 0.1336, Validation Loss: 0.1237\n",
      "[Trial 166] Epoch 24/60, Training Loss: 0.1265, Validation Loss: 0.1338\n",
      "[Trial 169] Epoch 14/60, Training Loss: 0.1275, Validation Loss: 0.1507\n",
      "[Trial 170] Epoch 4/60, Training Loss: 0.1475, Validation Loss: 0.2082\n",
      "[Trial 165] Epoch 37/60, Training Loss: 0.1263, Validation Loss: 0.1251\n",
      "[Trial 168] Epoch 18/60, Training Loss: 0.1308, Validation Loss: 0.1364\n",
      "[Trial 162] Epoch 57/60, Training Loss: 0.1250, Validation Loss: 0.1272\n",
      "[Trial 166] Epoch 25/60, Training Loss: 0.1284, Validation Loss: 0.1325\n",
      "[Trial 169] Epoch 15/60, Training Loss: 0.1275, Validation Loss: 0.1465\n",
      "[Trial 170] Epoch 5/60, Training Loss: 0.1423, Validation Loss: 0.1987\n",
      "[Trial 165] Epoch 38/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 168] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1343\n",
      "[Trial 162] Epoch 58/60, Training Loss: 0.1251, Validation Loss: 0.1272\n",
      "[Trial 166] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1308\n",
      "[Trial 169] Epoch 16/60, Training Loss: 0.1274, Validation Loss: 0.1433\n",
      "[Trial 170] Epoch 6/60, Training Loss: 0.1392, Validation Loss: 0.1906\n",
      "[Trial 165] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 168] Epoch 20/60, Training Loss: 0.1269, Validation Loss: 0.1319\n",
      "[Trial 162] Epoch 59/60, Training Loss: 0.1250, Validation Loss: 0.1272\n",
      "[Trial 169] Epoch 17/60, Training Loss: 0.1274, Validation Loss: 0.1407\n",
      "[Trial 166] Epoch 27/60, Training Loss: 0.1265, Validation Loss: 0.1297\n",
      "[Trial 170] Epoch 7/60, Training Loss: 0.1372, Validation Loss: 0.1853\n",
      "[Trial 165] Epoch 40/60, Training Loss: 0.1261, Validation Loss: 0.1246\n",
      "[Trial 168] Epoch 21/60, Training Loss: 0.1302, Validation Loss: 0.1308\n",
      "[Trial 169] Epoch 18/60, Training Loss: 0.1270, Validation Loss: 0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:21:55,013] Trial 162 finished with value: 0.12716757903496426 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.003436058116737757, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 162] Epoch 60/60, Training Loss: 0.1249, Validation Loss: 0.1272\n",
      "[Trial 170] Epoch 8/60, Training Loss: 0.1353, Validation Loss: 0.1795\n",
      "[Trial 166] Epoch 28/60, Training Loss: 0.1265, Validation Loss: 0.1290\n",
      "[Trial 165] Epoch 41/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 168] Epoch 22/60, Training Loss: 0.1281, Validation Loss: 0.1292\n",
      "[Trial 167] Epoch 6/60, Training Loss: 0.1321, Validation Loss: 0.1240\n",
      "[Trial 169] Epoch 19/60, Training Loss: 0.1267, Validation Loss: 0.1344\n",
      "[Trial 170] Epoch 9/60, Training Loss: 0.1340, Validation Loss: 0.1734\n",
      "[Trial 166] Epoch 29/60, Training Loss: 0.1272, Validation Loss: 0.1283\n",
      "[Trial 165] Epoch 42/60, Training Loss: 0.1261, Validation Loss: 0.1244\n",
      "[Trial 168] Epoch 23/60, Training Loss: 0.1259, Validation Loss: 0.1285\n",
      "[Trial 171] Epoch 1/60, Training Loss: 3.4004, Validation Loss: 0.2406\n",
      "[Trial 169] Epoch 20/60, Training Loss: 0.1268, Validation Loss: 0.1324\n",
      "[Trial 170] Epoch 10/60, Training Loss: 0.1332, Validation Loss: 0.1713\n",
      "[Trial 165] Epoch 43/60, Training Loss: 0.1263, Validation Loss: 0.1244\n",
      "[Trial 166] Epoch 30/60, Training Loss: 0.1321, Validation Loss: 0.1279\n",
      "[Trial 168] Epoch 24/60, Training Loss: 0.1259, Validation Loss: 0.1278\n",
      "[Trial 169] Epoch 21/60, Training Loss: 0.1266, Validation Loss: 0.1312\n",
      "[Trial 170] Epoch 11/60, Training Loss: 0.1323, Validation Loss: 0.1676\n",
      "[Trial 171] Epoch 2/60, Training Loss: 0.1390, Validation Loss: 0.2207\n",
      "[Trial 165] Epoch 44/60, Training Loss: 0.1260, Validation Loss: 0.1244\n",
      "[Trial 168] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1273\n",
      "[Trial 166] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1275\n",
      "[Trial 169] Epoch 22/60, Training Loss: 0.1265, Validation Loss: 0.1297\n",
      "[Trial 170] Epoch 12/60, Training Loss: 0.1314, Validation Loss: 0.1640\n",
      "[Trial 165] Epoch 45/60, Training Loss: 0.1264, Validation Loss: 0.1254\n",
      "[Trial 168] Epoch 26/60, Training Loss: 0.1260, Validation Loss: 0.1269\n",
      "[Trial 166] Epoch 32/60, Training Loss: 0.1261, Validation Loss: 0.1273\n",
      "[Trial 171] Epoch 3/60, Training Loss: 0.1325, Validation Loss: 0.2059\n",
      "[Trial 170] Epoch 13/60, Training Loss: 0.1310, Validation Loss: 0.1603\n",
      "[Trial 169] Epoch 23/60, Training Loss: 0.1286, Validation Loss: 0.1286\n",
      "[Trial 165] Epoch 46/60, Training Loss: 0.1375, Validation Loss: 0.1252\n",
      "[Trial 168] Epoch 27/60, Training Loss: 0.1261, Validation Loss: 0.1270\n",
      "[Trial 170] Epoch 14/60, Training Loss: 0.1306, Validation Loss: 0.1589\n",
      "[Trial 169] Epoch 24/60, Training Loss: 0.1268, Validation Loss: 0.1277\n",
      "[Trial 166] Epoch 33/60, Training Loss: 0.1261, Validation Loss: 0.1272\n",
      "[Trial 167] Epoch 7/60, Training Loss: 0.1313, Validation Loss: 0.1238\n",
      "[Trial 171] Epoch 4/60, Training Loss: 0.1308, Validation Loss: 0.1984\n",
      "[Trial 165] Epoch 47/60, Training Loss: 0.1265, Validation Loss: 0.1244\n",
      "[Trial 168] Epoch 28/60, Training Loss: 0.1296, Validation Loss: 0.1268\n",
      "[Trial 170] Epoch 15/60, Training Loss: 0.1302, Validation Loss: 0.1576\n",
      "[Trial 169] Epoch 25/60, Training Loss: 0.1265, Validation Loss: 0.1268\n",
      "[Trial 166] Epoch 34/60, Training Loss: 0.1310, Validation Loss: 0.1274\n",
      "[Trial 165] Epoch 48/60, Training Loss: 0.1259, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 29/60, Training Loss: 0.1400, Validation Loss: 0.1270\n",
      "[Trial 170] Epoch 16/60, Training Loss: 0.1301, Validation Loss: 0.1553\n",
      "[Trial 169] Epoch 26/60, Training Loss: 0.1261, Validation Loss: 0.1264\n",
      "[Trial 171] Epoch 5/60, Training Loss: 0.1296, Validation Loss: 0.1898\n",
      "[Trial 166] Epoch 35/60, Training Loss: 0.1308, Validation Loss: 0.1270\n",
      "[Trial 165] Epoch 49/60, Training Loss: 0.1262, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 30/60, Training Loss: 0.1261, Validation Loss: 0.1265\n",
      "[Trial 170] Epoch 17/60, Training Loss: 0.1294, Validation Loss: 0.1545\n",
      "[Trial 169] Epoch 27/60, Training Loss: 0.1263, Validation Loss: 0.1261\n",
      "[Trial 166] Epoch 36/60, Training Loss: 0.1267, Validation Loss: 0.1270\n",
      "[Trial 171] Epoch 6/60, Training Loss: 0.1292, Validation Loss: 0.1868\n",
      "[Trial 165] Epoch 50/60, Training Loss: 0.1262, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 31/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 170] Epoch 18/60, Training Loss: 0.1292, Validation Loss: 0.1527\n",
      "[Trial 169] Epoch 28/60, Training Loss: 0.1336, Validation Loss: 0.1377\n",
      "[Trial 166] Epoch 37/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 165] Epoch 51/60, Training Loss: 0.1262, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 32/60, Training Loss: 0.1255, Validation Loss: 0.1266\n",
      "[Trial 170] Epoch 19/60, Training Loss: 0.1292, Validation Loss: 0.1501\n",
      "[Trial 169] Epoch 29/60, Training Loss: 0.1282, Validation Loss: 0.1256\n",
      "[Trial 171] Epoch 7/60, Training Loss: 0.1287, Validation Loss: 0.1815\n",
      "[Trial 167] Epoch 8/60, Training Loss: 0.1307, Validation Loss: 0.1241\n",
      "[Trial 165] Epoch 52/60, Training Loss: 0.1265, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 166] Epoch 38/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 170] Epoch 20/60, Training Loss: 0.1289, Validation Loss: 0.1493\n",
      "[Trial 169] Epoch 30/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 165] Epoch 53/60, Training Loss: 0.1269, Validation Loss: 0.1248\n",
      "[Trial 168] Epoch 34/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 171] Epoch 8/60, Training Loss: 0.1282, Validation Loss: 0.1750\n",
      "[Trial 170] Epoch 21/60, Training Loss: 0.1284, Validation Loss: 0.1473\n",
      "[Trial 169] Epoch 31/60, Training Loss: 0.1259, Validation Loss: 0.1255\n",
      "[Trial 166] Epoch 39/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 165] Epoch 54/60, Training Loss: 0.1259, Validation Loss: 0.1242\n",
      "[Trial 168] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1265\n",
      "[Trial 170] Epoch 22/60, Training Loss: 0.1286, Validation Loss: 0.1442\n",
      "[Trial 169] Epoch 32/60, Training Loss: 0.1259, Validation Loss: 0.1254\n",
      "[Trial 166] Epoch 40/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 171] Epoch 9/60, Training Loss: 0.1280, Validation Loss: 0.1723\n",
      "[Trial 170] Epoch 23/60, Training Loss: 0.1282, Validation Loss: 0.1444\n",
      "[Trial 165] Epoch 55/60, Training Loss: 0.1260, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1265\n",
      "[Trial 169] Epoch 33/60, Training Loss: 0.1260, Validation Loss: 0.1254\n",
      "[Trial 166] Epoch 41/60, Training Loss: 0.1392, Validation Loss: 0.1354\n",
      "[Trial 170] Epoch 24/60, Training Loss: 0.1281, Validation Loss: 0.1418\n",
      "[Trial 165] Epoch 56/60, Training Loss: 0.1257, Validation Loss: 0.1243\n",
      "[Trial 168] Epoch 37/60, Training Loss: 0.1255, Validation Loss: 0.1265\n",
      "[Trial 169] Epoch 34/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 171] Epoch 10/60, Training Loss: 0.1279, Validation Loss: 0.1668\n",
      "[Trial 166] Epoch 42/60, Training Loss: 0.1361, Validation Loss: 0.1269\n",
      "[Trial 167] Epoch 9/60, Training Loss: 0.1301, Validation Loss: 0.1245\n",
      "[Trial 170] Epoch 25/60, Training Loss: 0.1281, Validation Loss: 0.1415\n",
      "[Trial 165] Epoch 57/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 168] Epoch 38/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 169] Epoch 35/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 166] Epoch 43/60, Training Loss: 0.1260, Validation Loss: 0.1269\n",
      "[Trial 171] Epoch 11/60, Training Loss: 0.1276, Validation Loss: 0.1638\n",
      "[Trial 170] Epoch 26/60, Training Loss: 0.1282, Validation Loss: 0.1407\n",
      "[Trial 165] Epoch 58/60, Training Loss: 0.1257, Validation Loss: 0.1242\n",
      "[Trial 168] Epoch 39/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 169] Epoch 36/60, Training Loss: 0.1264, Validation Loss: 0.1254\n",
      "[Trial 170] Epoch 27/60, Training Loss: 0.1286, Validation Loss: 0.1386\n",
      "[Trial 166] Epoch 44/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 165] Epoch 59/60, Training Loss: 0.1259, Validation Loss: 0.1242\n",
      "[Trial 169] Epoch 37/60, Training Loss: 0.1361, Validation Loss: 0.1256\n",
      "[Trial 168] Epoch 40/60, Training Loss: 0.1254, Validation Loss: 0.1265\n",
      "[Trial 171] Epoch 12/60, Training Loss: 0.1277, Validation Loss: 0.1611\n",
      "[Trial 170] Epoch 28/60, Training Loss: 0.1276, Validation Loss: 0.1366\n",
      "[Trial 166] Epoch 45/60, Training Loss: 0.1259, Validation Loss: 0.1269\n",
      "[Trial 169] Epoch 38/60, Training Loss: 0.1265, Validation Loss: 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:24:40,706] Trial 165 finished with value: 0.12422896176576614 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0037087048087578057, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 165] Epoch 60/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 168] Epoch 41/60, Training Loss: 0.1251, Validation Loss: 0.1265\n",
      "[Trial 170] Epoch 29/60, Training Loss: 0.1293, Validation Loss: 0.1368\n",
      "[Trial 171] Epoch 13/60, Training Loss: 0.1279, Validation Loss: 0.1558\n",
      "[Trial 169] Epoch 39/60, Training Loss: 0.1316, Validation Loss: 0.1273\n",
      "[Trial 168] Epoch 42/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 166] Epoch 46/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 172] Epoch 1/60, Training Loss: 17.8800, Validation Loss: 0.2680\n",
      "[Trial 170] Epoch 30/60, Training Loss: 0.1286, Validation Loss: 0.1364\n",
      "[Trial 169] Epoch 40/60, Training Loss: 0.1267, Validation Loss: 0.1254\n",
      "[Trial 167] Epoch 10/60, Training Loss: 0.1270, Validation Loss: 0.1239\n",
      "[Trial 168] Epoch 43/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 166] Epoch 47/60, Training Loss: 0.1258, Validation Loss: 0.1269\n",
      "[Trial 172] Epoch 2/60, Training Loss: 0.1353, Validation Loss: 0.2404\n",
      "[Trial 171] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1524\n",
      "[Trial 170] Epoch 31/60, Training Loss: 0.1278, Validation Loss: 0.1357\n",
      "[Trial 169] Epoch 41/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 168] Epoch 44/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 166] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1269\n",
      "[Trial 172] Epoch 3/60, Training Loss: 0.1297, Validation Loss: 0.2301\n",
      "[Trial 170] Epoch 32/60, Training Loss: 0.1292, Validation Loss: 0.1363\n",
      "[Trial 169] Epoch 42/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 168] Epoch 45/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 171] Epoch 15/60, Training Loss: 0.1274, Validation Loss: 0.1495\n",
      "[Trial 166] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1270\n",
      "[Trial 170] Epoch 33/60, Training Loss: 0.1281, Validation Loss: 0.1322\n",
      "[Trial 172] Epoch 4/60, Training Loss: 0.1288, Validation Loss: 0.2235\n",
      "[Trial 169] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 168] Epoch 46/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 170] Epoch 34/60, Training Loss: 0.1274, Validation Loss: 0.1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:25:29,654] Trial 166 finished with value: 0.12690107375383378 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.0053194144698349224, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 166] Epoch 50/60, Training Loss: 0.1253, Validation Loss: 0.1270\n",
      "[Trial 166] Early stopping after 50 epochs.\n",
      "[Trial 171] Epoch 16/60, Training Loss: 0.1287, Validation Loss: 0.1573\n",
      "[Trial 172] Epoch 5/60, Training Loss: 0.1280, Validation Loss: 0.2141\n",
      "[Trial 169] Epoch 44/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 168] Epoch 47/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 170] Epoch 35/60, Training Loss: 0.1276, Validation Loss: 0.1322\n",
      "[Trial 169] Epoch 45/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 172] Epoch 6/60, Training Loss: 0.1281, Validation Loss: 0.2089\n",
      "[Trial 168] Epoch 48/60, Training Loss: 0.1251, Validation Loss: 0.1265\n",
      "[Trial 173] Epoch 1/60, Training Loss: 3.1550, Validation Loss: 0.2546\n",
      "[Trial 167] Epoch 11/60, Training Loss: 0.1270, Validation Loss: 0.1238\n",
      "[Trial 171] Epoch 17/60, Training Loss: 0.1283, Validation Loss: 0.1436\n",
      "[Trial 170] Epoch 36/60, Training Loss: 0.1299, Validation Loss: 0.1305\n",
      "[Trial 169] Epoch 46/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 172] Epoch 7/60, Training Loss: 0.1279, Validation Loss: 0.2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:25:51,024] Trial 168 finished with value: 0.12645024309555689 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0049738992293548775, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 168] Epoch 49/60, Training Loss: 0.1251, Validation Loss: 0.1265\n",
      "[Trial 168] Early stopping after 49 epochs.\n",
      "[Trial 170] Epoch 37/60, Training Loss: 0.1278, Validation Loss: 0.1311\n",
      "[Trial 173] Epoch 2/60, Training Loss: 0.1351, Validation Loss: 0.2371\n",
      "[Trial 169] Epoch 47/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 171] Epoch 18/60, Training Loss: 0.1270, Validation Loss: 0.1409\n",
      "[Trial 172] Epoch 8/60, Training Loss: 0.1277, Validation Loss: 0.1990\n",
      "[Trial 170] Epoch 38/60, Training Loss: 0.1272, Validation Loss: 0.1297\n",
      "[Trial 169] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 174] Epoch 1/60, Training Loss: 0.7275, Validation Loss: 0.2269\n",
      "[Trial 173] Epoch 3/60, Training Loss: 0.1308, Validation Loss: 0.2236\n",
      "[Trial 170] Epoch 39/60, Training Loss: 0.1276, Validation Loss: 0.1293\n",
      "[Trial 172] Epoch 9/60, Training Loss: 0.1275, Validation Loss: 0.1932\n",
      "[Trial 171] Epoch 19/60, Training Loss: 0.1267, Validation Loss: 0.1387\n",
      "[Trial 169] Epoch 49/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 170] Epoch 40/60, Training Loss: 0.1287, Validation Loss: 0.1281\n",
      "[Trial 172] Epoch 10/60, Training Loss: 0.1276, Validation Loss: 0.1876\n",
      "[Trial 174] Epoch 2/60, Training Loss: 0.1518, Validation Loss: 0.2081\n",
      "[Trial 169] Epoch 50/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 173] Epoch 4/60, Training Loss: 0.1290, Validation Loss: 0.2106\n",
      "[Trial 171] Epoch 20/60, Training Loss: 0.1266, Validation Loss: 0.1372\n",
      "[Trial 170] Epoch 41/60, Training Loss: 0.1276, Validation Loss: 0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:26:27,262] Trial 167 finished with value: 0.12373582081248363 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.009236942412599538, 'batch_size': 8, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 167] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1238\n",
      "[Trial 167] Early stopping after 12 epochs.\n",
      "[Trial 172] Epoch 11/60, Training Loss: 0.1280, Validation Loss: 0.1818\n",
      "[Trial 169] Epoch 51/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 170] Epoch 42/60, Training Loss: 0.1299, Validation Loss: 0.1279\n",
      "[Trial 174] Epoch 3/60, Training Loss: 0.1411, Validation Loss: 0.1989\n",
      "[Trial 173] Epoch 5/60, Training Loss: 0.1282, Validation Loss: 0.2038\n",
      "[Trial 171] Epoch 21/60, Training Loss: 0.1267, Validation Loss: 0.1356\n",
      "[Trial 169] Epoch 52/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 172] Epoch 12/60, Training Loss: 0.1274, Validation Loss: 0.1760\n",
      "[Trial 175] Epoch 1/60, Training Loss: 1.0274, Validation Loss: 0.2029\n",
      "[Trial 170] Epoch 43/60, Training Loss: 0.1270, Validation Loss: 0.1277\n",
      "[Trial 169] Epoch 53/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 173] Epoch 6/60, Training Loss: 0.1279, Validation Loss: 0.1966\n",
      "[Trial 174] Epoch 4/60, Training Loss: 0.1366, Validation Loss: 0.1921\n",
      "[Trial 172] Epoch 13/60, Training Loss: 0.1269, Validation Loss: 0.1707\n",
      "[Trial 170] Epoch 44/60, Training Loss: 0.1269, Validation Loss: 0.1270\n",
      "[Trial 171] Epoch 22/60, Training Loss: 0.1265, Validation Loss: 0.1340\n",
      "[Trial 175] Epoch 2/60, Training Loss: 0.1327, Validation Loss: 0.1706\n",
      "[Trial 169] Epoch 54/60, Training Loss: 0.1253, Validation Loss: 0.1253\n",
      "[Trial 170] Epoch 45/60, Training Loss: 0.1268, Validation Loss: 0.1271\n",
      "[Trial 172] Epoch 14/60, Training Loss: 0.1277, Validation Loss: 0.1647\n",
      "[Trial 173] Epoch 7/60, Training Loss: 0.1276, Validation Loss: 0.1902\n",
      "[Trial 174] Epoch 5/60, Training Loss: 0.1343, Validation Loss: 0.1853\n",
      "[Trial 169] Epoch 55/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 171] Epoch 23/60, Training Loss: 0.1284, Validation Loss: 0.1327\n",
      "[Trial 175] Epoch 3/60, Training Loss: 0.1298, Validation Loss: 0.1517\n",
      "[Trial 170] Epoch 46/60, Training Loss: 0.1274, Validation Loss: 0.1274\n",
      "[Trial 172] Epoch 15/60, Training Loss: 0.1274, Validation Loss: 0.1587\n",
      "[Trial 169] Epoch 56/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 170] Epoch 47/60, Training Loss: 0.1269, Validation Loss: 0.1264\n",
      "[Trial 173] Epoch 8/60, Training Loss: 0.1271, Validation Loss: 0.1859\n",
      "[Trial 175] Epoch 4/60, Training Loss: 0.1291, Validation Loss: 0.1341\n",
      "[Trial 171] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1313\n",
      "[Trial 174] Epoch 6/60, Training Loss: 0.1325, Validation Loss: 0.1789\n",
      "[Trial 172] Epoch 16/60, Training Loss: 0.1267, Validation Loss: 0.1537\n",
      "[Trial 169] Epoch 57/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 170] Epoch 48/60, Training Loss: 0.1275, Validation Loss: 0.1263\n",
      "[Trial 169] Epoch 58/60, Training Loss: 0.1254, Validation Loss: 0.1253\n",
      "[Trial 172] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1502\n",
      "[Trial 175] Epoch 5/60, Training Loss: 0.1282, Validation Loss: 0.1290\n",
      "[Trial 173] Epoch 9/60, Training Loss: 0.1273, Validation Loss: 0.1808\n",
      "[Trial 170] Epoch 49/60, Training Loss: 0.1275, Validation Loss: 0.1258\n",
      "[Trial 171] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1298\n",
      "[Trial 174] Epoch 7/60, Training Loss: 0.1314, Validation Loss: 0.1722\n",
      "[Trial 169] Epoch 59/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 172] Epoch 18/60, Training Loss: 0.1270, Validation Loss: 0.1468\n",
      "[Trial 170] Epoch 50/60, Training Loss: 0.1313, Validation Loss: 0.1271\n",
      "[Trial 175] Epoch 6/60, Training Loss: 0.1277, Validation Loss: 0.1277\n",
      "[Trial 173] Epoch 10/60, Training Loss: 0.1274, Validation Loss: 0.1780\n",
      "[Trial 171] Epoch 26/60, Training Loss: 0.1275, Validation Loss: 0.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:27:42,633] Trial 169 finished with value: 0.1253435437877973 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0052985416987720935, 'batch_size': 64, 'patience': 7}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 169] Epoch 60/60, Training Loss: 0.1253, Validation Loss: 0.1253\n",
      "[Trial 174] Epoch 8/60, Training Loss: 0.1308, Validation Loss: 0.1694\n",
      "[Trial 170] Epoch 51/60, Training Loss: 0.1273, Validation Loss: 0.1257\n",
      "[Trial 172] Epoch 19/60, Training Loss: 0.1272, Validation Loss: 0.1436\n",
      "[Trial 175] Epoch 7/60, Training Loss: 0.1273, Validation Loss: 0.1261\n",
      "[Trial 170] Epoch 52/60, Training Loss: 0.1268, Validation Loss: 0.1251\n",
      "[Trial 173] Epoch 11/60, Training Loss: 0.1278, Validation Loss: 0.1702\n",
      "[Trial 172] Epoch 20/60, Training Loss: 0.1271, Validation Loss: 0.1406\n",
      "[Trial 171] Epoch 27/60, Training Loss: 0.1261, Validation Loss: 0.1285\n",
      "[Trial 176] Epoch 1/60, Training Loss: 0.8650, Validation Loss: 0.2448\n",
      "[Trial 174] Epoch 9/60, Training Loss: 0.1299, Validation Loss: 0.1635\n",
      "[Trial 170] Epoch 53/60, Training Loss: 0.1265, Validation Loss: 0.1249\n",
      "[Trial 175] Epoch 8/60, Training Loss: 0.1273, Validation Loss: 0.1264\n",
      "[Trial 172] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1381\n",
      "[Trial 173] Epoch 12/60, Training Loss: 0.1270, Validation Loss: 0.1679\n",
      "[Trial 171] Epoch 28/60, Training Loss: 0.1261, Validation Loss: 0.1276\n",
      "[Trial 170] Epoch 54/60, Training Loss: 0.1267, Validation Loss: 0.1250\n",
      "[Trial 176] Epoch 2/60, Training Loss: 0.1536, Validation Loss: 0.2221\n",
      "[Trial 174] Epoch 10/60, Training Loss: 0.1296, Validation Loss: 0.1597\n",
      "[Trial 172] Epoch 22/60, Training Loss: 0.1269, Validation Loss: 0.1358\n",
      "[Trial 175] Epoch 9/60, Training Loss: 0.1271, Validation Loss: 0.1258\n",
      "[Trial 170] Epoch 55/60, Training Loss: 0.1276, Validation Loss: 0.1249\n",
      "[Trial 173] Epoch 13/60, Training Loss: 0.1278, Validation Loss: 0.1644\n",
      "[Trial 171] Epoch 29/60, Training Loss: 0.1267, Validation Loss: 0.1272\n",
      "[Trial 172] Epoch 23/60, Training Loss: 0.1268, Validation Loss: 0.1338\n",
      "[Trial 176] Epoch 3/60, Training Loss: 0.1419, Validation Loss: 0.2130\n",
      "[Trial 170] Epoch 56/60, Training Loss: 0.1274, Validation Loss: 0.1249\n",
      "[Trial 174] Epoch 11/60, Training Loss: 0.1292, Validation Loss: 0.1550\n",
      "[Trial 175] Epoch 10/60, Training Loss: 0.1271, Validation Loss: 0.1243\n",
      "[Trial 172] Epoch 24/60, Training Loss: 0.1268, Validation Loss: 0.1323\n",
      "[Trial 170] Epoch 57/60, Training Loss: 0.1281, Validation Loss: 0.1247\n",
      "[Trial 173] Epoch 14/60, Training Loss: 0.1266, Validation Loss: 0.1590\n",
      "[Trial 171] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1265\n",
      "[Trial 176] Epoch 4/60, Training Loss: 0.1369, Validation Loss: 0.2056\n",
      "[Trial 175] Epoch 11/60, Training Loss: 0.1270, Validation Loss: 0.1239\n",
      "[Trial 174] Epoch 12/60, Training Loss: 0.1288, Validation Loss: 0.1514\n",
      "[Trial 170] Epoch 58/60, Training Loss: 0.1266, Validation Loss: 0.1246\n",
      "[Trial 172] Epoch 25/60, Training Loss: 0.1269, Validation Loss: 0.1311\n",
      "[Trial 173] Epoch 15/60, Training Loss: 0.1261, Validation Loss: 0.1560\n",
      "[Trial 171] Epoch 31/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 170] Epoch 59/60, Training Loss: 0.1265, Validation Loss: 0.1244\n",
      "[Trial 175] Epoch 12/60, Training Loss: 0.1269, Validation Loss: 0.1244\n",
      "[Trial 172] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1298\n",
      "[Trial 176] Epoch 5/60, Training Loss: 0.1340, Validation Loss: 0.1976\n",
      "[Trial 174] Epoch 13/60, Training Loss: 0.1287, Validation Loss: 0.1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:28:56,541] Trial 170 finished with value: 0.12439388732115428 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0022565578654730636, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 170] Epoch 60/60, Training Loss: 0.1314, Validation Loss: 0.1255\n",
      "[Trial 173] Epoch 16/60, Training Loss: 0.1280, Validation Loss: 0.1527\n",
      "[Trial 171] Epoch 32/60, Training Loss: 0.1275, Validation Loss: 0.1264\n",
      "[Trial 172] Epoch 27/60, Training Loss: 0.1264, Validation Loss: 0.1292\n",
      "[Trial 175] Epoch 13/60, Training Loss: 0.1269, Validation Loss: 0.1235\n",
      "[Trial 176] Epoch 6/60, Training Loss: 0.1325, Validation Loss: 0.1927\n",
      "[Trial 174] Epoch 14/60, Training Loss: 0.1287, Validation Loss: 0.1455\n",
      "[Trial 172] Epoch 28/60, Training Loss: 0.1266, Validation Loss: 0.1285\n",
      "[Trial 177] Epoch 1/60, Training Loss: 556.0278, Validation Loss: 0.2116\n",
      "[Trial 173] Epoch 17/60, Training Loss: 0.1263, Validation Loss: 0.1492\n",
      "[Trial 171] Epoch 33/60, Training Loss: 0.1270, Validation Loss: 0.1257\n",
      "[Trial 175] Epoch 14/60, Training Loss: 0.1268, Validation Loss: 0.1234\n",
      "[Trial 176] Epoch 7/60, Training Loss: 0.1313, Validation Loss: 0.1855\n",
      "[Trial 172] Epoch 29/60, Training Loss: 0.1263, Validation Loss: 0.1280\n",
      "[Trial 174] Epoch 15/60, Training Loss: 0.1282, Validation Loss: 0.1419\n",
      "[Trial 177] Epoch 2/60, Training Loss: 0.1412, Validation Loss: 0.1982\n",
      "[Trial 173] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1485\n",
      "[Trial 175] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1231\n",
      "[Trial 171] Epoch 34/60, Training Loss: 0.1259, Validation Loss: 0.1256\n",
      "[Trial 172] Epoch 30/60, Training Loss: 0.1283, Validation Loss: 0.1277\n",
      "[Trial 176] Epoch 8/60, Training Loss: 0.1302, Validation Loss: 0.1811\n",
      "[Trial 174] Epoch 16/60, Training Loss: 0.1279, Validation Loss: 0.1401\n",
      "[Trial 175] Epoch 16/60, Training Loss: 0.1268, Validation Loss: 0.1239\n",
      "[Trial 172] Epoch 31/60, Training Loss: 0.1260, Validation Loss: 0.1274\n",
      "[Trial 173] Epoch 19/60, Training Loss: 0.1261, Validation Loss: 0.1456\n",
      "[Trial 171] Epoch 35/60, Training Loss: 0.1260, Validation Loss: 0.1255\n",
      "[Trial 177] Epoch 3/60, Training Loss: 0.1647, Validation Loss: 0.1896\n",
      "[Trial 176] Epoch 9/60, Training Loss: 0.1301, Validation Loss: 0.1761\n",
      "[Trial 172] Epoch 32/60, Training Loss: 0.1264, Validation Loss: 0.1273\n",
      "[Trial 174] Epoch 17/60, Training Loss: 0.1278, Validation Loss: 0.1377\n",
      "[Trial 175] Epoch 17/60, Training Loss: 0.1267, Validation Loss: 0.1233\n",
      "[Trial 173] Epoch 20/60, Training Loss: 0.1270, Validation Loss: 0.1421\n",
      "[Trial 171] Epoch 36/60, Training Loss: 0.1288, Validation Loss: 0.1273\n",
      "[Trial 177] Epoch 4/60, Training Loss: 0.1360, Validation Loss: 0.1763\n",
      "[Trial 172] Epoch 33/60, Training Loss: 0.1325, Validation Loss: 0.1272\n",
      "[Trial 176] Epoch 10/60, Training Loss: 0.1294, Validation Loss: 0.1716\n",
      "[Trial 175] Epoch 18/60, Training Loss: 0.1266, Validation Loss: 0.1242\n",
      "[Trial 174] Epoch 18/60, Training Loss: 0.1278, Validation Loss: 0.1363\n",
      "[Trial 173] Epoch 21/60, Training Loss: 0.1255, Validation Loss: 0.1401\n",
      "[Trial 171] Epoch 37/60, Training Loss: 0.1268, Validation Loss: 0.1253\n",
      "[Trial 172] Epoch 34/60, Training Loss: 0.1264, Validation Loss: 0.1271\n",
      "[Trial 177] Epoch 5/60, Training Loss: 0.1334, Validation Loss: 0.1672\n",
      "[Trial 175] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1235\n",
      "[Trial 176] Epoch 11/60, Training Loss: 0.1290, Validation Loss: 0.1661\n",
      "[Trial 172] Epoch 35/60, Training Loss: 0.1263, Validation Loss: 0.1269\n",
      "[Trial 173] Epoch 22/60, Training Loss: 0.1260, Validation Loss: 0.1382\n",
      "[Trial 174] Epoch 19/60, Training Loss: 0.1276, Validation Loss: 0.1341\n",
      "[Trial 171] Epoch 38/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 177] Epoch 6/60, Training Loss: 0.1328, Validation Loss: 0.1576\n",
      "[Trial 175] Epoch 20/60, Training Loss: 0.1265, Validation Loss: 0.1232\n",
      "[Trial 172] Epoch 36/60, Training Loss: 0.1259, Validation Loss: 0.1269\n",
      "[Trial 176] Epoch 12/60, Training Loss: 0.1285, Validation Loss: 0.1644\n",
      "[Trial 173] Epoch 23/60, Training Loss: 0.1257, Validation Loss: 0.1374\n",
      "[Trial 171] Epoch 39/60, Training Loss: 0.1263, Validation Loss: 0.1253\n",
      "[Trial 174] Epoch 20/60, Training Loss: 0.1273, Validation Loss: 0.1325\n",
      "[Trial 172] Epoch 37/60, Training Loss: 0.1265, Validation Loss: 0.1269\n",
      "[Trial 177] Epoch 7/60, Training Loss: 0.1295, Validation Loss: 0.1508\n",
      "[Trial 175] Epoch 21/60, Training Loss: 0.1263, Validation Loss: 0.1230\n",
      "[Trial 176] Epoch 13/60, Training Loss: 0.1283, Validation Loss: 0.1594\n",
      "[Trial 173] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1405\n",
      "[Trial 171] Epoch 40/60, Training Loss: 0.1272, Validation Loss: 0.1366\n",
      "[Trial 172] Epoch 38/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 174] Epoch 21/60, Training Loss: 0.1277, Validation Loss: 0.1311\n",
      "[Trial 175] Epoch 22/60, Training Loss: 0.1262, Validation Loss: 0.1232\n",
      "[Trial 177] Epoch 8/60, Training Loss: 0.1452, Validation Loss: 0.1623\n",
      "[Trial 172] Epoch 39/60, Training Loss: 0.1318, Validation Loss: 0.1269\n",
      "[Trial 176] Epoch 14/60, Training Loss: 0.1281, Validation Loss: 0.1556\n",
      "[Trial 173] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1342\n",
      "[Trial 171] Epoch 41/60, Training Loss: 0.1442, Validation Loss: 0.1251\n",
      "[Trial 174] Epoch 22/60, Training Loss: 0.1274, Validation Loss: 0.1307\n",
      "[Trial 175] Epoch 23/60, Training Loss: 0.1262, Validation Loss: 0.1232\n",
      "[Trial 172] Epoch 40/60, Training Loss: 0.1259, Validation Loss: 0.1269\n",
      "[Trial 177] Epoch 9/60, Training Loss: 0.1479, Validation Loss: 0.1401\n",
      "[Trial 176] Epoch 15/60, Training Loss: 0.1278, Validation Loss: 0.1533\n",
      "[Trial 173] Epoch 26/60, Training Loss: 0.1261, Validation Loss: 0.1333\n",
      "[Trial 171] Epoch 42/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 172] Epoch 41/60, Training Loss: 0.1260, Validation Loss: 0.1269\n",
      "[Trial 175] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1231\n",
      "[Trial 174] Epoch 23/60, Training Loss: 0.1272, Validation Loss: 0.1309\n",
      "[Trial 177] Epoch 10/60, Training Loss: 0.1283, Validation Loss: 0.1363\n",
      "[Trial 172] Epoch 42/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 173] Epoch 27/60, Training Loss: 0.1254, Validation Loss: 0.1374\n",
      "[Trial 176] Epoch 16/60, Training Loss: 0.1274, Validation Loss: 0.1501\n",
      "[Trial 171] Epoch 43/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 175] Epoch 25/60, Training Loss: 0.1261, Validation Loss: 0.1228\n",
      "[Trial 174] Epoch 24/60, Training Loss: 0.1274, Validation Loss: 0.1287\n",
      "[Trial 172] Epoch 43/60, Training Loss: 0.1260, Validation Loss: 0.1269\n",
      "[Trial 177] Epoch 11/60, Training Loss: 0.1597, Validation Loss: 0.2525\n",
      "[Trial 173] Epoch 28/60, Training Loss: 0.1327, Validation Loss: 0.1317\n",
      "[Trial 175] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 171] Epoch 44/60, Training Loss: 0.1260, Validation Loss: 0.1251\n",
      "[Trial 176] Epoch 17/60, Training Loss: 0.1273, Validation Loss: 0.1482\n",
      "[Trial 172] Epoch 44/60, Training Loss: 0.1285, Validation Loss: 0.1271\n",
      "[Trial 174] Epoch 25/60, Training Loss: 0.1274, Validation Loss: 0.1282\n",
      "[Trial 175] Epoch 27/60, Training Loss: 0.1262, Validation Loss: 0.1229\n",
      "[Trial 173] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1310\n",
      "[Trial 172] Epoch 45/60, Training Loss: 0.1259, Validation Loss: 0.1269\n",
      "[Trial 177] Epoch 12/60, Training Loss: 0.1733, Validation Loss: 0.1310\n",
      "[Trial 171] Epoch 45/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 176] Epoch 18/60, Training Loss: 0.1272, Validation Loss: 0.1444\n",
      "[Trial 174] Epoch 26/60, Training Loss: 0.1274, Validation Loss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:31:57,698] Trial 172 finished with value: 0.12685390710830688 and parameters: {'hidden_dim': 256, 'latent_dim': 128, 'learning_rate': 0.005127677718214438, 'batch_size': 64, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 172] Epoch 46/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 172] Early stopping after 46 epochs.\n",
      "[Trial 175] Epoch 28/60, Training Loss: 0.1264, Validation Loss: 0.1232\n",
      "[Trial 173] Epoch 30/60, Training Loss: 0.1253, Validation Loss: 0.1308\n",
      "[Trial 171] Epoch 46/60, Training Loss: 0.1260, Validation Loss: 0.1250\n",
      "[Trial 176] Epoch 19/60, Training Loss: 0.1270, Validation Loss: 0.1417\n",
      "[Trial 177] Epoch 13/60, Training Loss: 0.1274, Validation Loss: 0.1293\n",
      "[Trial 174] Epoch 27/60, Training Loss: 0.1269, Validation Loss: 0.1269\n",
      "[Trial 175] Epoch 29/60, Training Loss: 0.1264, Validation Loss: 0.1228\n",
      "[Trial 178] Epoch 1/60, Training Loss: 460.5083, Validation Loss: 0.2057\n",
      "[Trial 173] Epoch 31/60, Training Loss: 0.1253, Validation Loss: 0.1305\n",
      "[Trial 171] Epoch 47/60, Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "[Trial 176] Epoch 20/60, Training Loss: 0.1270, Validation Loss: 0.1400\n",
      "[Trial 177] Epoch 14/60, Training Loss: 0.1275, Validation Loss: 0.1279\n",
      "[Trial 175] Epoch 30/60, Training Loss: 0.1262, Validation Loss: 0.1226\n",
      "[Trial 174] Epoch 28/60, Training Loss: 0.1276, Validation Loss: 0.1274\n",
      "[Trial 173] Epoch 32/60, Training Loss: 0.1252, Validation Loss: 0.1301\n",
      "[Trial 171] Epoch 48/60, Training Loss: 0.1298, Validation Loss: 0.1289\n",
      "[Trial 178] Epoch 2/60, Training Loss: 0.1453, Validation Loss: 0.1879\n",
      "[Trial 176] Epoch 21/60, Training Loss: 0.1270, Validation Loss: 0.1377\n",
      "[Trial 177] Epoch 15/60, Training Loss: 0.1266, Validation Loss: 0.1271\n",
      "[Trial 175] Epoch 31/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 174] Epoch 29/60, Training Loss: 0.1272, Validation Loss: 0.1260\n",
      "[Trial 173] Epoch 33/60, Training Loss: 0.1284, Validation Loss: 0.1299\n",
      "[Trial 171] Epoch 49/60, Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "[Trial 178] Epoch 3/60, Training Loss: 0.1403, Validation Loss: 0.1778\n",
      "[Trial 176] Epoch 22/60, Training Loss: 0.1271, Validation Loss: 0.1370\n",
      "[Trial 175] Epoch 32/60, Training Loss: 0.1263, Validation Loss: 0.1229\n",
      "[Trial 177] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1267\n",
      "[Trial 174] Epoch 30/60, Training Loss: 0.1271, Validation Loss: 0.1255\n",
      "[Trial 173] Epoch 34/60, Training Loss: 0.1251, Validation Loss: 0.1297\n",
      "[Trial 171] Epoch 50/60, Training Loss: 0.1258, Validation Loss: 0.1250\n",
      "[Trial 175] Epoch 33/60, Training Loss: 0.1262, Validation Loss: 0.1226\n",
      "[Trial 178] Epoch 4/60, Training Loss: 0.1373, Validation Loss: 0.1655\n",
      "[Trial 176] Epoch 23/60, Training Loss: 0.1268, Validation Loss: 0.1352\n",
      "[Trial 177] Epoch 17/60, Training Loss: 0.1538, Validation Loss: 0.1267\n",
      "[Trial 174] Epoch 31/60, Training Loss: 0.1267, Validation Loss: 0.1255\n",
      "[Trial 173] Epoch 35/60, Training Loss: 0.1251, Validation Loss: 0.1292\n",
      "[Trial 171] Epoch 51/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 175] Epoch 34/60, Training Loss: 0.1263, Validation Loss: 0.1227\n",
      "[Trial 176] Epoch 24/60, Training Loss: 0.1269, Validation Loss: 0.1339\n",
      "[Trial 178] Epoch 5/60, Training Loss: 0.1347, Validation Loss: 0.1560\n",
      "[Trial 177] Epoch 18/60, Training Loss: 0.1265, Validation Loss: 0.1265\n",
      "[Trial 173] Epoch 36/60, Training Loss: 0.1250, Validation Loss: 0.1291\n",
      "[Trial 174] Epoch 32/60, Training Loss: 0.1270, Validation Loss: 0.1255\n",
      "[Trial 175] Epoch 35/60, Training Loss: 0.1262, Validation Loss: 0.1229\n",
      "[Trial 171] Epoch 52/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 176] Epoch 25/60, Training Loss: 0.1271, Validation Loss: 0.1336\n",
      "[Trial 178] Epoch 6/60, Training Loss: 0.1329, Validation Loss: 0.1489\n",
      "[Trial 177] Epoch 19/60, Training Loss: 0.1311, Validation Loss: 0.1265\n",
      "[Trial 173] Epoch 37/60, Training Loss: 0.1257, Validation Loss: 0.1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:33:29,133] Trial 175 finished with value: 0.12263379146655401 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.002276503971114414, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 175] Epoch 36/60, Training Loss: 0.1261, Validation Loss: 0.1227\n",
      "[Trial 175] Early stopping after 36 epochs.\n",
      "[Trial 171] Epoch 53/60, Training Loss: 0.1257, Validation Loss: 0.1250\n",
      "[Trial 174] Epoch 33/60, Training Loss: 0.1271, Validation Loss: 0.1257\n",
      "[Trial 176] Epoch 26/60, Training Loss: 0.1264, Validation Loss: 0.1321\n",
      "[Trial 178] Epoch 7/60, Training Loss: 0.1363, Validation Loss: 0.1423\n",
      "[Trial 179] Epoch 1/60, Training Loss: 0.4997, Validation Loss: 0.2103\n",
      "[Trial 173] Epoch 38/60, Training Loss: 0.1326, Validation Loss: 0.1291\n",
      "[Trial 177] Epoch 20/60, Training Loss: 0.3563, Validation Loss: 0.1265\n",
      "[Trial 171] Epoch 54/60, Training Loss: 0.1257, Validation Loss: 0.1250\n",
      "[Trial 174] Epoch 34/60, Training Loss: 0.1273, Validation Loss: 0.1249\n",
      "[Trial 176] Epoch 27/60, Training Loss: 0.1268, Validation Loss: 0.1317\n",
      "[Trial 179] Epoch 2/60, Training Loss: 0.1304, Validation Loss: 0.1780\n",
      "[Trial 173] Epoch 39/60, Training Loss: 0.1251, Validation Loss: 0.1292\n",
      "[Trial 178] Epoch 8/60, Training Loss: 0.1293, Validation Loss: 0.1385\n",
      "[Trial 171] Epoch 55/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 177] Epoch 21/60, Training Loss: 0.1261, Validation Loss: 0.1264\n",
      "[Trial 174] Epoch 35/60, Training Loss: 0.1266, Validation Loss: 0.1250\n",
      "[Trial 176] Epoch 28/60, Training Loss: 0.1264, Validation Loss: 0.1306\n",
      "[Trial 179] Epoch 3/60, Training Loss: 0.1284, Validation Loss: 0.1575\n",
      "[Trial 173] Epoch 40/60, Training Loss: 0.1261, Validation Loss: 0.1290\n",
      "[Trial 171] Epoch 56/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 178] Epoch 9/60, Training Loss: 0.1292, Validation Loss: 0.1338\n",
      "[Trial 177] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1266\n",
      "[Trial 174] Epoch 36/60, Training Loss: 0.1267, Validation Loss: 0.1244\n",
      "[Trial 179] Epoch 4/60, Training Loss: 0.1275, Validation Loss: 0.1508\n",
      "[Trial 176] Epoch 29/60, Training Loss: 0.1267, Validation Loss: 0.1327\n",
      "[Trial 173] Epoch 41/60, Training Loss: 0.1246, Validation Loss: 0.1290\n",
      "[Trial 171] Epoch 57/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 178] Epoch 10/60, Training Loss: 0.1279, Validation Loss: 0.1309\n",
      "[Trial 177] Epoch 23/60, Training Loss: 0.1262, Validation Loss: 0.1264\n",
      "[Trial 174] Epoch 37/60, Training Loss: 0.1268, Validation Loss: 0.1247\n",
      "[Trial 179] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1403\n",
      "[Trial 176] Epoch 30/60, Training Loss: 0.1268, Validation Loss: 0.1298\n",
      "[Trial 173] Epoch 42/60, Training Loss: 0.1248, Validation Loss: 0.1290\n",
      "[Trial 171] Epoch 58/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 174] Epoch 38/60, Training Loss: 0.1269, Validation Loss: 0.1252\n",
      "[Trial 178] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.1291\n",
      "[Trial 177] Epoch 24/60, Training Loss: 0.1263, Validation Loss: 0.1263\n",
      "[Trial 179] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1355\n",
      "[Trial 176] Epoch 31/60, Training Loss: 0.1261, Validation Loss: 0.1288\n",
      "[Trial 173] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1296\n",
      "[Trial 171] Epoch 59/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 179] Epoch 7/60, Training Loss: 0.1262, Validation Loss: 0.1333\n",
      "[Trial 174] Epoch 39/60, Training Loss: 0.1272, Validation Loss: 0.1244\n",
      "[Trial 177] Epoch 25/60, Training Loss: 0.1263, Validation Loss: 0.1264\n",
      "[Trial 178] Epoch 12/60, Training Loss: 0.1278, Validation Loss: 0.1286\n",
      "[Trial 176] Epoch 32/60, Training Loss: 0.1262, Validation Loss: 0.1289\n",
      "[Trial 173] Epoch 44/60, Training Loss: 0.1259, Validation Loss: 0.1297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:35:00,066] Trial 171 finished with value: 0.12497564827402433 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.002327923971018771, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 171] Epoch 60/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 179] Epoch 8/60, Training Loss: 0.1259, Validation Loss: 0.1322\n",
      "[Trial 174] Epoch 40/60, Training Loss: 0.1269, Validation Loss: 0.1245\n",
      "[Trial 177] Epoch 26/60, Training Loss: 0.1270, Validation Loss: 0.1265\n",
      "[Trial 178] Epoch 13/60, Training Loss: 0.1387, Validation Loss: 0.1268\n",
      "[Trial 176] Epoch 33/60, Training Loss: 0.1263, Validation Loss: 0.1281\n",
      "[Trial 173] Epoch 45/60, Training Loss: 0.1251, Validation Loss: 0.1289\n",
      "[Trial 180] Epoch 1/60, Training Loss: 0.4770, Validation Loss: 0.1983\n",
      "[Trial 179] Epoch 9/60, Training Loss: 0.1257, Validation Loss: 0.1303\n",
      "[Trial 174] Epoch 41/60, Training Loss: 0.1268, Validation Loss: 0.1244\n",
      "[Trial 177] Epoch 27/60, Training Loss: 0.1267, Validation Loss: 0.1264\n",
      "[Trial 173] Epoch 46/60, Training Loss: 0.1252, Validation Loss: 0.1324\n",
      "[Trial 178] Epoch 14/60, Training Loss: 0.1283, Validation Loss: 0.1266\n",
      "[Trial 176] Epoch 34/60, Training Loss: 0.1266, Validation Loss: 0.1279\n",
      "[Trial 180] Epoch 2/60, Training Loss: 0.1306, Validation Loss: 0.1671\n",
      "[Trial 179] Epoch 10/60, Training Loss: 0.1259, Validation Loss: 0.1310\n",
      "[Trial 174] Epoch 42/60, Training Loss: 0.1267, Validation Loss: 0.1242\n",
      "[Trial 177] Epoch 28/60, Training Loss: 0.1268, Validation Loss: 0.1264\n",
      "[Trial 173] Epoch 47/60, Training Loss: 0.1296, Validation Loss: 0.1289\n",
      "[Trial 180] Epoch 3/60, Training Loss: 0.1282, Validation Loss: 0.1470\n",
      "[Trial 176] Epoch 35/60, Training Loss: 0.1263, Validation Loss: 0.1271\n",
      "[Trial 178] Epoch 15/60, Training Loss: 0.1474, Validation Loss: 0.1259\n",
      "[Trial 179] Epoch 11/60, Training Loss: 0.1259, Validation Loss: 0.1305\n",
      "[Trial 174] Epoch 43/60, Training Loss: 0.1268, Validation Loss: 0.1246\n",
      "[Trial 173] Epoch 48/60, Training Loss: 0.1249, Validation Loss: 0.1290\n",
      "[Trial 180] Epoch 4/60, Training Loss: 0.1277, Validation Loss: 0.1377\n",
      "[Trial 177] Epoch 29/60, Training Loss: 0.1268, Validation Loss: 0.1279\n",
      "[Trial 176] Epoch 36/60, Training Loss: 0.1263, Validation Loss: 0.1273\n",
      "[Trial 178] Epoch 16/60, Training Loss: 0.1293, Validation Loss: 0.1573\n",
      "[Trial 179] Epoch 12/60, Training Loss: 0.1255, Validation Loss: 0.1286\n",
      "[Trial 173] Epoch 49/60, Training Loss: 0.1248, Validation Loss: 0.1290\n",
      "[Trial 174] Epoch 44/60, Training Loss: 0.1270, Validation Loss: 0.1245\n",
      "[Trial 180] Epoch 5/60, Training Loss: 0.1274, Validation Loss: 0.1306\n",
      "[Trial 176] Epoch 37/60, Training Loss: 0.1263, Validation Loss: 0.1276\n",
      "[Trial 177] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1263\n",
      "[Trial 179] Epoch 13/60, Training Loss: 0.1255, Validation Loss: 0.1300\n",
      "[Trial 178] Epoch 17/60, Training Loss: 0.1848, Validation Loss: 0.1258\n",
      "[Trial 173] Epoch 50/60, Training Loss: 0.1248, Validation Loss: 0.1289\n",
      "[Trial 180] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1296\n",
      "[Trial 174] Epoch 45/60, Training Loss: 0.1265, Validation Loss: 0.1241\n",
      "[Trial 176] Epoch 38/60, Training Loss: 0.1263, Validation Loss: 0.1270\n",
      "[Trial 179] Epoch 14/60, Training Loss: 0.1259, Validation Loss: 0.1280\n",
      "[Trial 177] Epoch 31/60, Training Loss: 0.1256, Validation Loss: 0.1264\n",
      "[Trial 178] Epoch 18/60, Training Loss: 0.1265, Validation Loss: 0.1258\n",
      "[Trial 173] Epoch 51/60, Training Loss: 0.1247, Validation Loss: 0.1288\n",
      "[Trial 180] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1272\n",
      "[Trial 174] Epoch 46/60, Training Loss: 0.1268, Validation Loss: 0.1242\n",
      "[Trial 179] Epoch 15/60, Training Loss: 0.1255, Validation Loss: 0.1285\n",
      "[Trial 176] Epoch 39/60, Training Loss: 0.1262, Validation Loss: 0.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:36:34,852] Trial 177 finished with value: 0.1263091596464316 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.011693262055886097, 'batch_size': 32, 'patience': 8}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 177] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1264\n",
      "[Trial 177] Early stopping after 32 epochs.\n",
      "[Trial 178] Epoch 19/60, Training Loss: 0.1264, Validation Loss: 0.1258\n",
      "[Trial 173] Epoch 52/60, Training Loss: 0.1246, Validation Loss: 0.1289\n",
      "[Trial 180] Epoch 8/60, Training Loss: 0.1264, Validation Loss: 0.1267\n",
      "[Trial 179] Epoch 16/60, Training Loss: 0.1256, Validation Loss: 0.1277\n",
      "[Trial 174] Epoch 47/60, Training Loss: 0.1269, Validation Loss: 0.1243\n",
      "[Trial 176] Epoch 40/60, Training Loss: 0.1263, Validation Loss: 0.1279\n",
      "[Trial 181] Epoch 1/60, Training Loss: 0.7251, Validation Loss: 0.1886\n",
      "[Trial 178] Epoch 20/60, Training Loss: 0.1267, Validation Loss: 0.1259\n",
      "[Trial 173] Epoch 53/60, Training Loss: 0.1248, Validation Loss: 0.1288\n",
      "[Trial 180] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1285\n",
      "[Trial 179] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1282\n",
      "[Trial 174] Epoch 48/60, Training Loss: 0.1264, Validation Loss: 0.1246\n",
      "[Trial 181] Epoch 2/60, Training Loss: 0.1299, Validation Loss: 0.1615\n",
      "[Trial 176] Epoch 41/60, Training Loss: 0.1263, Validation Loss: 0.1276\n",
      "[Trial 173] Epoch 54/60, Training Loss: 0.1246, Validation Loss: 0.1288\n",
      "[Trial 180] Epoch 10/60, Training Loss: 0.1267, Validation Loss: 0.1271\n",
      "[Trial 179] Epoch 18/60, Training Loss: 0.1253, Validation Loss: 0.1281\n",
      "[Trial 178] Epoch 21/60, Training Loss: 0.1356, Validation Loss: 0.1405\n",
      "[Trial 181] Epoch 3/60, Training Loss: 0.1280, Validation Loss: 0.1454\n",
      "[Trial 176] Epoch 42/60, Training Loss: 0.1263, Validation Loss: 0.1268\n",
      "[Trial 174] Epoch 49/60, Training Loss: 0.1264, Validation Loss: 0.1242\n",
      "[Trial 173] Epoch 55/60, Training Loss: 0.1248, Validation Loss: 0.1288\n",
      "[Trial 179] Epoch 19/60, Training Loss: 0.1256, Validation Loss: 0.1277\n",
      "[Trial 180] Epoch 11/60, Training Loss: 0.1267, Validation Loss: 0.1260\n",
      "[Trial 178] Epoch 22/60, Training Loss: 0.1710, Validation Loss: 0.1259\n",
      "[Trial 181] Epoch 4/60, Training Loss: 0.1270, Validation Loss: 0.1382\n",
      "[Trial 176] Epoch 43/60, Training Loss: 0.1266, Validation Loss: 0.1269\n",
      "[Trial 174] Epoch 50/60, Training Loss: 0.1264, Validation Loss: 0.1245\n",
      "[Trial 173] Epoch 56/60, Training Loss: 0.1248, Validation Loss: 0.1288\n",
      "[Trial 179] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1284\n",
      "[Trial 180] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1259\n",
      "[Trial 181] Epoch 5/60, Training Loss: 0.1267, Validation Loss: 0.1326\n",
      "[Trial 178] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1258\n",
      "[Trial 176] Epoch 44/60, Training Loss: 0.1259, Validation Loss: 0.1264\n",
      "[Trial 174] Epoch 51/60, Training Loss: 0.1263, Validation Loss: 0.1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:37:44,399] Trial 173 finished with value: 0.12878565043210982 and parameters: {'hidden_dim': 128, 'latent_dim': 128, 'learning_rate': 0.0022573092501158303, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 173] Epoch 57/60, Training Loss: 0.1247, Validation Loss: 0.1289\n",
      "[Trial 173] Early stopping after 57 epochs.\n",
      "[Trial 179] Epoch 21/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 180] Epoch 13/60, Training Loss: 0.1263, Validation Loss: 0.1263\n",
      "[Trial 181] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1297\n",
      "[Trial 178] Epoch 24/60, Training Loss: 0.1266, Validation Loss: 0.1259\n",
      "[Trial 176] Epoch 45/60, Training Loss: 0.1261, Validation Loss: 0.1267\n",
      "[Trial 174] Epoch 52/60, Training Loss: 0.1264, Validation Loss: 0.1241\n",
      "[Trial 182] Epoch 1/60, Training Loss: 0.6611, Validation Loss: 0.1794\n",
      "[Trial 179] Epoch 22/60, Training Loss: 0.1252, Validation Loss: 0.1280\n",
      "[Trial 180] Epoch 14/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 181] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1307\n",
      "[Trial 176] Epoch 46/60, Training Loss: 0.1264, Validation Loss: 0.1271\n",
      "[Trial 178] Epoch 25/60, Training Loss: 0.1268, Validation Loss: 0.1260\n",
      "[Trial 174] Epoch 53/60, Training Loss: 0.1263, Validation Loss: 0.1242\n",
      "[Trial 182] Epoch 2/60, Training Loss: 0.1301, Validation Loss: 0.1565\n",
      "[Trial 179] Epoch 23/60, Training Loss: 0.1251, Validation Loss: 0.1276\n",
      "[Trial 180] Epoch 15/60, Training Loss: 0.1261, Validation Loss: 0.1269\n",
      "[Trial 181] Epoch 8/60, Training Loss: 0.1267, Validation Loss: 0.1284\n",
      "[Trial 176] Epoch 47/60, Training Loss: 0.1262, Validation Loss: 0.1263\n",
      "[Trial 182] Epoch 3/60, Training Loss: 0.1289, Validation Loss: 0.1410\n",
      "[Trial 179] Epoch 24/60, Training Loss: 0.1251, Validation Loss: 0.1277\n",
      "[Trial 178] Epoch 26/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 174] Epoch 54/60, Training Loss: 0.1263, Validation Loss: 0.1238\n",
      "[Trial 180] Epoch 16/60, Training Loss: 0.1265, Validation Loss: 0.1257\n",
      "[Trial 181] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1286\n",
      "[Trial 176] Epoch 48/60, Training Loss: 0.1262, Validation Loss: 0.1264\n",
      "[Trial 182] Epoch 4/60, Training Loss: 0.1284, Validation Loss: 0.1339\n",
      "[Trial 179] Epoch 25/60, Training Loss: 0.1250, Validation Loss: 0.1274\n",
      "[Trial 174] Epoch 55/60, Training Loss: 0.1263, Validation Loss: 0.1241\n",
      "[Trial 180] Epoch 17/60, Training Loss: 0.1259, Validation Loss: 0.1254\n",
      "[Trial 178] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 181] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1276\n",
      "[Trial 182] Epoch 5/60, Training Loss: 0.1278, Validation Loss: 0.1285\n",
      "[Trial 179] Epoch 26/60, Training Loss: 0.1249, Validation Loss: 0.1275\n",
      "[Trial 176] Epoch 49/60, Training Loss: 0.1266, Validation Loss: 0.1270\n",
      "[Trial 180] Epoch 18/60, Training Loss: 0.1262, Validation Loss: 0.1263\n",
      "[Trial 174] Epoch 56/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 181] Epoch 11/60, Training Loss: 0.1260, Validation Loss: 0.1277\n",
      "[Trial 178] Epoch 28/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 179] Epoch 27/60, Training Loss: 0.1250, Validation Loss: 0.1277\n",
      "[Trial 182] Epoch 6/60, Training Loss: 0.1275, Validation Loss: 0.1280\n",
      "[Trial 176] Epoch 50/60, Training Loss: 0.1260, Validation Loss: 0.1264\n",
      "[Trial 180] Epoch 19/60, Training Loss: 0.1263, Validation Loss: 0.1254\n",
      "[Trial 181] Epoch 12/60, Training Loss: 0.1258, Validation Loss: 0.1273\n",
      "[Trial 174] Epoch 57/60, Training Loss: 0.1263, Validation Loss: 0.1241\n",
      "[Trial 178] Epoch 29/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 179] Epoch 28/60, Training Loss: 0.1252, Validation Loss: 0.1278\n",
      "[Trial 182] Epoch 7/60, Training Loss: 0.1272, Validation Loss: 0.1259\n",
      "[Trial 176] Epoch 51/60, Training Loss: 0.1258, Validation Loss: 0.1266\n",
      "[Trial 181] Epoch 13/60, Training Loss: 0.1257, Validation Loss: 0.1272\n",
      "[Trial 180] Epoch 20/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 174] Epoch 58/60, Training Loss: 0.1262, Validation Loss: 0.1241\n",
      "[Trial 178] Epoch 30/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 179] Epoch 29/60, Training Loss: 0.1252, Validation Loss: 0.1275\n",
      "[Trial 182] Epoch 8/60, Training Loss: 0.1273, Validation Loss: 0.1244\n",
      "[Trial 180] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 176] Epoch 52/60, Training Loss: 0.1257, Validation Loss: 0.1264\n",
      "[Trial 181] Epoch 14/60, Training Loss: 0.1259, Validation Loss: 0.1272\n",
      "[Trial 174] Epoch 59/60, Training Loss: 0.1262, Validation Loss: 0.1240\n",
      "[Trial 179] Epoch 30/60, Training Loss: 0.1249, Validation Loss: 0.1275\n",
      "[Trial 178] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 182] Epoch 9/60, Training Loss: 0.1273, Validation Loss: 0.1254\n",
      "[Trial 180] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 181] Epoch 15/60, Training Loss: 0.1257, Validation Loss: 0.1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:39:41,617] Trial 176 finished with value: 0.12630701089898747 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.00032775749043071327, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 176] Epoch 53/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 176] Early stopping after 53 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:39:46,309] Trial 174 finished with value: 0.12384677653511365 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'learning_rate': 0.00032939599218846644, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 174] Epoch 60/60, Training Loss: 0.1262, Validation Loss: 0.1239\n",
      "[Trial 174] Early stopping after 60 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:39:48,205] Trial 179 finished with value: 0.12744230752189953 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0023178311306044207, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 179] Epoch 31/60, Training Loss: 0.1250, Validation Loss: 0.1277\n",
      "[Trial 179] Early stopping after 31 epochs.\n",
      "[Trial 182] Epoch 10/60, Training Loss: 0.1273, Validation Loss: 0.1238\n",
      "[Trial 178] Epoch 32/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 180] Epoch 23/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 181] Epoch 16/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 183] Epoch 1/60, Training Loss: 100.2385, Validation Loss: 0.1740\n",
      "[Trial 184] Epoch 1/60, Training Loss: 0.5283, Validation Loss: 0.1896\n",
      "[Trial 182] Epoch 11/60, Training Loss: 0.1271, Validation Loss: 0.1234\n",
      "[Trial 181] Epoch 17/60, Training Loss: 0.1258, Validation Loss: 0.1297\n",
      "[Trial 180] Epoch 24/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 178] Epoch 33/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 183] Epoch 2/60, Training Loss: 0.1327, Validation Loss: 0.1511\n",
      "[Trial 185] Epoch 1/60, Training Loss: 50.7686, Validation Loss: 0.1689\n",
      "[Trial 184] Epoch 2/60, Training Loss: 0.1305, Validation Loss: 0.1636\n",
      "[Trial 182] Epoch 12/60, Training Loss: 0.1268, Validation Loss: 0.1231\n",
      "[Trial 181] Epoch 18/60, Training Loss: 0.1267, Validation Loss: 0.1270\n",
      "[Trial 180] Epoch 25/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 183] Epoch 3/60, Training Loss: 0.1307, Validation Loss: 0.1420\n",
      "[Trial 178] Epoch 34/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 184] Epoch 3/60, Training Loss: 0.1285, Validation Loss: 0.1486\n",
      "[Trial 185] Epoch 2/60, Training Loss: 0.1343, Validation Loss: 0.1475\n",
      "[Trial 182] Epoch 13/60, Training Loss: 0.1269, Validation Loss: 0.1232\n",
      "[Trial 181] Epoch 19/60, Training Loss: 0.1257, Validation Loss: 0.1268\n",
      "[Trial 183] Epoch 4/60, Training Loss: 0.1293, Validation Loss: 0.1366\n",
      "[Trial 180] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 178] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1258\n",
      "[Trial 184] Epoch 4/60, Training Loss: 0.1278, Validation Loss: 0.1415\n",
      "[Trial 182] Epoch 14/60, Training Loss: 0.1266, Validation Loss: 0.1228\n",
      "[Trial 181] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 183] Epoch 5/60, Training Loss: 0.1284, Validation Loss: 0.1325\n",
      "[Trial 180] Epoch 27/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 185] Epoch 3/60, Training Loss: 0.1299, Validation Loss: 0.1346\n",
      "[Trial 184] Epoch 5/60, Training Loss: 0.1274, Validation Loss: 0.1358\n",
      "[Trial 178] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 182] Epoch 15/60, Training Loss: 0.1268, Validation Loss: 0.1238\n",
      "[Trial 181] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1268\n",
      "[Trial 183] Epoch 6/60, Training Loss: 0.1272, Validation Loss: 0.1305\n",
      "[Trial 180] Epoch 28/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 184] Epoch 6/60, Training Loss: 0.1270, Validation Loss: 0.1322\n",
      "[Trial 178] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1258\n",
      "[Trial 182] Epoch 16/60, Training Loss: 0.1272, Validation Loss: 0.1232\n",
      "[Trial 181] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1267\n",
      "[Trial 183] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1287\n",
      "[Trial 185] Epoch 4/60, Training Loss: 0.1562, Validation Loss: 0.1263\n",
      "[Trial 180] Epoch 29/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 184] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1309\n",
      "[Trial 178] Epoch 38/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 182] Epoch 17/60, Training Loss: 0.1270, Validation Loss: 0.1238\n",
      "[Trial 181] Epoch 23/60, Training Loss: 0.1257, Validation Loss: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:41:21,695] Trial 180 finished with value: 0.12525731672843296 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0028445247075159497, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 183] Epoch 8/60, Training Loss: 0.1269, Validation Loss: 0.1277\n",
      "[Trial 180] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 180] Early stopping after 30 epochs.\n",
      "[Trial 184] Epoch 8/60, Training Loss: 0.1264, Validation Loss: 0.1292\n",
      "[Trial 185] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1239\n",
      "[Trial 182] Epoch 18/60, Training Loss: 0.1265, Validation Loss: 0.1225\n",
      "[Trial 181] Epoch 24/60, Training Loss: 0.1255, Validation Loss: 0.1269\n",
      "[Trial 178] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 183] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1268\n",
      "[Trial 186] Epoch 1/60, Training Loss: 0.3128, Validation Loss: 0.1637\n",
      "[Trial 184] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1277\n",
      "[Trial 182] Epoch 19/60, Training Loss: 0.1265, Validation Loss: 0.1227\n",
      "[Trial 181] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1268\n",
      "[Trial 183] Epoch 10/60, Training Loss: 0.1283, Validation Loss: 0.1262\n",
      "[Trial 178] Epoch 40/60, Training Loss: 0.1257, Validation Loss: 0.1258\n",
      "[Trial 185] Epoch 6/60, Training Loss: 0.1268, Validation Loss: 0.1241\n",
      "[Trial 184] Epoch 10/60, Training Loss: 0.1266, Validation Loss: 0.1272\n",
      "[Trial 182] Epoch 20/60, Training Loss: 0.1264, Validation Loss: 0.1227\n",
      "[Trial 181] Epoch 26/60, Training Loss: 0.1254, Validation Loss: 0.1267\n",
      "[Trial 183] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1256\n",
      "[Trial 186] Epoch 2/60, Training Loss: 0.1283, Validation Loss: 0.1425\n",
      "[Trial 178] Epoch 41/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 184] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1278\n",
      "[Trial 182] Epoch 21/60, Training Loss: 0.1264, Validation Loss: 0.1231\n",
      "[Trial 185] Epoch 7/60, Training Loss: 0.1266, Validation Loss: 0.1236\n",
      "[Trial 181] Epoch 27/60, Training Loss: 0.1257, Validation Loss: 0.1280\n",
      "[Trial 183] Epoch 12/60, Training Loss: 0.1283, Validation Loss: 0.1254\n",
      "[Trial 178] Epoch 42/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 184] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1270\n",
      "[Trial 186] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1303\n",
      "[Trial 182] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1224\n",
      "[Trial 181] Epoch 28/60, Training Loss: 0.1254, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 13/60, Training Loss: 0.1259, Validation Loss: 0.1252\n",
      "[Trial 185] Epoch 8/60, Training Loss: 0.1263, Validation Loss: 0.1239\n",
      "[Trial 178] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 184] Epoch 13/60, Training Loss: 0.1260, Validation Loss: 0.1273\n",
      "[Trial 182] Epoch 23/60, Training Loss: 0.1263, Validation Loss: 0.1225\n",
      "[Trial 181] Epoch 29/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 14/60, Training Loss: 0.1309, Validation Loss: 0.1267\n",
      "[Trial 186] Epoch 4/60, Training Loss: 0.1267, Validation Loss: 0.1289\n",
      "[Trial 178] Epoch 44/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 184] Epoch 14/60, Training Loss: 0.1259, Validation Loss: 0.1269\n",
      "[Trial 182] Epoch 24/60, Training Loss: 0.1262, Validation Loss: 0.1223\n",
      "[Trial 181] Epoch 30/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 15/60, Training Loss: 0.1264, Validation Loss: 0.1255\n",
      "[Trial 185] Epoch 9/60, Training Loss: 0.1263, Validation Loss: 0.1242\n",
      "[Trial 184] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1270\n",
      "[Trial 182] Epoch 25/60, Training Loss: 0.1264, Validation Loss: 0.1225\n",
      "[Trial 178] Epoch 45/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 181] Epoch 31/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 186] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1273\n",
      "[Trial 183] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 185] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1236\n",
      "[Trial 182] Epoch 26/60, Training Loss: 0.1263, Validation Loss: 0.1224\n",
      "[Trial 184] Epoch 16/60, Training Loss: 0.1262, Validation Loss: 0.1265\n",
      "[Trial 181] Epoch 32/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 178] Epoch 46/60, Training Loss: 0.1255, Validation Loss: 0.1257\n",
      "[Trial 183] Epoch 17/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 186] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1271\n",
      "[Trial 182] Epoch 27/60, Training Loss: 0.1263, Validation Loss: 0.1223\n",
      "[Trial 181] Epoch 33/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 184] Epoch 17/60, Training Loss: 0.1261, Validation Loss: 0.1263\n",
      "[Trial 183] Epoch 18/60, Training Loss: 0.1259, Validation Loss: 0.1266\n",
      "[Trial 178] Epoch 47/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 185] Epoch 11/60, Training Loss: 0.1274, Validation Loss: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:43:35,368] Trial 182 finished with value: 0.12226987654964129 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0030752673902744282, 'batch_size': 32, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 182] Epoch 28/60, Training Loss: 0.1262, Validation Loss: 0.1223\n",
      "[Trial 182] Early stopping after 28 epochs.\n",
      "[Trial 181] Epoch 34/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 184] Epoch 18/60, Training Loss: 0.1260, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 19/60, Training Loss: 0.1304, Validation Loss: 0.1254\n",
      "[Trial 186] Epoch 7/60, Training Loss: 0.1265, Validation Loss: 0.1264\n",
      "[Trial 178] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 187] Epoch 1/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 181] Epoch 35/60, Training Loss: 0.1251, Validation Loss: 0.1265\n",
      "[Trial 184] Epoch 19/60, Training Loss: 0.1262, Validation Loss: 0.1264\n",
      "[Trial 185] Epoch 12/60, Training Loss: 0.1539, Validation Loss: 0.1238\n",
      "[Trial 183] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 178] Epoch 49/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 187] Epoch 2/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 186] Epoch 8/60, Training Loss: 0.1261, Validation Loss: 0.1255\n",
      "[Trial 181] Epoch 36/60, Training Loss: 0.1253, Validation Loss: 0.1266\n",
      "[Trial 184] Epoch 20/60, Training Loss: 0.1256, Validation Loss: 0.1262\n",
      "[Trial 183] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 178] Epoch 50/60, Training Loss: 0.1255, Validation Loss: 0.1257\n",
      "[Trial 185] Epoch 13/60, Training Loss: 0.1263, Validation Loss: 0.1238\n",
      "[Trial 187] Epoch 3/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 181] Epoch 37/60, Training Loss: 0.1251, Validation Loss: 0.1268\n",
      "[Trial 184] Epoch 21/60, Training Loss: 0.1258, Validation Loss: 0.1264\n",
      "[Trial 183] Epoch 22/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 186] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:44:22,370] Trial 187 finished with value: inf and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.09968899329906643, 'batch_size': 32, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 187] Epoch 4/60, Training Loss: nan, Validation Loss: nan\n",
      "[Trial 187] Early stopping after 4 epochs.\n",
      "[Trial 178] Epoch 51/60, Training Loss: 0.1253, Validation Loss: 0.1258\n",
      "[Trial 181] Epoch 38/60, Training Loss: 0.1251, Validation Loss: 0.1266\n",
      "[Trial 184] Epoch 22/60, Training Loss: 0.1264, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 23/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 185] Epoch 14/60, Training Loss: 0.1259, Validation Loss: 0.1236\n",
      "[Trial 188] Epoch 1/60, Training Loss: 0.7203, Validation Loss: 0.1874\n",
      "[Trial 181] Epoch 39/60, Training Loss: 0.1253, Validation Loss: 0.1265\n",
      "[Trial 178] Epoch 52/60, Training Loss: 0.1255, Validation Loss: 0.1258\n",
      "[Trial 186] Epoch 10/60, Training Loss: 0.1261, Validation Loss: 0.1267\n",
      "[Trial 184] Epoch 23/60, Training Loss: 0.1265, Validation Loss: 0.1261\n",
      "[Trial 183] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 188] Epoch 2/60, Training Loss: 0.1288, Validation Loss: 0.1570\n",
      "[Trial 181] Epoch 40/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 185] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1237\n",
      "[Trial 178] Epoch 53/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 183] Epoch 25/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1263\n",
      "[Trial 188] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1428\n",
      "[Trial 186] Epoch 11/60, Training Loss: 0.1265, Validation Loss: 0.1253\n",
      "[Trial 181] Epoch 41/60, Training Loss: 0.1251, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 26/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 184] Epoch 25/60, Training Loss: 0.1257, Validation Loss: 0.1265\n",
      "[Trial 178] Epoch 54/60, Training Loss: 0.1252, Validation Loss: 0.1258\n",
      "[Trial 185] Epoch 16/60, Training Loss: 0.1263, Validation Loss: 0.1237\n",
      "[Trial 188] Epoch 4/60, Training Loss: 0.1273, Validation Loss: 0.1339\n",
      "[Trial 181] Epoch 42/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 27/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 26/60, Training Loss: 0.1257, Validation Loss: 0.1264\n",
      "[Trial 186] Epoch 12/60, Training Loss: 0.1261, Validation Loss: 0.1252\n",
      "[Trial 178] Epoch 55/60, Training Loss: 0.1254, Validation Loss: 0.1257\n",
      "[Trial 188] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1309\n",
      "[Trial 181] Epoch 43/60, Training Loss: 0.1254, Validation Loss: 0.1266\n",
      "[Trial 183] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:45:31,216] Trial 185 finished with value: 0.12362650744616985 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.015118923278309061, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 184] Epoch 27/60, Training Loss: 0.1258, Validation Loss: 0.1261\n",
      "[Trial 185] Epoch 17/60, Training Loss: 0.1261, Validation Loss: 0.1237\n",
      "[Trial 185] Early stopping after 17 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:45:35,418] Trial 178 finished with value: 0.1257390188674132 and parameters: {'hidden_dim': 320, 'latent_dim': 96, 'learning_rate': 0.012426512940083357, 'batch_size': 32, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 178] Epoch 56/60, Training Loss: 0.1252, Validation Loss: 0.1257\n",
      "[Trial 178] Early stopping after 56 epochs.\n",
      "[Trial 188] Epoch 6/60, Training Loss: 0.1265, Validation Loss: 0.1268\n",
      "[Trial 181] Epoch 44/60, Training Loss: 0.1252, Validation Loss: 0.1265\n",
      "[Trial 186] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1254\n",
      "[Trial 183] Epoch 29/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 28/60, Training Loss: 0.1257, Validation Loss: 0.1261\n",
      "[Trial 188] Epoch 7/60, Training Loss: 0.1267, Validation Loss: 0.1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:45:50,085] Trial 181 finished with value: 0.12648463398218154 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0029101517865085533, 'batch_size': 32, 'patience': 6}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 181] Epoch 45/60, Training Loss: 0.1252, Validation Loss: 0.1266\n",
      "[Trial 181] Early stopping after 45 epochs.\n",
      "[Trial 189] Epoch 1/60, Training Loss: 0.2426, Validation Loss: 0.1844\n",
      "[Trial 183] Epoch 30/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 184] Epoch 29/60, Training Loss: 0.1256, Validation Loss: 0.1266\n",
      "[Trial 190] Epoch 1/60, Training Loss: 0.2308, Validation Loss: 0.2090\n",
      "[Trial 186] Epoch 14/60, Training Loss: 0.1262, Validation Loss: 0.1258\n",
      "[Trial 188] Epoch 8/60, Training Loss: 0.1267, Validation Loss: 0.1275\n",
      "[Trial 183] Epoch 31/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 30/60, Training Loss: 0.1453, Validation Loss: 0.1262\n",
      "[Trial 189] Epoch 2/60, Training Loss: 0.1281, Validation Loss: 0.1472\n",
      "[Trial 191] Epoch 1/60, Training Loss: 27.8734, Validation Loss: 0.1501\n",
      "[Trial 188] Epoch 9/60, Training Loss: 0.1264, Validation Loss: 0.1270\n",
      "[Trial 190] Epoch 2/60, Training Loss: 0.1297, Validation Loss: 0.1693\n",
      "[Trial 186] Epoch 15/60, Training Loss: 0.1260, Validation Loss: 0.1257\n",
      "[Trial 183] Epoch 32/60, Training Loss: 0.1259, Validation Loss: 0.1253\n",
      "[Trial 184] Epoch 31/60, Training Loss: 0.1257, Validation Loss: 0.1261\n",
      "[Trial 188] Epoch 10/60, Training Loss: 0.1258, Validation Loss: 0.1267\n",
      "[Trial 189] Epoch 3/60, Training Loss: 0.1267, Validation Loss: 0.1356\n",
      "[Trial 191] Epoch 2/60, Training Loss: 0.1310, Validation Loss: 0.1351\n",
      "[Trial 183] Epoch 33/60, Training Loss: 0.1267, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1264\n",
      "[Trial 188] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1259\n",
      "[Trial 186] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 190] Epoch 3/60, Training Loss: 0.1277, Validation Loss: 0.1487\n",
      "[Trial 183] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 33/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 189] Epoch 4/60, Training Loss: 0.1272, Validation Loss: 0.1317\n",
      "[Trial 191] Epoch 3/60, Training Loss: 0.1281, Validation Loss: 0.1276\n",
      "[Trial 188] Epoch 12/60, Training Loss: 0.1257, Validation Loss: 0.1261\n",
      "[Trial 183] Epoch 35/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 186] Epoch 17/60, Training Loss: 0.1300, Validation Loss: 0.1251\n",
      "[Trial 184] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 190] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1413\n",
      "[Trial 188] Epoch 13/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 189] Epoch 5/60, Training Loss: 0.1262, Validation Loss: 0.1298\n",
      "[Trial 183] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 191] Epoch 4/60, Training Loss: 0.1267, Validation Loss: 0.1250\n",
      "[Trial 188] Epoch 14/60, Training Loss: 0.1257, Validation Loss: 0.1267\n",
      "[Trial 186] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1253\n",
      "[Trial 183] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1252\n",
      "[Trial 190] Epoch 5/60, Training Loss: 0.1266, Validation Loss: 0.1547\n",
      "[Trial 184] Epoch 36/60, Training Loss: 0.1255, Validation Loss: 0.1261\n",
      "[Trial 189] Epoch 6/60, Training Loss: 0.1261, Validation Loss: 0.1297\n",
      "[Trial 188] Epoch 15/60, Training Loss: 0.1257, Validation Loss: 0.1256\n",
      "[Trial 191] Epoch 5/60, Training Loss: 0.1263, Validation Loss: 0.1251\n",
      "[Trial 183] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 37/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 186] Epoch 19/60, Training Loss: 0.1259, Validation Loss: 0.1250\n",
      "[Trial 188] Epoch 16/60, Training Loss: 0.1258, Validation Loss: 0.1256\n",
      "[Trial 190] Epoch 6/60, Training Loss: 0.1268, Validation Loss: 0.1312\n",
      "[Trial 183] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 189] Epoch 7/60, Training Loss: 0.1261, Validation Loss: 0.1279\n",
      "[Trial 184] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 17/60, Training Loss: 0.1259, Validation Loss: 0.1258\n",
      "[Trial 191] Epoch 6/60, Training Loss: 0.1258, Validation Loss: 0.1248\n",
      "[Trial 186] Epoch 20/60, Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "[Trial 183] Epoch 40/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 39/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 18/60, Training Loss: 0.1258, Validation Loss: 0.1264\n",
      "[Trial 190] Epoch 7/60, Training Loss: 0.1255, Validation Loss: 0.1298\n",
      "[Trial 189] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1290\n",
      "[Trial 183] Epoch 41/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 184] Epoch 40/60, Training Loss: 0.1256, Validation Loss: 0.1261\n",
      "[Trial 191] Epoch 7/60, Training Loss: 0.1260, Validation Loss: 0.1248\n",
      "[Trial 186] Epoch 21/60, Training Loss: 0.1259, Validation Loss: 0.1251\n",
      "[Trial 188] Epoch 19/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 183] Epoch 42/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 189] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1276\n",
      "[Trial 184] Epoch 41/60, Training Loss: 0.1255, Validation Loss: 0.1260\n",
      "[Trial 190] Epoch 8/60, Training Loss: 0.1255, Validation Loss: 0.1296\n",
      "[Trial 188] Epoch 20/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 191] Epoch 8/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 183] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 186] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1250\n",
      "[Trial 184] Epoch 42/60, Training Loss: 0.1254, Validation Loss: 0.1261\n",
      "[Trial 188] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 189] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1371\n",
      "[Trial 183] Epoch 44/60, Training Loss: 0.1255, Validation Loss: 0.1252\n",
      "[Trial 190] Epoch 9/60, Training Loss: 0.1256, Validation Loss: 0.1293\n",
      "[Trial 184] Epoch 43/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 22/60, Training Loss: 0.1257, Validation Loss: 0.1254\n",
      "[Trial 191] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1249\n",
      "[Trial 186] Epoch 23/60, Training Loss: 0.1257, Validation Loss: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:48:57,683] Trial 183 finished with value: 0.12516493250926336 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.015140800365835181, 'batch_size': 32, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 183] Epoch 45/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 183] Early stopping after 45 epochs.\n",
      "[Trial 184] Epoch 44/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 189] Epoch 11/60, Training Loss: 0.1281, Validation Loss: 0.1273\n",
      "[Trial 188] Epoch 23/60, Training Loss: 0.1255, Validation Loss: 0.1257\n",
      "[Trial 190] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1299\n",
      "[Trial 191] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1251\n",
      "[Trial 186] Epoch 24/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 184] Epoch 45/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 24/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 192] Epoch 1/60, Training Loss: 0.2370, Validation Loss: 0.1925\n",
      "[Trial 189] Epoch 12/60, Training Loss: 0.1258, Validation Loss: 0.1271\n",
      "[Trial 184] Epoch 46/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 190] Epoch 11/60, Training Loss: 0.1253, Validation Loss: 0.1289\n",
      "[Trial 188] Epoch 25/60, Training Loss: 0.1255, Validation Loss: 0.1255\n",
      "[Trial 186] Epoch 25/60, Training Loss: 0.1260, Validation Loss: 0.1252\n",
      "[Trial 191] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 192] Epoch 2/60, Training Loss: 0.1300, Validation Loss: 0.1634\n",
      "[Trial 184] Epoch 47/60, Training Loss: 0.1255, Validation Loss: 0.1264\n",
      "[Trial 188] Epoch 26/60, Training Loss: 0.1256, Validation Loss: 0.1255\n",
      "[Trial 189] Epoch 13/60, Training Loss: 0.1258, Validation Loss: 0.1275\n",
      "[Trial 190] Epoch 12/60, Training Loss: 0.1252, Validation Loss: 0.1290\n",
      "[Trial 184] Epoch 48/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 186] Epoch 26/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 188] Epoch 27/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 191] Epoch 12/60, Training Loss: 0.1408, Validation Loss: 0.1248\n",
      "[Trial 192] Epoch 3/60, Training Loss: 0.1289, Validation Loss: 0.1462\n",
      "[Trial 189] Epoch 14/60, Training Loss: 0.1258, Validation Loss: 0.1268\n",
      "[Trial 184] Epoch 49/60, Training Loss: 0.1255, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 28/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 190] Epoch 13/60, Training Loss: 0.1252, Validation Loss: 0.1281\n",
      "[Trial 186] Epoch 27/60, Training Loss: 0.1255, Validation Loss: 0.1250\n",
      "[Trial 191] Epoch 13/60, Training Loss: 0.1255, Validation Loss: 0.1248\n",
      "[Trial 184] Epoch 50/60, Training Loss: 0.1252, Validation Loss: 0.1261\n",
      "[Trial 188] Epoch 29/60, Training Loss: 0.1253, Validation Loss: 0.1254\n",
      "[Trial 192] Epoch 4/60, Training Loss: 0.1282, Validation Loss: 0.1396\n",
      "[Trial 189] Epoch 15/60, Training Loss: 0.1257, Validation Loss: 0.1270\n",
      "[Trial 184] Epoch 51/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 186] Epoch 28/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 190] Epoch 14/60, Training Loss: 0.1251, Validation Loss: 0.1284\n",
      "[Trial 191] Epoch 14/60, Training Loss: 0.1255, Validation Loss: 0.1249\n",
      "[Trial 189] Epoch 16/60, Training Loss: 0.1257, Validation Loss: 0.1269\n",
      "[Trial 192] Epoch 5/60, Training Loss: 0.1274, Validation Loss: 0.1324\n",
      "[Trial 184] Epoch 52/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 31/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 186] Epoch 29/60, Training Loss: 0.1253, Validation Loss: 0.1252\n",
      "[Trial 190] Epoch 15/60, Training Loss: 0.1254, Validation Loss: 0.1279\n",
      "[Trial 184] Epoch 53/60, Training Loss: 0.1252, Validation Loss: 0.1260\n",
      "[Trial 188] Epoch 32/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 191] Epoch 15/60, Training Loss: 0.1257, Validation Loss: 0.1250\n",
      "[Trial 189] Epoch 17/60, Training Loss: 0.1256, Validation Loss: 0.1269\n",
      "[Trial 192] Epoch 6/60, Training Loss: 0.1274, Validation Loss: 0.1329\n",
      "[Trial 188] Epoch 33/60, Training Loss: 0.1253, Validation Loss: 0.1253\n",
      "[Trial 184] Epoch 54/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 186] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 190] Epoch 16/60, Training Loss: 0.1251, Validation Loss: 0.1290\n",
      "[Trial 191] Epoch 16/60, Training Loss: 0.1256, Validation Loss: 0.1248\n",
      "[Trial 188] Epoch 34/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 189] Epoch 18/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 184] Epoch 55/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 192] Epoch 7/60, Training Loss: 0.1272, Validation Loss: 0.1288\n",
      "[Trial 188] Epoch 35/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 186] Epoch 31/60, Training Loss: 0.1258, Validation Loss: 0.1250\n",
      "[Trial 184] Epoch 56/60, Training Loss: 0.1252, Validation Loss: 0.1260\n",
      "[Trial 190] Epoch 17/60, Training Loss: 0.1252, Validation Loss: 0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:51:34,855] Trial 191 finished with value: 0.12478192510704199 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.014644526687378798, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 191] Epoch 17/60, Training Loss: 0.1258, Validation Loss: 0.1249\n",
      "[Trial 191] Early stopping after 17 epochs.\n",
      "[Trial 189] Epoch 19/60, Training Loss: 0.1255, Validation Loss: 0.1271\n",
      "[Trial 192] Epoch 8/60, Training Loss: 0.1273, Validation Loss: 0.1291\n",
      "[Trial 188] Epoch 36/60, Training Loss: 0.1254, Validation Loss: 0.1254\n",
      "[Trial 184] Epoch 57/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 186] Epoch 32/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 188] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1254\n",
      "[Trial 184] Epoch 58/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 193] Epoch 1/60, Training Loss: 0.2343, Validation Loss: 0.1947\n",
      "[Trial 190] Epoch 18/60, Training Loss: 0.1252, Validation Loss: 0.1279\n",
      "[Trial 189] Epoch 20/60, Training Loss: 0.1259, Validation Loss: 0.1271\n",
      "[Trial 192] Epoch 9/60, Training Loss: 0.1272, Validation Loss: 0.1267\n",
      "[Trial 188] Epoch 38/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 184] Epoch 59/60, Training Loss: 0.1252, Validation Loss: 0.1260\n",
      "[Trial 186] Epoch 33/60, Training Loss: 0.1258, Validation Loss: 0.1250\n",
      "[Trial 193] Epoch 2/60, Training Loss: 0.1289, Validation Loss: 0.1639\n",
      "[Trial 189] Epoch 21/60, Training Loss: 0.1252, Validation Loss: 0.1269\n",
      "[Trial 190] Epoch 19/60, Training Loss: 0.1255, Validation Loss: 0.1281\n",
      "[Trial 188] Epoch 39/60, Training Loss: 0.1253, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:52:18,865] Trial 184 finished with value: 0.12595658699671428 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0028635507444850694, 'batch_size': 32, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 184] Epoch 60/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 192] Epoch 10/60, Training Loss: 0.1267, Validation Loss: 0.1265\n",
      "[Trial 186] Epoch 34/60, Training Loss: 0.1253, Validation Loss: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:52:28,436] Trial 188 finished with value: 0.12534271727005641 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.004180503808112558, 'batch_size': 32, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 188] Epoch 40/60, Training Loss: 0.1253, Validation Loss: 0.1253\n",
      "[Trial 188] Early stopping after 40 epochs.\n",
      "[Trial 189] Epoch 22/60, Training Loss: 0.1253, Validation Loss: 0.1269\n",
      "[Trial 193] Epoch 3/60, Training Loss: 0.1274, Validation Loss: 0.1454\n",
      "[Trial 190] Epoch 20/60, Training Loss: 0.1251, Validation Loss: 0.1287\n",
      "[Trial 194] Epoch 1/60, Training Loss: 0.5417, Validation Loss: 0.1510\n",
      "[Trial 192] Epoch 11/60, Training Loss: 0.1264, Validation Loss: 0.1260\n",
      "[Trial 186] Epoch 35/60, Training Loss: 0.1254, Validation Loss: 0.1250\n",
      "[Trial 195] Epoch 1/60, Training Loss: 0.5437, Validation Loss: 0.1282\n",
      "[Trial 189] Epoch 23/60, Training Loss: 0.1252, Validation Loss: 0.1269\n",
      "[Trial 193] Epoch 4/60, Training Loss: 0.1270, Validation Loss: 0.1420\n",
      "[Trial 194] Epoch 2/60, Training Loss: 0.1287, Validation Loss: 0.1313\n",
      "[Trial 190] Epoch 21/60, Training Loss: 0.1251, Validation Loss: 0.1281\n",
      "[Trial 192] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:53:05,491] Trial 186 finished with value: 0.12497951922317346 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'learning_rate': 0.0029468118847866607, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 195] Epoch 2/60, Training Loss: 0.1262, Validation Loss: 0.1254\n",
      "[Trial 186] Epoch 36/60, Training Loss: 0.1256, Validation Loss: 0.1250\n",
      "[Trial 186] Early stopping after 36 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:53:11,719] Trial 189 finished with value: 0.126834940413634 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.002859194953493712, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 189] Epoch 24/60, Training Loss: 0.1250, Validation Loss: 0.1271\n",
      "[Trial 189] Early stopping after 24 epochs.\n",
      "[Trial 193] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1352\n",
      "[Trial 194] Epoch 3/60, Training Loss: 0.1275, Validation Loss: 0.1285\n",
      "[Trial 190] Epoch 22/60, Training Loss: 0.1249, Validation Loss: 0.1281\n",
      "[Trial 192] Epoch 13/60, Training Loss: 0.1262, Validation Loss: 0.1245\n",
      "[Trial 196] Epoch 1/60, Training Loss: 0.6906, Validation Loss: 0.1841\n",
      "[Trial 195] Epoch 3/60, Training Loss: 0.1265, Validation Loss: 0.1254\n",
      "[Trial 197] Epoch 1/60, Training Loss: 1.6264, Validation Loss: 0.1264\n",
      "[Trial 193] Epoch 6/60, Training Loss: 0.1263, Validation Loss: 0.1310\n",
      "[Trial 194] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1287\n",
      "[Trial 190] Epoch 23/60, Training Loss: 0.1247, Validation Loss: 0.1283\n",
      "[Trial 192] Epoch 14/60, Training Loss: 0.1265, Validation Loss: 0.1256\n",
      "[Trial 196] Epoch 2/60, Training Loss: 0.1309, Validation Loss: 0.1511\n",
      "[Trial 195] Epoch 4/60, Training Loss: 0.1263, Validation Loss: 0.1255\n",
      "[Trial 197] Epoch 2/60, Training Loss: 0.1265, Validation Loss: 0.1256\n",
      "[Trial 193] Epoch 7/60, Training Loss: 0.1262, Validation Loss: 0.1306\n",
      "[Trial 194] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1264\n",
      "[Trial 192] Epoch 15/60, Training Loss: 0.1266, Validation Loss: 0.1253\n",
      "[Trial 190] Epoch 24/60, Training Loss: 0.1249, Validation Loss: 0.1277\n",
      "[Trial 196] Epoch 3/60, Training Loss: 0.1284, Validation Loss: 0.1389\n",
      "[Trial 195] Epoch 5/60, Training Loss: 0.1260, Validation Loss: 0.1274\n",
      "[Trial 197] Epoch 3/60, Training Loss: 0.1264, Validation Loss: 0.1259\n",
      "[Trial 194] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 193] Epoch 8/60, Training Loss: 0.1258, Validation Loss: 0.1302\n",
      "[Trial 196] Epoch 4/60, Training Loss: 0.1269, Validation Loss: 0.1311\n",
      "[Trial 192] Epoch 16/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 195] Epoch 6/60, Training Loss: 0.1265, Validation Loss: 0.1257\n",
      "[Trial 197] Epoch 4/60, Training Loss: 0.1265, Validation Loss: 0.1257\n",
      "[Trial 190] Epoch 25/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 194] Epoch 7/60, Training Loss: 0.1262, Validation Loss: 0.1261\n",
      "[Trial 193] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1286\n",
      "[Trial 197] Epoch 5/60, Training Loss: 0.1265, Validation Loss: 0.1264\n",
      "[Trial 196] Epoch 5/60, Training Loss: 0.1268, Validation Loss: 0.1280\n",
      "[Trial 195] Epoch 7/60, Training Loss: 0.1255, Validation Loss: 0.1253\n",
      "[Trial 192] Epoch 17/60, Training Loss: 0.1265, Validation Loss: 0.1260\n",
      "[Trial 190] Epoch 26/60, Training Loss: 0.1248, Validation Loss: 0.1279\n",
      "[Trial 194] Epoch 8/60, Training Loss: 0.1265, Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:54:53,608] Trial 197 finished with value: 0.1255899790674448 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.024139025801042067, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 197] Epoch 6/60, Training Loss: 0.1258, Validation Loss: 0.1258\n",
      "[Trial 197] Early stopping after 6 epochs.\n",
      "[Trial 193] Epoch 10/60, Training Loss: 0.1258, Validation Loss: 0.1285\n",
      "[Trial 196] Epoch 6/60, Training Loss: 0.1264, Validation Loss: 0.1284\n",
      "[Trial 195] Epoch 8/60, Training Loss: 0.1258, Validation Loss: 0.1257\n",
      "[Trial 192] Epoch 18/60, Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "[Trial 190] Epoch 27/60, Training Loss: 0.1248, Validation Loss: 0.1279\n",
      "[Trial 194] Epoch 9/60, Training Loss: 0.1260, Validation Loss: 0.1260\n",
      "[Trial 198] Epoch 1/60, Training Loss: 0.7125, Validation Loss: 0.1834\n",
      "[Trial 196] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1272\n",
      "[Trial 193] Epoch 11/60, Training Loss: 0.1255, Validation Loss: 0.1292\n",
      "[Trial 195] Epoch 9/60, Training Loss: 0.1258, Validation Loss: 0.1255\n",
      "[Trial 192] Epoch 19/60, Training Loss: 0.1263, Validation Loss: 0.1249\n",
      "[Trial 190] Epoch 28/60, Training Loss: 0.1248, Validation Loss: 0.1282\n",
      "[Trial 198] Epoch 2/60, Training Loss: 0.1307, Validation Loss: 0.1549\n",
      "[Trial 194] Epoch 10/60, Training Loss: 0.1262, Validation Loss: 0.1256\n",
      "[Trial 196] Epoch 8/60, Training Loss: 0.1264, Validation Loss: 0.1266\n",
      "[Trial 193] Epoch 12/60, Training Loss: 0.1257, Validation Loss: 0.1286\n",
      "[Trial 195] Epoch 10/60, Training Loss: 0.1253, Validation Loss: 0.1257\n",
      "[Trial 192] Epoch 20/60, Training Loss: 0.1257, Validation Loss: 0.1245\n",
      "[Trial 190] Epoch 29/60, Training Loss: 0.1254, Validation Loss: 0.1278\n",
      "[Trial 198] Epoch 3/60, Training Loss: 0.1277, Validation Loss: 0.1367\n",
      "[Trial 194] Epoch 11/60, Training Loss: 0.1262, Validation Loss: 0.1253\n",
      "[Trial 196] Epoch 9/60, Training Loss: 0.1267, Validation Loss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:55:53,074] Trial 195 finished with value: 0.12534233070909978 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.017773255576633087, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 195] Epoch 11/60, Training Loss: 0.1255, Validation Loss: 0.1254\n",
      "[Trial 195] Early stopping after 11 epochs.\n",
      "[Trial 193] Epoch 13/60, Training Loss: 0.1253, Validation Loss: 0.1291\n",
      "[Trial 192] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 198] Epoch 4/60, Training Loss: 0.1271, Validation Loss: 0.1318\n",
      "[Trial 190] Epoch 30/60, Training Loss: 0.1249, Validation Loss: 0.1278\n",
      "[Trial 194] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1256\n",
      "[Trial 196] Epoch 10/60, Training Loss: 0.1260, Validation Loss: 0.1260\n",
      "[Trial 199] Epoch 1/60, Training Loss: 0.5426, Validation Loss: 0.1476\n",
      "[Trial 193] Epoch 14/60, Training Loss: 0.1252, Validation Loss: 0.1278\n",
      "[Trial 198] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1318\n",
      "[Trial 192] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1245\n",
      "[Trial 190] Epoch 31/60, Training Loss: 0.1246, Validation Loss: 0.1277\n",
      "[Trial 194] Epoch 13/60, Training Loss: 0.1266, Validation Loss: 0.1251\n",
      "[Trial 196] Epoch 11/60, Training Loss: 0.1268, Validation Loss: 0.1271\n",
      "[Trial 199] Epoch 2/60, Training Loss: 0.1275, Validation Loss: 0.1323\n",
      "[Trial 193] Epoch 15/60, Training Loss: 0.1252, Validation Loss: 0.1279\n",
      "[Trial 198] Epoch 6/60, Training Loss: 0.1266, Validation Loss: 0.1279\n",
      "[Trial 192] Epoch 23/60, Training Loss: 0.1261, Validation Loss: 0.1247\n",
      "[Trial 190] Epoch 32/60, Training Loss: 0.1248, Validation Loss: 0.1279\n",
      "[Trial 194] Epoch 14/60, Training Loss: 0.1257, Validation Loss: 0.1253\n",
      "[Trial 196] Epoch 12/60, Training Loss: 0.1264, Validation Loss: 0.1263\n",
      "[Trial 199] Epoch 3/60, Training Loss: 0.1271, Validation Loss: 0.1291\n",
      "[Trial 198] Epoch 7/60, Training Loss: 0.1264, Validation Loss: 0.1285\n",
      "[Trial 193] Epoch 16/60, Training Loss: 0.1251, Validation Loss: 0.1281\n",
      "[Trial 192] Epoch 24/60, Training Loss: 0.1260, Validation Loss: 0.1245\n",
      "[Trial 196] Epoch 13/60, Training Loss: 0.1261, Validation Loss: 0.1261\n",
      "[Trial 194] Epoch 15/60, Training Loss: 0.1258, Validation Loss: 0.1252\n",
      "[Trial 190] Epoch 33/60, Training Loss: 0.1248, Validation Loss: 0.1278\n",
      "[Trial 199] Epoch 4/60, Training Loss: 0.1268, Validation Loss: 0.1280\n",
      "[Trial 198] Epoch 8/60, Training Loss: 0.1260, Validation Loss: 0.1267\n",
      "[Trial 193] Epoch 17/60, Training Loss: 0.1250, Validation Loss: 0.1277\n",
      "[Trial 192] Epoch 25/60, Training Loss: 0.1258, Validation Loss: 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:57:22,041] Trial 196 finished with value: 0.12596090311805408 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0017928801039622912, 'batch_size': 16, 'patience': 4}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 196] Epoch 14/60, Training Loss: 0.1255, Validation Loss: 0.1262\n",
      "[Trial 196] Early stopping after 14 epochs.\n",
      "[Trial 194] Epoch 16/60, Training Loss: 0.1263, Validation Loss: 0.1257\n",
      "[Trial 198] Epoch 9/60, Training Loss: 0.1261, Validation Loss: 0.1268\n",
      "[Trial 199] Epoch 5/60, Training Loss: 0.1264, Validation Loss: 0.1281\n",
      "[Trial 190] Epoch 34/60, Training Loss: 0.1250, Validation Loss: 0.1278\n",
      "[Trial 193] Epoch 18/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 192] Epoch 26/60, Training Loss: 0.1261, Validation Loss: 0.1244\n",
      "[Trial 198] Epoch 10/60, Training Loss: 0.1263, Validation Loss: 0.1262\n",
      "[Trial 194] Epoch 17/60, Training Loss: 0.1257, Validation Loss: 0.1251\n",
      "[Trial 199] Epoch 6/60, Training Loss: 0.1258, Validation Loss: 0.1290\n",
      "[Trial 190] Epoch 35/60, Training Loss: 0.1247, Validation Loss: 0.1279\n",
      "[Trial 193] Epoch 19/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 198] Epoch 11/60, Training Loss: 0.1258, Validation Loss: 0.1271\n",
      "[Trial 192] Epoch 27/60, Training Loss: 0.1261, Validation Loss: 0.1248\n",
      "[Trial 194] Epoch 18/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 199] Epoch 7/60, Training Loss: 0.1256, Validation Loss: 0.1269\n",
      "[Trial 190] Epoch 36/60, Training Loss: 0.1250, Validation Loss: 0.1280\n",
      "[Trial 193] Epoch 20/60, Training Loss: 0.1251, Validation Loss: 0.1277\n",
      "[Trial 198] Epoch 12/60, Training Loss: 0.1262, Validation Loss: 0.1270\n",
      "[Trial 194] Epoch 19/60, Training Loss: 0.1256, Validation Loss: 0.1251\n",
      "[Trial 192] Epoch 28/60, Training Loss: 0.1261, Validation Loss: 0.1245\n",
      "[Trial 199] Epoch 8/60, Training Loss: 0.1256, Validation Loss: 0.1270\n",
      "[Trial 190] Epoch 37/60, Training Loss: 0.1249, Validation Loss: 0.1278\n",
      "[Trial 193] Epoch 21/60, Training Loss: 0.1254, Validation Loss: 0.1279\n",
      "[Trial 198] Epoch 13/60, Training Loss: 0.1262, Validation Loss: 0.1264\n",
      "[Trial 194] Epoch 20/60, Training Loss: 0.1252, Validation Loss: 0.1251\n",
      "[Trial 199] Epoch 9/60, Training Loss: 0.1257, Validation Loss: 0.1268\n",
      "[Trial 192] Epoch 29/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 193] Epoch 22/60, Training Loss: 0.1252, Validation Loss: 0.1277\n",
      "[Trial 198] Epoch 14/60, Training Loss: 0.1257, Validation Loss: 0.1263\n",
      "[Trial 190] Epoch 38/60, Training Loss: 0.1246, Validation Loss: 0.1278\n",
      "[Trial 194] Epoch 21/60, Training Loss: 0.1254, Validation Loss: 0.1252\n",
      "[Trial 199] Epoch 10/60, Training Loss: 0.1252, Validation Loss: 0.1271\n",
      "[Trial 192] Epoch 30/60, Training Loss: 0.1258, Validation Loss: 0.1246\n",
      "[Trial 198] Epoch 15/60, Training Loss: 0.1259, Validation Loss: 0.1260\n",
      "[Trial 193] Epoch 23/60, Training Loss: 0.1252, Validation Loss: 0.1283\n",
      "[Trial 190] Epoch 39/60, Training Loss: 0.1246, Validation Loss: 0.1278\n",
      "[Trial 194] Epoch 22/60, Training Loss: 0.1258, Validation Loss: 0.1251\n",
      "[Trial 199] Epoch 11/60, Training Loss: 0.1254, Validation Loss: 0.1268\n",
      "[Trial 198] Epoch 16/60, Training Loss: 0.1258, Validation Loss: 0.1261\n",
      "[Trial 192] Epoch 31/60, Training Loss: 0.1259, Validation Loss: 0.1244\n",
      "[Trial 193] Epoch 24/60, Training Loss: 0.1252, Validation Loss: 0.1277\n",
      "[Trial 190] Epoch 40/60, Training Loss: 0.1247, Validation Loss: 0.1278\n",
      "[Trial 199] Epoch 12/60, Training Loss: 0.1254, Validation Loss: 0.1267\n",
      "[Trial 194] Epoch 23/60, Training Loss: 0.1254, Validation Loss: 0.1251\n",
      "[Trial 198] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1260\n",
      "[Trial 192] Epoch 32/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 193] Epoch 25/60, Training Loss: 0.1250, Validation Loss: 0.1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:59:28,376] Trial 190 finished with value: 0.1277347066750129 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0018320341362708444, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 190] Epoch 41/60, Training Loss: 0.1244, Validation Loss: 0.1278\n",
      "[Trial 190] Early stopping after 41 epochs.\n",
      "[Trial 199] Epoch 13/60, Training Loss: 0.1254, Validation Loss: 0.1268\n",
      "[Trial 194] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 198] Epoch 18/60, Training Loss: 0.1255, Validation Loss: 0.1260\n",
      "[Trial 192] Epoch 33/60, Training Loss: 0.1260, Validation Loss: 0.1243\n",
      "[Trial 193] Epoch 26/60, Training Loss: 0.1250, Validation Loss: 0.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 15:59:42,560] Trial 194 finished with value: 0.12505543678998948 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.004305368226278831, 'batch_size': 16, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 198] Epoch 19/60, Training Loss: 0.1252, Validation Loss: 0.1259\n",
      "[Trial 194] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1251\n",
      "[Trial 194] Early stopping after 25 epochs.\n",
      "[Trial 199] Epoch 14/60, Training Loss: 0.1250, Validation Loss: 0.1268\n",
      "[Trial 192] Epoch 34/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 193] Epoch 27/60, Training Loss: 0.1251, Validation Loss: 0.1278\n",
      "[Trial 198] Epoch 20/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 199] Epoch 15/60, Training Loss: 0.1250, Validation Loss: 0.1267\n",
      "[Trial 193] Epoch 28/60, Training Loss: 0.1248, Validation Loss: 0.1277\n",
      "[Trial 192] Epoch 35/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 198] Epoch 21/60, Training Loss: 0.1256, Validation Loss: 0.1260\n",
      "[Trial 199] Epoch 16/60, Training Loss: 0.1250, Validation Loss: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 16:00:11,304] Trial 193 finished with value: 0.12765253918866318 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0018535791012237789, 'batch_size': 16, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 193] Epoch 29/60, Training Loss: 0.1251, Validation Loss: 0.1277\n",
      "[Trial 193] Early stopping after 29 epochs.\n",
      "[Trial 192] Epoch 36/60, Training Loss: 0.1260, Validation Loss: 0.1244\n",
      "[Trial 199] Epoch 17/60, Training Loss: 0.1254, Validation Loss: 0.1267\n",
      "[Trial 198] Epoch 22/60, Training Loss: 0.1255, Validation Loss: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 16:00:19,897] Trial 199 finished with value: 0.1266759352137645 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0043563897447358325, 'batch_size': 16, 'patience': 3}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 198] Epoch 23/60, Training Loss: 0.1253, Validation Loss: 0.1260\n",
      "[Trial 199] Epoch 18/60, Training Loss: 0.1251, Validation Loss: 0.1267\n",
      "[Trial 199] Early stopping after 18 epochs.\n",
      "[Trial 192] Epoch 37/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 198] Epoch 24/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 192] Epoch 38/60, Training Loss: 0.1256, Validation Loss: 0.1244\n",
      "[Trial 198] Epoch 25/60, Training Loss: 0.1253, Validation Loss: 0.1259\n",
      "[Trial 192] Epoch 39/60, Training Loss: 0.1255, Validation Loss: 0.1243\n",
      "[Trial 198] Epoch 26/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 192] Epoch 40/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 198] Epoch 27/60, Training Loss: 0.1254, Validation Loss: 0.1258\n",
      "[Trial 198] Epoch 28/60, Training Loss: 0.1256, Validation Loss: 0.1259\n",
      "[Trial 192] Epoch 41/60, Training Loss: 0.1256, Validation Loss: 0.1243\n",
      "[Trial 198] Epoch 29/60, Training Loss: 0.1252, Validation Loss: 0.1258\n",
      "[Trial 192] Epoch 42/60, Training Loss: 0.1259, Validation Loss: 0.1243\n",
      "[Trial 198] Epoch 30/60, Training Loss: 0.1256, Validation Loss: 0.1258\n",
      "[Trial 192] Epoch 43/60, Training Loss: 0.1255, Validation Loss: 0.1243\n",
      "[Trial 198] Epoch 31/60, Training Loss: 0.1254, Validation Loss: 0.1259\n",
      "[Trial 198] Epoch 32/60, Training Loss: 0.1256, Validation Loss: 0.1259\n",
      "[Trial 192] Epoch 44/60, Training Loss: 0.1257, Validation Loss: 0.1243\n",
      "[Trial 198] Epoch 33/60, Training Loss: 0.1252, Validation Loss: 0.1258\n",
      "[Trial 192] Epoch 45/60, Training Loss: 0.1257, Validation Loss: 0.1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 16:01:11,139] Trial 198 finished with value: 0.12582179270684718 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0018020413741597124, 'batch_size': 16, 'patience': 5}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 198] Epoch 34/60, Training Loss: 0.1252, Validation Loss: 0.1259\n",
      "[Trial 198] Early stopping after 34 epochs.\n",
      "[Trial 192] Epoch 46/60, Training Loss: 0.1260, Validation Loss: 0.1242\n",
      "[Trial 192] Epoch 47/60, Training Loss: 0.1256, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 48/60, Training Loss: 0.1254, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 49/60, Training Loss: 0.1257, Validation Loss: 0.1242\n",
      "[Trial 192] Epoch 50/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 51/60, Training Loss: 0.1254, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 52/60, Training Loss: 0.1257, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 53/60, Training Loss: 0.1259, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 54/60, Training Loss: 0.1258, Validation Loss: 0.1243\n",
      "[Trial 192] Epoch 55/60, Training Loss: 0.1256, Validation Loss: 0.1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 16:01:38,020] Trial 192 finished with value: 0.12423308280607065 and parameters: {'hidden_dim': 192, 'latent_dim': 32, 'learning_rate': 0.0018458443832739656, 'batch_size': 16, 'patience': 10}. Best is trial 67 with value: 0.12163179100801548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 192] Epoch 56/60, Training Loss: 0.1258, Validation Loss: 0.1242\n",
      "[Trial 192] Early stopping after 56 epochs.\n",
      "\n",
      "Best Trial Number: 67\n",
      "Best Parameters: {'hidden_dim': 64, 'latent_dim': 32, 'learning_rate': 0.0020937734068014295, 'batch_size': 8, 'patience': 3}\n",
      "Best Value (Objective): 0.12163179100801548\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# Define the VGAE model (same as before)\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim, latent_dim):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.encoder = VGEncoder(node_features, edge_features, hidden_dim, latent_dim)\n",
    "        self.decoder = VGDecoder(latent_dim, hidden_dim, node_features, edge_features)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        mu, logvar = self.encode(x, edge_index, edge_attr)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_x, reconstructed_edge_attr = self.decode(z, edge_index)\n",
    "        return reconstructed_x, reconstructed_edge_attr, mu, logvar\n",
    "\n",
    "    def generate_multiple_outputs(self, x, edge_index, edge_attr, num_samples=5):\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            z, _ = self.encode(x, edge_index, edge_attr)\n",
    "            for _ in range(num_samples):\n",
    "                # Add noise to z (sampling from normal distribution)\n",
    "                z_noisy = z + torch.randn_like(z) * 0.5  # Adjust std deviation for noise level\n",
    "                reconstructed_x, reconstructed_edge_attr = self.decode(z_noisy, edge_index)\n",
    "                outputs.append((reconstructed_x, reconstructed_edge_attr, edge_index))\n",
    "        return outputs\n",
    "\n",
    "class VGEncoder(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim, latent_dim):\n",
    "        super(VGEncoder, self).__init__()\n",
    "        self.conv1 = SAGEConv(node_features + edge_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)  # Additional layer\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_attr_expanded = torch.zeros((x.size(0), edge_attr.size(1))).to(x.device)\n",
    "        edge_attr_expanded[edge_index[0]] = edge_attr\n",
    "        x = torch.cat([x, edge_attr_expanded], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))  # Added layer\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "class VGDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, node_features, edge_features):\n",
    "        super(VGDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.node_decoder = nn.Linear(hidden_dim, node_features)\n",
    "        self.edge_decoder = nn.Linear(hidden_dim * 2, edge_features)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        h = F.relu(self.fc(z))\n",
    "        reconstructed_x = self.node_decoder(h)\n",
    "\n",
    "        # Edge reconstruction\n",
    "        row, col = edge_index\n",
    "        edge_h = torch.cat([h[row], h[col]], dim=1)\n",
    "        reconstructed_edge_attr = self.edge_decoder(edge_h)\n",
    "\n",
    "        return reconstructed_x, reconstructed_edge_attr\n",
    "\n",
    "# Loss function with KL divergence\n",
    "def loss_function(recon_x, x, recon_edge_attr, edge_attr, mu, logvar):\n",
    "    node_recon_loss = F.mse_loss(recon_x, x)\n",
    "    edge_recon_loss = F.mse_loss(recon_edge_attr, edge_attr)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return node_recon_loss + edge_recon_loss + kl_divergence\n",
    "\n",
    "# Objective function for Optuna to optimize the hyperparameters\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to search\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 512, step=64)\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8,16, 32, 64])\n",
    "    patience = trial.suggest_int(\"patience\", 3, 10)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_ratio = 0.8\n",
    "    valid_ratio = 0.2\n",
    "\n",
    "    num_train = int(len(pyg_dataset) * train_ratio)\n",
    "    num_valid = len(pyg_dataset) - num_train\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(pyg_dataset, [num_train, num_valid])\n",
    "    train_loader = PyGDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = PyGDataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = VGAE(node_features, edge_features, hidden_dim, latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience // 2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, recon_edge_attr, mu, logvar = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "            loss = loss_function(recon_x, data.x, recon_edge_attr, data.edge_attr, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                recon_x_val, recon_edge_attr_val, mu_val, logvar_val = model(val_data.x, val_data.edge_index, val_data.edge_attr)\n",
    "                total_val_loss += loss_function(recon_x_val, val_data.x, recon_edge_attr_val, val_data.edge_attr, mu_val, logvar_val).item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # Print training and validation losses\n",
    "        print(f\"[Trial {trial.number}] Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"[Trial {trial.number}] Early stopping after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Run the study with Optuna\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing\n",
    "\n",
    "    num_jobs = min(multiprocessing.cpu_count(), 7)  # Use up to 4 cores\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=200, n_jobs=num_jobs)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"\\nBest Trial Number: {best_trial.number}\")\n",
    "    print(\"Best Parameters:\", best_trial.params)\n",
    "    print(\"Best Value (Objective):\", best_trial.value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
